{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Htrflow","text":""},{"location":"index.html#htrflow","title":"Htrflow","text":"<p>An open-source framework designed to enhance productivity and flexibility for both Handwritten Text Recognition (HTR) and Optical Character Recogntion (OCR). It is easy to learn, delivers high performance, and is ready for seamless integration and scaling in production environments.</p>"},{"location":"index.html#key-features","title":"Key features","text":"<ul> <li>Standardization: Avoid reinventing the wheel by standardizing HTR and OCR processes.</li> <li>Intuitive Design: User-friendly and intuitive interface. </li> <li>Collaboration: Facilitate seamless collaboration both internally and with external partners within the humanities space.</li> <li>Modular Design: Employ the \"Lego principle\" to create building blocks for specific document types using \u201cblueprints.\u201d</li> <li>Ease of Learning: Simple and intuitive design that is easy to learn.</li> <li>Robustness: Production-ready code that has been thoroughly tested.</li> <li>Standards-Based: Supports various industry standard formats such as XML and JSON (and more).</li> </ul>"},{"location":"index.html#htrflow-at-riksarkivet-the-swedish-national-archives","title":"Htrflow at Riksarkivet (The Swedish National Archives)","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p>"},{"location":"index.html#_1","title":"Htrflow","text":""},{"location":"index.html#ecosystem","title":"Ecosystem","text":"Htrflow as a ServiceHtrflow Sandbox <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p> <p> </p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p> <p> </p>"},{"location":"api/index.html","title":"Test","text":"<p>Release Notes</p> <p>Releases on Github </p>"},{"location":"api/image/image.html","title":"Image","text":""},{"location":"api/results/results.html","title":"Results","text":""},{"location":"api/results/results.html#htrflow_core.results.RecognizedText","title":"<code>RecognizedText</code>  <code>dataclass</code>","text":"<p>Recognized text class</p> <p>This class represents a result from a text recognition model.</p> <p>Attributes:</p> Name Type Description <code>texts</code> <code>list[str]</code> <p>A sequence of candidate texts</p> <code>scores</code> <code>list[float]</code> <p>The scores of the candidate texts</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@dataclass\nclass RecognizedText:\n    \"\"\"Recognized text class\n\n    This class represents a result from a text recognition model.\n\n    Attributes:\n        texts: A sequence of candidate texts\n        scores: The scores of the candidate texts\n    \"\"\"\n\n    texts: list[str]\n    scores: list[float]\n\n    def __post_init__(self):\n        if not isinstance(self.texts, list):\n            self.texts = [self.texts]\n        if not isinstance(self.scores, list):\n            self.scores = [self.scores]\n\n    def top_candidate(self) -&gt; str:\n        \"\"\"The candidate with the highest confidence score\"\"\"\n        return self.texts[self.scores.index(self.top_score())]\n\n    def top_score(self):\n        \"\"\"The highest confidence score\"\"\"\n        return max(self.scores)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.RecognizedText.top_candidate","title":"<code>top_candidate()</code>","text":"<p>The candidate with the highest confidence score</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def top_candidate(self) -&gt; str:\n    \"\"\"The candidate with the highest confidence score\"\"\"\n    return self.texts[self.scores.index(self.top_score())]\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.RecognizedText.top_score","title":"<code>top_score()</code>","text":"<p>The highest confidence score</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def top_score(self):\n    \"\"\"The highest confidence score\"\"\"\n    return max(self.scores)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result","title":"<code>Result</code>","text":"<p>A result from an arbitrary model (or process)</p> <p>One result instance corresponds to one input image.</p> <p>Attributes:</p> Name Type Description <code>metadata</code> <p>Metadata regarding the result, model-dependent.</p> <code>segments</code> <p><code>Segment</code> instances representing results from an object detection or instance segmentation model, or similar. May be empty if not applicable.</p> <code>data</code> <p>Any other data associated with the result, stored as a sequence of dictionaries. Is assumed to correspond one-to-one with <code>segments</code> when <code>segments</code> is non-empty. If <code>segments</code> is empty, the first entry in <code>data</code> is assumed to apply for the entire input image.</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>class Result:\n    \"\"\"\n    A result from an arbitrary model (or process)\n\n    One result instance corresponds to one input image.\n\n    Attributes:\n        metadata: Metadata regarding the result, model-dependent.\n        segments: `Segment` instances representing results from an object\n            detection or instance segmentation model, or similar. May\n            be empty if not applicable.\n        data: Any other data associated with the result, stored as a\n            sequence of dictionaries. Is assumed to correspond one-to-one\n            with `segments` when `segments` is non-empty. If `segments`\n            is empty, the first entry in `data` is assumed to apply for\n            the entire input image.\n    \"\"\"\n\n    def __init__(\n        self,\n        metadata: dict[str, str] | None = None,\n        segments: Sequence[Segment] | None = None,\n        data: Sequence[dict[str, Any]] | None = None,\n        texts: Sequence[RecognizedText] | None = None,\n    ):\n        self.metadata = metadata or {}\n        self.segments = segments or []\n        self.data = []\n        for entry, text in _zip_longest_none(data, texts, fillvalue={}):\n            self.data.append(entry | {TEXT_RESULT_KEY: text})\n\n    def rescale(self, factor: float):\n        \"\"\"Rescale the Result's segments\"\"\"\n        for segment in self.segments:\n            segment.rescale(factor)\n\n    @property\n    def bboxes(self) -&gt; Sequence[Bbox]:\n        \"\"\"Bounding boxes relative to input image\"\"\"\n        return [segment.bbox for segment in self.segments]\n\n    @property\n    def global_masks(self) -&gt; Sequence[Mask | None]:\n        \"\"\"Global masks relative to input image\"\"\"\n        return [segment.global_mask for segment in self.segments]\n\n    @property\n    def local_mask(self) -&gt; Sequence[Mask | None]:\n        \"\"\"Local masks relative to bounding boxes\"\"\"\n        return [segment.local_mask for segment in self.segments]\n\n    @property\n    def polygons(self) -&gt; Sequence[Polygon | None]:\n        \"\"\"Polygons relative to input image\"\"\"\n        return [segment.polygon for segment in self.segments]\n\n    @property\n    def class_labels(self) -&gt; Sequence[str | None]:\n        \"\"\"Class labels of segments\"\"\"\n        return [segment.class_label for segment in self.segments]\n\n    @classmethod\n    def text_recognition_result(cls, metadata: dict[str, Any], texts: list[str], scores: list[float]) -&gt; \"Result\":\n        \"\"\"Create a text recognition result\n\n        Arguments:\n            metadata: Result metadata\n            text: The recognized text\n\n        Returns:\n            A Result instance with the specified data and no segments.\n        \"\"\"\n        return cls(metadata, texts=[RecognizedText(texts, scores)])\n\n    @classmethod\n    def segmentation_result(\n        cls,\n        orig_shape: tuple[int, int],\n        metadata: dict[str, Any],\n        bboxes: Sequence[Bbox | Iterable[int]] | None = None,\n        masks: Sequence[Mask] | None = None,\n        polygons: Sequence[Polygon] | None = None,\n        scores: Iterable[float] | None = None,\n        labels: Iterable[str] | None = None,\n    ) -&gt; \"Result\":\n        \"\"\"Create a segmentation result\n\n        Arguments:\n            image: The original image\n            metadata: Result metadata\n            segments: The segments\n\n        Returns:\n            A Result instance with the specified data and no texts.\n        \"\"\"\n        segments = []\n        for item in _zip_longest_none(bboxes, masks, scores, labels, polygons):\n            segments.append(Segment(*item, orig_shape=orig_shape))\n        return cls(metadata, segments=segments)\n\n    def reorder(self, index: Sequence[int]) -&gt; None:\n        \"\"\"Reorder result\n\n        Example: Given a `Result` with three segments s0, s1 and s2,\n        index = [2, 0, 1] will put the segments in order [s2, s0, s1].\n        Any indices not in `index` will be dropped from the result.\n\n        Arguments:\n            index: A list of indices representing the new ordering.\n        \"\"\"\n        if self.segments:\n            self.segments = [self.segments[i] for i in index]\n        if self.data:\n            self.data = [self.data[i] for i in index]\n\n    def drop_indices(self, index: Sequence[int]) -&gt; None:\n        \"\"\"Drop segments from result\n\n        Example: Given a `Result` with three segments s0, s1 and s2,\n        index = [0, 2] will drop segments s0 and s2.\n\n        Arguments:\n            index: Indices of segments to drop\n        \"\"\"\n        keep = [i for i in range(len(self.segments)) if i not in index]\n        self.reorder(keep)\n\n    def filter(self, key: str, predicate: Callable[[Any], bool]) -&gt; None:\n        \"\"\"Filter segments and data based on a predicate applied to a specified key.\n\n        Args:\n            key: The key in the data dictionary to test the predicate against.\n            predicate [Callable]: A function that takes a value associated with the key\n            and returns True if the segment should be kept.\n\n        Example:\n        ```\n        &gt;&gt;&gt; def remove_certain_text(text_results):\n        &gt;&gt;&gt;    return text_results != 'lorem'\n        &gt;&gt;&gt; result.filter('text_results', remove_certain_text)\n        True\n        ```\n        \"\"\"\n        keep = [i for i, item in enumerate(self.data) if predicate(item.get(key, None))]\n        self.reorder(keep)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.bboxes","title":"<code>bboxes: Sequence[Bbox]</code>  <code>property</code>","text":"<p>Bounding boxes relative to input image</p>"},{"location":"api/results/results.html#htrflow_core.results.Result.class_labels","title":"<code>class_labels: Sequence[str | None]</code>  <code>property</code>","text":"<p>Class labels of segments</p>"},{"location":"api/results/results.html#htrflow_core.results.Result.global_masks","title":"<code>global_masks: Sequence[Mask | None]</code>  <code>property</code>","text":"<p>Global masks relative to input image</p>"},{"location":"api/results/results.html#htrflow_core.results.Result.local_mask","title":"<code>local_mask: Sequence[Mask | None]</code>  <code>property</code>","text":"<p>Local masks relative to bounding boxes</p>"},{"location":"api/results/results.html#htrflow_core.results.Result.polygons","title":"<code>polygons: Sequence[Polygon | None]</code>  <code>property</code>","text":"<p>Polygons relative to input image</p>"},{"location":"api/results/results.html#htrflow_core.results.Result.drop_indices","title":"<code>drop_indices(index)</code>","text":"<p>Drop segments from result</p> <p>Example: Given a <code>Result</code> with three segments s0, s1 and s2, index = [0, 2] will drop segments s0 and s2.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Sequence[int]</code> <p>Indices of segments to drop</p> required Source code in <code>src/htrflow_core/results.py</code> <pre><code>def drop_indices(self, index: Sequence[int]) -&gt; None:\n    \"\"\"Drop segments from result\n\n    Example: Given a `Result` with three segments s0, s1 and s2,\n    index = [0, 2] will drop segments s0 and s2.\n\n    Arguments:\n        index: Indices of segments to drop\n    \"\"\"\n    keep = [i for i in range(len(self.segments)) if i not in index]\n    self.reorder(keep)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.filter","title":"<code>filter(key, predicate)</code>","text":"<p>Filter segments and data based on a predicate applied to a specified key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key in the data dictionary to test the predicate against.</p> required <code>predicate</code> <code>[Callable]</code> <p>A function that takes a value associated with the key</p> required <p>Example: <pre><code>&gt;&gt;&gt; def remove_certain_text(text_results):\n&gt;&gt;&gt;    return text_results != 'lorem'\n&gt;&gt;&gt; result.filter('text_results', remove_certain_text)\nTrue\n</code></pre></p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def filter(self, key: str, predicate: Callable[[Any], bool]) -&gt; None:\n    \"\"\"Filter segments and data based on a predicate applied to a specified key.\n\n    Args:\n        key: The key in the data dictionary to test the predicate against.\n        predicate [Callable]: A function that takes a value associated with the key\n        and returns True if the segment should be kept.\n\n    Example:\n    ```\n    &gt;&gt;&gt; def remove_certain_text(text_results):\n    &gt;&gt;&gt;    return text_results != 'lorem'\n    &gt;&gt;&gt; result.filter('text_results', remove_certain_text)\n    True\n    ```\n    \"\"\"\n    keep = [i for i, item in enumerate(self.data) if predicate(item.get(key, None))]\n    self.reorder(keep)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.reorder","title":"<code>reorder(index)</code>","text":"<p>Reorder result</p> <p>Example: Given a <code>Result</code> with three segments s0, s1 and s2, index = [2, 0, 1] will put the segments in order [s2, s0, s1]. Any indices not in <code>index</code> will be dropped from the result.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Sequence[int]</code> <p>A list of indices representing the new ordering.</p> required Source code in <code>src/htrflow_core/results.py</code> <pre><code>def reorder(self, index: Sequence[int]) -&gt; None:\n    \"\"\"Reorder result\n\n    Example: Given a `Result` with three segments s0, s1 and s2,\n    index = [2, 0, 1] will put the segments in order [s2, s0, s1].\n    Any indices not in `index` will be dropped from the result.\n\n    Arguments:\n        index: A list of indices representing the new ordering.\n    \"\"\"\n    if self.segments:\n        self.segments = [self.segments[i] for i in index]\n    if self.data:\n        self.data = [self.data[i] for i in index]\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.rescale","title":"<code>rescale(factor)</code>","text":"<p>Rescale the Result's segments</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def rescale(self, factor: float):\n    \"\"\"Rescale the Result's segments\"\"\"\n    for segment in self.segments:\n        segment.rescale(factor)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.segmentation_result","title":"<code>segmentation_result(orig_shape, metadata, bboxes=None, masks=None, polygons=None, scores=None, labels=None)</code>  <code>classmethod</code>","text":"<p>Create a segmentation result</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <p>The original image</p> required <code>metadata</code> <code>dict[str, Any]</code> <p>Result metadata</p> required <code>segments</code> <p>The segments</p> required <p>Returns:</p> Type Description <code>Result</code> <p>A Result instance with the specified data and no texts.</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@classmethod\ndef segmentation_result(\n    cls,\n    orig_shape: tuple[int, int],\n    metadata: dict[str, Any],\n    bboxes: Sequence[Bbox | Iterable[int]] | None = None,\n    masks: Sequence[Mask] | None = None,\n    polygons: Sequence[Polygon] | None = None,\n    scores: Iterable[float] | None = None,\n    labels: Iterable[str] | None = None,\n) -&gt; \"Result\":\n    \"\"\"Create a segmentation result\n\n    Arguments:\n        image: The original image\n        metadata: Result metadata\n        segments: The segments\n\n    Returns:\n        A Result instance with the specified data and no texts.\n    \"\"\"\n    segments = []\n    for item in _zip_longest_none(bboxes, masks, scores, labels, polygons):\n        segments.append(Segment(*item, orig_shape=orig_shape))\n    return cls(metadata, segments=segments)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.text_recognition_result","title":"<code>text_recognition_result(metadata, texts, scores)</code>  <code>classmethod</code>","text":"<p>Create a text recognition result</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[str, Any]</code> <p>Result metadata</p> required <code>text</code> <p>The recognized text</p> required <p>Returns:</p> Type Description <code>Result</code> <p>A Result instance with the specified data and no segments.</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@classmethod\ndef text_recognition_result(cls, metadata: dict[str, Any], texts: list[str], scores: list[float]) -&gt; \"Result\":\n    \"\"\"Create a text recognition result\n\n    Arguments:\n        metadata: Result metadata\n        text: The recognized text\n\n    Returns:\n        A Result instance with the specified data and no segments.\n    \"\"\"\n    return cls(metadata, texts=[RecognizedText(texts, scores)])\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment","title":"<code>Segment</code>","text":"<p>Segment class</p> <p>Class representing a segment of an image, typically a result from a segmentation model or a detection model.</p> <p>Attributes:</p> Name Type Description <code>bbox</code> <code>Bbox</code> <p>The bounding box of the segment</p> <code>mask</code> <code>Mask | None</code> <p>The segment's mask, if available. The mask is stored relative to the bounding box. Use the <code>global_mask()</code> method to retrieve the mask relative to the original image.</p> <code>score</code> <code>float | None</code> <p>Segment confidence score, if available.</p> <code>class_label</code> <code>str | None</code> <p>Segment class label, if available.</p> <code>polygon</code> <code>Polygon | None</code> <p>An approximation of the segment mask, relative to the original image. If no mask is available, <code>polygon</code> defaults to a polygon representation of the segment's bounding box.</p> <code>orig_shape</code> <code>tuple[int, int] | None</code> <p>The shape of the orginal input image.</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>class Segment:\n    \"\"\"Segment class\n\n    Class representing a segment of an image, typically a result from\n    a segmentation model or a detection model.\n\n    Attributes:\n        bbox: The bounding box of the segment\n        mask: The segment's mask, if available. The mask is stored\n            relative to the bounding box. Use the `global_mask()`\n            method to retrieve the mask relative to the original image.\n        score: Segment confidence score, if available.\n        class_label: Segment class label, if available.\n        polygon: An approximation of the segment mask, relative to the\n            original image. If no mask is available, `polygon` defaults\n            to a polygon representation of the segment's bounding box.\n        orig_shape: The shape of the orginal input image.\n    \"\"\"\n\n    bbox: Bbox\n    mask: Mask | None\n    score: float | None\n    class_label: str | None\n    polygon: Polygon | None\n    orig_shape: tuple[int, int] | None\n\n    def __init__(\n        self,\n        bbox: tuple[int, int, int, int] | Bbox | None = None,\n        mask: Mask | None = None,\n        score: float | None = None,\n        class_label: str | None = None,\n        polygon: Polygon | Sequence[tuple[int, int]] | None = None,\n        orig_shape: tuple[int, int] | None = None,\n    ):\n        \"\"\"Create a `Segment` instance\n\n        A segment can be created from a bounding box, a polygon, a mask\n        or any combination of the three.\n\n        Arguments:\n            bbox: The segment's bounding box, as either a `geometry.Bbox`\n                instance or as a (xmin, ymin, xmax, ymax) tuple. Required\n                if `mask` and `polygon` are None. Defaults to None.\n            mask: The segment's mask relative to the original input image.\n                Required if both `polygon` and `bbox` are None. Defaults\n                to None.\n            score: Segment confidence score. Defaults to None.\n            class_label: Segment class label. Defaults to None.\n            polygon: A polygon defining the segment, relative to the input\n                image. Defaults to None. Required if both `mask` and `bbox`\n                are None.\n            orig_shape: The shape of the orginal input image. Defaults to\n                None.\n        \"\"\"\n        if all(item is None for item in (bbox, mask, polygon)):\n            raise ValueError(\"Cannot create a Segment without bbox, mask or polygon\")\n\n        # Mask (and possibly bbox) is given: The mask is assumed to be aligned\n        # with the original image. The bounding box is discarded (if given) and\n        # recomputed from the mask. A polygon is also inferred from the mask.\n        # The mask is then converted to a local mask.\n        if mask is not None:\n            bbox = geometry.mask2bbox(mask)\n            polygon = geometry.mask2polygon(mask)\n            mask = imgproc.crop(mask, bbox)\n\n        if polygon is not None:\n            polygon = geometry.Polygon(polygon)\n\n            # Use the polygon's bounding box if no other bounding box was provided\n            if bbox is None:\n                bbox = polygon.bbox()\n\n        self.bbox = geometry.Bbox(*bbox)\n        self.polygon = polygon\n        self.mask = mask\n        self.score = score\n        self.class_label = class_label\n        self.orig_shape = orig_shape\n\n    def __str__(self):\n        return f\"Segment(class_label={self.class_label}, score={self.score}, bbox={self.bbox}, polygon={self.polygon}, mask={self.mask})\"  # noqa: E501\n\n    @property\n    def global_mask(self, orig_shape: tuple[int, int] | None = None) -&gt; Mask | None:\n        \"\"\"\n        The segment mask relative to the original input image.\n\n        Arguments:\n            orig_shape: Pass this argument to use another original shape\n                than the segment's `orig_shape` attribute. Defaults to None.\n        \"\"\"\n        if self.mask is None:\n            return None\n\n        orig_shape = self.orig_shape if orig_shape is None else orig_shape\n        if orig_shape is None:\n            raise ValueError(\"Cannot compute the global mask without knowing the original shape.\")\n\n        x1, y1, x2, y2 = self.bbox\n        mask = np.zeros(orig_shape, dtype=np.uint8)\n        mask[y1:y2, x1:x2] = self.mask\n        return mask\n\n    def approximate_mask(self, ratio: float) -&gt; Mask | None:\n        \"\"\"A lower resolution version of the global mask\n\n        Arguments:\n            ratio: Size of approximate mask relative to the original.\n        \"\"\"\n        global_mask = self.global_mask\n        if global_mask is None:\n            return None\n        return imgproc.rescale(global_mask, ratio)\n\n    @property\n    def local_mask(self):\n        \"\"\"The segment mask relative to the bounding box (alias for self.mask)\"\"\"\n        return self.mask\n\n    def rescale(self, factor: float) -&gt; None:\n        \"\"\"Rescale the segment's mask, bounding box and polygon by `factor`\"\"\"\n        if self.mask is not None:\n            self.mask = imgproc.rescale_linear(self.mask, factor)\n        self.bbox = self.bbox.rescale(factor)\n        if self.polygon is not None:\n            self.polygon = self.polygon.rescale(factor)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment.global_mask","title":"<code>global_mask: Mask | None</code>  <code>property</code>","text":"<p>The segment mask relative to the original input image.</p> <p>Parameters:</p> Name Type Description Default <code>orig_shape</code> <p>Pass this argument to use another original shape than the segment's <code>orig_shape</code> attribute. Defaults to None.</p> required"},{"location":"api/results/results.html#htrflow_core.results.Segment.local_mask","title":"<code>local_mask</code>  <code>property</code>","text":"<p>The segment mask relative to the bounding box (alias for self.mask)</p>"},{"location":"api/results/results.html#htrflow_core.results.Segment.__init__","title":"<code>__init__(bbox=None, mask=None, score=None, class_label=None, polygon=None, orig_shape=None)</code>","text":"<p>Create a <code>Segment</code> instance</p> <p>A segment can be created from a bounding box, a polygon, a mask or any combination of the three.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>tuple[int, int, int, int] | Bbox | None</code> <p>The segment's bounding box, as either a <code>geometry.Bbox</code> instance or as a (xmin, ymin, xmax, ymax) tuple. Required if <code>mask</code> and <code>polygon</code> are None. Defaults to None.</p> <code>None</code> <code>mask</code> <code>Mask | None</code> <p>The segment's mask relative to the original input image. Required if both <code>polygon</code> and <code>bbox</code> are None. Defaults to None.</p> <code>None</code> <code>score</code> <code>float | None</code> <p>Segment confidence score. Defaults to None.</p> <code>None</code> <code>class_label</code> <code>str | None</code> <p>Segment class label. Defaults to None.</p> <code>None</code> <code>polygon</code> <code>Polygon | Sequence[tuple[int, int]] | None</code> <p>A polygon defining the segment, relative to the input image. Defaults to None. Required if both <code>mask</code> and <code>bbox</code> are None.</p> <code>None</code> <code>orig_shape</code> <code>tuple[int, int] | None</code> <p>The shape of the orginal input image. Defaults to None.</p> <code>None</code> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def __init__(\n    self,\n    bbox: tuple[int, int, int, int] | Bbox | None = None,\n    mask: Mask | None = None,\n    score: float | None = None,\n    class_label: str | None = None,\n    polygon: Polygon | Sequence[tuple[int, int]] | None = None,\n    orig_shape: tuple[int, int] | None = None,\n):\n    \"\"\"Create a `Segment` instance\n\n    A segment can be created from a bounding box, a polygon, a mask\n    or any combination of the three.\n\n    Arguments:\n        bbox: The segment's bounding box, as either a `geometry.Bbox`\n            instance or as a (xmin, ymin, xmax, ymax) tuple. Required\n            if `mask` and `polygon` are None. Defaults to None.\n        mask: The segment's mask relative to the original input image.\n            Required if both `polygon` and `bbox` are None. Defaults\n            to None.\n        score: Segment confidence score. Defaults to None.\n        class_label: Segment class label. Defaults to None.\n        polygon: A polygon defining the segment, relative to the input\n            image. Defaults to None. Required if both `mask` and `bbox`\n            are None.\n        orig_shape: The shape of the orginal input image. Defaults to\n            None.\n    \"\"\"\n    if all(item is None for item in (bbox, mask, polygon)):\n        raise ValueError(\"Cannot create a Segment without bbox, mask or polygon\")\n\n    # Mask (and possibly bbox) is given: The mask is assumed to be aligned\n    # with the original image. The bounding box is discarded (if given) and\n    # recomputed from the mask. A polygon is also inferred from the mask.\n    # The mask is then converted to a local mask.\n    if mask is not None:\n        bbox = geometry.mask2bbox(mask)\n        polygon = geometry.mask2polygon(mask)\n        mask = imgproc.crop(mask, bbox)\n\n    if polygon is not None:\n        polygon = geometry.Polygon(polygon)\n\n        # Use the polygon's bounding box if no other bounding box was provided\n        if bbox is None:\n            bbox = polygon.bbox()\n\n    self.bbox = geometry.Bbox(*bbox)\n    self.polygon = polygon\n    self.mask = mask\n    self.score = score\n    self.class_label = class_label\n    self.orig_shape = orig_shape\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment.approximate_mask","title":"<code>approximate_mask(ratio)</code>","text":"<p>A lower resolution version of the global mask</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>Size of approximate mask relative to the original.</p> required Source code in <code>src/htrflow_core/results.py</code> <pre><code>def approximate_mask(self, ratio: float) -&gt; Mask | None:\n    \"\"\"A lower resolution version of the global mask\n\n    Arguments:\n        ratio: Size of approximate mask relative to the original.\n    \"\"\"\n    global_mask = self.global_mask\n    if global_mask is None:\n        return None\n    return imgproc.rescale(global_mask, ratio)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment.rescale","title":"<code>rescale(factor)</code>","text":"<p>Rescale the segment's mask, bounding box and polygon by <code>factor</code></p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def rescale(self, factor: float) -&gt; None:\n    \"\"\"Rescale the segment's mask, bounding box and polygon by `factor`\"\"\"\n    if self.mask is not None:\n        self.mask = imgproc.rescale_linear(self.mask, factor)\n    self.bbox = self.bbox.rescale(factor)\n    if self.polygon is not None:\n        self.polygon = self.polygon.rescale(factor)\n</code></pre>"},{"location":"api/volume/node.html","title":"Noe","text":""},{"location":"api/volume/volume.html","title":"Volume","text":""},{"location":"getting_started/index.html","title":"Getting Started","text":"<ul> <li> <p> Set up in 5 minutes</p> <p>Install <code>mkdocs-material</code> with <code>pip</code> and get up and running in minutes</p> <p> Getting started</p> </li> <li> <p> It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site</p> <p> Reference</p> </li> <li> <p> Made to measure</p> <p>Change the colors, fonts, language, icons, logo and more with a few lines</p> <p> Customization</p> </li> <li> <p> Open Source, MIT</p> <p>Material for MkDocs is licensed under MIT and available on [GitHub]</p> <p> License</p> </li> </ul>"},{"location":"getting_started/data_structure.html","title":"Working with the data","text":""},{"location":"getting_started/installation.html","title":"Installation / Setup","text":""},{"location":"getting_started/installation.html#installation","title":"Installation","text":"&gt; pip install htrflow_coreInstalled InstallationDevelopment <p>You can install <code>htrflow_core</code> with pypi in a Python&gt;=3.10 environment.</p> <p>pip install (recommended)</p> coremodels <p>The core installation of <code>htrflow_core</code> install everything you need to get you started with structuring output in your htr workflow.</p> <pre><code>pip install htrflow_core\n</code></pre> <p>This installation add support f\u00f6r models we have implemented.</p> <pre><code>pip install \"htrflow_core[models]\"\n</code></pre> <p>git clone (for development)</p> virtualenvpoetry <pre><code># clone repository and navigate to root directory\ngit clone https://github.com/Swedish-National-Archives-AI-lab/htrflow_core\ncd htrflow_core\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\n\n# core install\npip install -e \".\"\n\n# all models install\npip install -e \".[huggingface, openmmlab, ultralytics]\"\n</code></pre> <pre><code># clone repository and navigate to root directory\ngit clone https://github.com/Swedish-National-Archives-AI-lab/htrflow_core\ncd htrflow_core\n\n# setup python environment and activate it\npoetry env use python3.10\npoetry shell\n\n# core install\npoetry install\n\n# all models install\npoetry install --all-extras\n\n# or specific framework\npoetry install --extras huggingface\n</code></pre>"},{"location":"getting_started/installation.html#docker","title":"Docker","text":""},{"location":"getting_started/installation.html#docker-compose","title":"docker-compose","text":""},{"location":"getting_started/installation.html#helm","title":"Helm","text":""},{"location":"getting_started/models.html","title":"Adding new models","text":""},{"location":"getting_started/pipeline.html","title":"Building your pipeline","text":""},{"location":"getting_started/quick_start.html","title":"Quickstart","text":"<p> - Quickstart</p>"},{"location":"getting_started/quick_start.html#data","title":"Data","text":"<p>Load dataset from huggingface</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"Riksarkivet/Trolldomkomission\")[\"train\"]\n\nimages = dataset[\"image\"]\n</code></pre>"},{"location":"getting_started/quick_start.html#volume","title":"Volume","text":"<pre><code>from htrflow_core.volume import Volume\n\nvol = Volume([images])\n</code></pre>"},{"location":"getting_started/quick_start.html#segment-images","title":"Segment Images","text":"<pre><code>from htrflow_core.models.ultralytics.yolo import YOLO\n\nseg_model = YOLO('ultralyticsplus/yolov8s')\nres = seg_model(vol.images()) # vol.segments() is also possible since it points to the images\n</code></pre>"},{"location":"getting_started/quick_start.html#update-volume","title":"Update Volume","text":"<pre><code>vol.update(res)\n</code></pre>"},{"location":"getting_started/quick_start.html#htr","title":"HTR","text":"<pre><code>from htrflow_core.models.huggingface.trocr import TrOCR\n\nrec_model = TrOCR()\nres = rec_model(vol.segments())\n\nvol.update(res)\n</code></pre> <p>Note</p> <p>The final volume <pre><code>    print(vol)\n</code></pre></p>"},{"location":"getting_started/quick_start.html#serialize","title":"Serialize","text":"<p>Saves at outputs/.xml, since the two demo images are called the same, we get only one output file</p> <pre><code>vol.save('outputs', 'alto')\n</code></pre> <p>..</p> <p>Whenever you have large documents, you typicall ...</p> <pre><code># Something here\n</code></pre>"},{"location":"getting_started/serialization.html","title":"Serialization","text":""},{"location":"getting_started/serialization.html#support-output-formats","title":"Support output formats..","text":""},{"location":"getting_started/core/design.html","title":"Htrflow Design","text":"<pre><code>graph TD\n    A[htrflow_core] --&gt; B(dummies)\n    A --&gt; C(image)\n    A --&gt; E(logging)\n    A --&gt; F(models)\n    A --&gt; G(overlapping_masks)\n\n    A --&gt; J(reading_order)\n    A --&gt; K(results)\n    A --&gt; L(serialization)\n    A --&gt; M(templates)\n    A --&gt; N(volume)\n\n\n    F --&gt; F3(huggingface)\n    F --&gt; F5(openmmlab)\n    F --&gt; F7(ultralytics)\n\n    M --&gt; M1(alto)\n    M --&gt; M2(page)</code></pre>"},{"location":"getting_started/core/design.html#sequence-diagram-workflow","title":"Sequence Diagram workflow","text":"<p>The Swedish National Archives introduces a...</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p>"},{"location":"getting_started/core/design.html#data-strucutre","title":"Data strucutre","text":"<pre><code>graph TD\n    A(\"Image.jpg\") --&gt; B(\"Region\")\n    B --&gt; C1(\"Line - Text: 'Lorem non ipsum dolor.'\")\n    C1 --&gt; D1(\"Word: 'Lorem'\")\n    C1 --&gt; D2(\"Word: 'non'\")\n    C1 --&gt; D3(\"Word: 'ipsum'\")\n    C1 --&gt; D4(\"Word: 'dolor.'\")\n    B --&gt; C2(\"Line - Text: 'Numquam consectetur ut'\")\n    C2 --&gt; D5(\"Word: 'Numquam'\")\n    C2 --&gt; D6(\"Word: 'consectetur'\")\n    C2 --&gt; D7(\"Word: 'ut'\")</code></pre>"},{"location":"getting_started/core/ecosystem.html","title":"Htrflow Ecosystem","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p> <pre><code>mindmap\n  root((htrlow))\n    htrflow_core\n      Core\n      CLI\n    sandbox\n      Fasttrack\n      Stepwise\n      More..\n    As-a-service?\n      Rest API?\n      Infrastructure?</code></pre>"},{"location":"getting_started/core/ecosystem.html#haas","title":"HaaS?","text":""},{"location":"help/index.html","title":"Help","text":""},{"location":"help/contributing.html","title":"Contributing to htrflow_core \ud83d\udee0\ufe0f","text":"<p>Thank you for your interest in contributing to htrflow_core! We appreciate contributions in the following areas:</p> <ol> <li>New Features: Enhance the library by adding new functionality. Refer to the section below for guidelines.</li> <li>Documentation: Help us improve our documentation with clear examples demonstrating how to use htrflow_core.</li> <li>Bug Reports: Identify and report any issues in the project.</li> <li>Feature Requests: Suggest new features or improvements.</li> </ol>"},{"location":"help/contributing.html#contributing-features","title":"Contributing Features \u2728","text":"<p>htrflow_core aims to provide versatile tools applicable across a broad range of projects. We value contributions that offer generic solutions to common problems. Before proposing a new feature, please open an issue to discuss your idea with the community. This encourages feedback and support.</p>"},{"location":"help/contributing.html#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Fork the htrflow_core repository to your GitHub account by clicking \"fork\" at the top right of the repository page.</li> <li>Clone your fork locally and create a new branch for your changes:</li> </ol> <pre><code>git clone https://github.com/yourusername/htrflow_core.git\ncd htrflow_core\ngit checkout -b &lt;your_branch_name&gt;\n</code></pre> <ol> <li>Develop your feature, fix, or documentation update on your branch.</li> </ol>"},{"location":"help/contributing.html#code-quality","title":"Code Quality \ud83c\udfa8","text":"<p>Ensure your code adheres to our quality standards using tools like:</p> <ul> <li>ruff</li> <li>mypy</li> </ul>"},{"location":"help/contributing.html#documentation","title":"Documentation \ud83d\udcdd","text":"<p>Our documentation utilizes docstrings combined with type hinting from mypy. Update or add necessary documentation in the <code>docs/</code> directory and test it locally with:</p> <pre><code>mkdocs serve -v\n</code></pre>"},{"location":"help/contributing.html#tests","title":"Tests \ud83e\uddea","text":"<p>We employ pytest for testing. Ensure you add tests for your changes and run:</p> <pre><code>pytest\n</code></pre>"},{"location":"help/contributing.html#making-a-pull-request","title":"Making a Pull Request","text":"<p>After pushing your changes to GitHub, initiate a pull request from your fork to the main <code>htrflow_core</code> repository:</p> <ol> <li>Push your branch:</li> </ol> <pre><code>git push -u origin &lt;your_branch_name&gt;\n</code></pre> <ol> <li>Visit the repository on GitHub and click \"New Pull Request.\" Set the base branch to <code>develop</code> and describe your changes.</li> </ol> <p>Ensure all tests pass before requesting a review.</p>"},{"location":"help/contributing.html#license","title":"License \ud83d\udcc4","text":"<p>By contributing to htrflow_core, you agree that your contributions will be licensed under the EUPL-1.2 license.</p> <p>Thank you for contributing to htrflow_core!</p>"},{"location":"help/faq.html","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"help/faq.html#info","title":"INFO","text":"LicenseContactCitation"},{"location":"help/faq.html#license","title":"License","text":"<p>INFO</p> <p>dsda</p>"},{"location":"help/faq.html#contact","title":"Contact","text":"<p>INFO</p> <p>dsda</p>"},{"location":"help/faq.html#citation","title":"Citation","text":"<p>INFO</p> <p>dsda</p>"},{"location":"help/license.html","title":"License Overview","text":""},{"location":"help/license.html#european-union-public-licence-v-12-eupl-v-12","title":"European Union Public Licence v. 1.2 (EUPL v. 1.2)","text":""},{"location":"help/license.html#summary","title":"Summary","text":"<p>This document provides a summary of the European Union Public Licence version 1.2. It outlines the scope of the license, definitions of key terms, the rights granted, and the obligations of users.</p>"},{"location":"help/license.html#key-features-of-eupl-v-12","title":"Key Features of EUPL v. 1.2","text":"<ul> <li>Scope of License: Grants permission to use, reproduce, modify, and distribute the work under specific conditions.</li> <li>What\u2019s Covered:</li> <li>Original Work: Any work provided under this license, accessible as both source and executable code.</li> <li>Derivative Works: Any modifications or developments based on the original work.</li> <li>Licensor: The entity providing the work under this license.</li> <li>Licensee (You): Users of the work.</li> </ul>"},{"location":"help/license.html#rights-and-permissions","title":"Rights and Permissions","text":"<ul> <li>Full usage rights for any purpose, including:</li> <li>Modification and Derivative Works: Freedom to alter the work and create new derivatives.</li> <li>Distribution and Communication: Share the work publicly in any form.</li> <li>Patent Rights: Use of Licensor\u2019s patents as needed to utilize the work.</li> </ul>"},{"location":"help/license.html#requirements-for-using-the-license","title":"Requirements for Using the License","text":"<ul> <li>Source Code: If distributed as executable code, the source must be accessible.</li> <li>Obligations:</li> <li>Retain all copyright and license notices.</li> <li>Distribute the license copy with the work.</li> <li>Ensure derivative works are clearly marked as modified.</li> </ul>"},{"location":"help/license.html#legal-framework","title":"Legal Framework","text":"<ul> <li>Copyleft Requirement: Redistribution or sharing must adhere to this license or a future version.</li> <li>Compatibility: Derivative works may be distributed under a compatible license (listed in the appendices).</li> <li>Trademark Use: The license does not permit the use of the licensor\u2019s trademarks unless explicitly necessary.</li> </ul>"},{"location":"help/license.html#guarantees-and-liabilities","title":"Guarantees and Liabilities","text":"<ul> <li>No Warranties: The work is provided as-is, without any guarantees.</li> <li>Limited Liability: The licensor is not liable for damages, except as required by law.</li> <li>Termination of Rights: Breaching license terms results in automatic termination of rights.</li> </ul>"},{"location":"help/license.html#jurisdiction","title":"Jurisdiction","text":"<ul> <li>Governed by the laws of the licensor's residence, or by Belgian law if outside an EU Member State.</li> </ul>"},{"location":"help/license.html#additional-information","title":"Additional Information","text":"<ul> <li>Appendices: List of compatible licenses like GPL, AGPL, MPL, LGPL, etc.</li> <li>Additional Agreements: Users may negotiate further agreements in line with this license.</li> </ul>"},{"location":"help/license.html#eupl-v-12-full-version","title":"EUPL v. 1.2 (Full version)","text":"<pre><code>                      EUROPEAN UNION PUBLIC LICENCE v. 1.2\n                      EUPL \u00a9 the European Union 2007, 2016\n\nThis European Union Public Licence (the \u2018EUPL\u2019) applies to the Work (as defined\nbelow) which is provided under the terms of this Licence. Any use of the Work,\nother than as authorised under this Licence is prohibited (to the extent such\nuse is covered by a right of the copyright holder of the Work).\n\nThe Work is provided under the terms of this Licence when the Licensor (as\ndefined below) has placed the following notice immediately following the\ncopyright notice for the Work:\n\n        Licensed under the EUPL\n\nor has expressed by any other means his willingness to license under the EUPL.\n\n1. Definitions\n\nIn this Licence, the following terms have the following meaning:\n\n- \u2018The Licence\u2019: this Licence.\n\n- \u2018The Original Work\u2019: the work or software distributed or communicated by the\n  Licensor under this Licence, available as Source Code and also as Executable\n  Code as the case may be.\n\n- \u2018Derivative Works\u2019: the works or software that could be created by the\n  Licensee, based upon the Original Work or modifications thereof. This Licence\n  does not define the extent of modification or dependence on the Original Work\n  required in order to classify a work as a Derivative Work; this extent is\n  determined by copyright law applicable in the country mentioned in Article 15.\n\n- \u2018The Work\u2019: the Original Work or its Derivative Works.\n\n- \u2018The Source Code\u2019: the human-readable form of the Work which is the most\n  convenient for people to study and modify.\n\n- \u2018The Executable Code\u2019: any code which has generally been compiled and which is\n  meant to be interpreted by a computer as a program.\n\n- \u2018The Licensor\u2019: the natural or legal person that distributes or communicates\n  the Work under the Licence.\n\n- \u2018Contributor(s)\u2019: any natural or legal person who modifies the Work under the\n  Licence, or otherwise contributes to the creation of a Derivative Work.\n\n- \u2018The Licensee\u2019 or \u2018You\u2019: any natural or legal person who makes any usage of\n  the Work under the terms of the Licence.\n\n- \u2018Distribution\u2019 or \u2018Communication\u2019: any act of selling, giving, lending,\n  renting, distributing, communicating, transmitting, or otherwise making\n  available, online or offline, copies of the Work or providing access to its\n  essential functionalities at the disposal of any other natural or legal\n  person.\n\n2. Scope of the rights granted by the Licence\n\nThe Licensor hereby grants You a worldwide, royalty-free, non-exclusive,\nsublicensable licence to do the following, for the duration of copyright vested\nin the Original Work:\n\n- use the Work in any circumstance and for all usage,\n- reproduce the Work,\n- modify the Work, and make Derivative Works based upon the Work,\n- communicate to the public, including the right to make available or display\n  the Work or copies thereof to the public and perform publicly, as the case may\n  be, the Work,\n- distribute the Work or copies thereof,\n- lend and rent the Work or copies thereof,\n- sublicense rights in the Work or copies thereof.\n\nThose rights can be exercised on any media, supports and formats, whether now\nknown or later invented, as far as the applicable law permits so.\n\nIn the countries where moral rights apply, the Licensor waives his right to\nexercise his moral right to the extent allowed by law in order to make effective\nthe licence of the economic rights here above listed.\n\nThe Licensor grants to the Licensee royalty-free, non-exclusive usage rights to\nany patents held by the Licensor, to the extent necessary to make use of the\nrights granted on the Work under this Licence.\n\n3. Communication of the Source Code\n\nThe Licensor may provide the Work either in its Source Code form, or as\nExecutable Code. If the Work is provided as Executable Code, the Licensor\nprovides in addition a machine-readable copy of the Source Code of the Work\nalong with each copy of the Work that the Licensor distributes or indicates, in\na notice following the copyright notice attached to the Work, a repository where\nthe Source Code is easily and freely accessible for as long as the Licensor\ncontinues to distribute or communicate the Work.\n\n4. Limitations on copyright\n\nNothing in this Licence is intended to deprive the Licensee of the benefits from\nany exception or limitation to the exclusive rights of the rights owners in the\nWork, of the exhaustion of those rights or of other applicable limitations\nthereto.\n\n5. Obligations of the Licensee\n\nThe grant of the rights mentioned above is subject to some restrictions and\nobligations imposed on the Licensee. Those obligations are the following:\n\nAttribution right: The Licensee shall keep intact all copyright, patent or\ntrademarks notices and all notices that refer to the Licence and to the\ndisclaimer of warranties. The Licensee must include a copy of such notices and a\ncopy of the Licence with every copy of the Work he/she distributes or\ncommunicates. The Licensee must cause any Derivative Work to carry prominent\nnotices stating that the Work has been modified and the date of modification.\n\nCopyleft clause: If the Licensee distributes or communicates copies of the\nOriginal Works or Derivative Works, this Distribution or Communication will be\ndone under the terms of this Licence or of a later version of this Licence\nunless the Original Work is expressly distributed only under this version of the\nLicence \u2014 for example by communicating \u2018EUPL v. 1.2 only\u2019. The Licensee\n(becoming Licensor) cannot offer or impose any additional terms or conditions on\nthe Work or Derivative Work that alter or restrict the terms of the Licence.\n\nCompatibility clause: If the Licensee Distributes or Communicates Derivative\nWorks or copies thereof based upon both the Work and another work licensed under\na Compatible Licence, this Distribution or Communication can be done under the\nterms of this Compatible Licence. For the sake of this clause, \u2018Compatible\nLicence\u2019 refers to the licences listed in the appendix attached to this Licence.\nShould the Licensee's obligations under the Compatible Licence conflict with\nhis/her obligations under this Licence, the obligations of the Compatible\nLicence shall prevail.\n\nProvision of Source Code: When distributing or communicating copies of the Work,\nthe Licensee will provide a machine-readable copy of the Source Code or indicate\na repository where this Source will be easily and freely available for as long\nas the Licensee continues to distribute or communicate the Work.\n\nLegal Protection: This Licence does not grant permission to use the trade names,\ntrademarks, service marks, or names of the Licensor, except as required for\nreasonable and customary use in describing the origin of the Work and\nreproducing the content of the copyright notice.\n\n6. Chain of Authorship\n\nThe original Licensor warrants that the copyright in the Original Work granted\nhereunder is owned by him/her or licensed to him/her and that he/she has the\npower and authority to grant the Licence.\n\nEach Contributor warrants that the copyright in the modifications he/she brings\nto the Work are owned by him/her or licensed to him/her and that he/she has the\npower and authority to grant the Licence.\n\nEach time You accept the Licence, the original Licensor and subsequent\nContributors grant You a licence to their contributions to the Work, under the\nterms of this Licence.\n\n7. Disclaimer of Warranty\n\nThe Work is a work in progress, which is continuously improved by numerous\nContributors. It is not a finished work and may therefore contain defects or\n\u2018bugs\u2019 inherent to this type of development.\n\nFor the above reason, the Work is provided under the Licence on an \u2018as is\u2019 basis\nand without warranties of any kind concerning the Work, including without\nlimitation merchantability, fitness for a particular purpose, absence of defects\nor errors, accuracy, non-infringement of intellectual property rights other than\ncopyright as stated in Article 6 of this Licence.\n\nThis disclaimer of warranty is an essential part of the Licence and a condition\nfor the grant of any rights to the Work.\n\n8. Disclaimer of Liability\n\nExcept in the cases of wilful misconduct or damages directly caused to natural\npersons, the Licensor will in no event be liable for any direct or indirect,\nmaterial or moral, damages of any kind, arising out of the Licence or of the use\nof the Work, including without limitation, damages for loss of goodwill, work\nstoppage, computer failure or malfunction, loss of data or any commercial\ndamage, even if the Licensor has been advised of the possibility of such damage.\nHowever, the Licensor will be liable under statutory product liability laws as\nfar such laws apply to the Work.\n\n9. Additional agreements\n\nWhile distributing the Work, You may choose to conclude an additional agreement,\ndefining obligations or services consistent with this Licence. However, if\naccepting obligations, You may act only on your own behalf and on your sole\nresponsibility, not on behalf of the original Licensor or any other Contributor,\nand only if You agree to indemnify, defend, and hold each Contributor harmless\nfor any liability incurred by, or claims asserted against such Contributor by\nthe fact You have accepted any warranty or additional liability.\n\n10. Acceptance of the Licence\n\nThe provisions of this Licence can be accepted by clicking on an icon \u2018I agree\u2019\nplaced under the bottom of a window displaying the text of this Licence or by\naffirming consent in any other similar way, in accordance with the rules of\napplicable law. Clicking on that icon indicates your clear and irrevocable\nacceptance of this Licence and all of its terms and conditions.\n\nSimilarly, you irrevocably accept this Licence and all of its terms and\nconditions by exercising any rights granted to You by Article 2 of this Licence,\nsuch as the use of the Work, the creation by You of a Derivative Work or the\nDistribution or Communication by You of the Work or copies thereof.\n\n11. Information to the public\n\nIn case of any Distribution or Communication of the Work by means of electronic\ncommunication by You (for example, by offering to download the Work from a\nremote location) the distribution channel or media (for example, a website) must\nat least provide to the public the information requested by the applicable law\nregarding the Licensor, the Licence and the way it may be accessible, concluded,\nstored and reproduced by the Licensee.\n\n12. Termination of the Licence\n\nThe Licence and the rights granted hereunder will terminate automatically upon\nany breach by the Licensee of the terms of the Licence.\n\nSuch a termination will not terminate the licences of any person who has\nreceived the Work from the Licensee under the Licence, provided such persons\nremain in full compliance with the Licence.\n\n13. Miscellaneous\n\nWithout prejudice of Article 9 above, the Licence represents the complete\nagreement between the Parties as to the Work.\n\nIf any provision of the Licence is invalid or unenforceable under applicable\nlaw, this will not affect the validity or enforceability of the Licence as a\nwhole. Such provision will be construed or reformed so as necessary to make it\nvalid and enforceable.\n\nThe European Commission may publish other linguistic versions or new versions of\nthis Licence or updated versions of the Appendix, so far this is required and\nreasonable, without reducing the scope of the rights granted by the Licence. New\nversions of the Licence will be published with a unique version number.\n\nAll linguistic versions of this Licence, approved by the European Commission,\nhave identical value. Parties can take advantage of the linguistic version of\ntheir choice.\n\n14. Jurisdiction\n\nWithout prejudice to specific agreement between parties,\n\n- any litigation resulting from the interpretation of this License, arising\n  between the European Union institutions, bodies, offices or agencies, as a\n  Licensor, and any Licensee, will be subject to the jurisdiction of the Court\n  of Justice of the European Union, as laid down in article 272 of the Treaty on\n  the Functioning of the European Union,\n\n- any litigation arising between other parties and resulting from the\n  interpretation of this License, will be subject to the exclusive jurisdiction\n  of the competent court where the Licensor resides or conducts its primary\n  business.\n\n15. Applicable Law\n\nWithout prejudice to specific agreement between parties,\n\n- this Licence shall be governed by the law of the European Union Member State\n  where the Licensor has his seat, resides or has his registered office,\n\n- this licence shall be governed by Belgian law if the Licensor has no seat,\n  residence or registered office inside a European Union Member State.\n\nAppendix\n\n\u2018Compatible Licences\u2019 according to Article 5 EUPL are:\n\n- GNU General Public License (GPL) v. 2, v. 3\n- GNU Affero General Public License (AGPL) v. 3\n- Open Software License (OSL) v. 2.1, v. 3.0\n- Eclipse Public License (EPL) v. 1.0\n- CeCILL v. 2.0, v. 2.1\n- Mozilla Public Licence (MPL) v. 2\n- GNU Lesser General Public Licence (LGPL) v. 2.1, v. 3\n- Creative Commons Attribution-ShareAlike v. 3.0 Unported (CC BY-SA 3.0) for\n  works other than software\n- European Union Public Licence (EUPL) v. 1.1, v. 1.2\n- Qu\u00e9bec Free and Open-Source Licence \u2014 Reciprocity (LiLiQ-R) or Strong\n  Reciprocity (LiLiQ-R+).\n\nThe European Commission may update this Appendix to later versions of the above\nlicences without producing a new version of the EUPL, as long as they provide\nthe rights granted in Article 2 of this Licence and protect the covered Source\nCode from exclusive appropriation.\n\nAll other changes or additions to this Appendix require the production of a new\nEUPL version.\n</code></pre>"},{"location":"integrations/index.html","title":"test","text":""},{"location":"integrations/haas.html","title":"HaaS","text":"Image caption"},{"location":"integrations/search.html","title":"Enabling Full Text Search in The National Archives","text":"<p>This project focuses on integrating advanced search functionality within The National Archives' online interface to maximize the potential of transcribed texts derived from Htrflow. Currently, users can only access archival documents by searching metadata. If a search term isn't in the metadata, no results are returned, even if the term appears in the document.</p> <p>By utilizing Htrflow to transcribe a large volume of digitized archival documents, The National Archives aims to enable direct searching within document texts via the online interface. This enhancement will significantly improve accessibility and usability of archival information for researchers and the general public.</p> <p>The project involves indexing Alto XML files generated by Htrflow into Solr, enabling queries via a REST API to locate words within archival documents. This indexed data will power the new document search feature within the online interface, facilitating more comprehensive access to archival content</p> <p></p>"},{"location":"integrations/spaces.html","title":"Spaces","text":""},{"location":"integrations/spaces.html#fasttrack-stepwsie","title":"Fasttrack &amp; Stepwsie","text":"<p>htrflow_app</p> <p>htrflow_app is designed to provide users with a step-by-step visualization of the HTR-process, and offer non-expert users an inside look into the workings of an AI-transcription pipeline. At the moment htrflow_app is mainly a demo-application. It\u2019s not intended for production, but instead to showcase the immense possibilities that HTR-technology is opening up for cultural heritage institutions around the world.</p> <p>All code is open-source, all our models are on Hugging Face and are free to use, and all data will be made available for download and use on Hugging Face as well.</p> <p>Note</p> <p>Note</p> <p>The backend (src) for the app will be rewritten and packaged to be more optimized under the project name htrflow_core.</p> <p> </p>"},{"location":"notebooks/demo.html","title":"The internals","text":"In\u00a0[1]: Copied! <pre>import random\n\n\nrandom.seed(123)\n</pre> import random   random.seed(123) In\u00a0[2]: Copied! <pre>from htrflow_core.volume import Volume\n\n\nimages = [\"../assets/demo_image.jpg\"] * 5\n\nvolume = Volume(images)\n</pre> from htrflow_core.volume import Volume   images = [\"../assets/demo_image.jpg\"] * 5  volume = Volume(images) <p>The <code>Volume</code> instance holds a tree. We see the root <code>node</code> and its five children, each representing one input image:</p> In\u00a0[3]: Copied! <pre>print(volume)\n</pre> print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2514\u2500\u2500626x1629 image demo_image\n</pre> <p>The images are available through <code>volume.images()</code>. We pass them through a segmentation model:</p> In\u00a0[4]: Copied! <pre>from htrflow_core.models.dummy_models import SegmentationModel\n\n\nmodel = SegmentationModel()\nresults = model(volume.images())\nprint(results[0])\n</pre> from htrflow_core.models.dummy_models import SegmentationModel   model = SegmentationModel() results = model(volume.images()) print(results[0]) <pre>SegmentationResult(metadata={'model_name': 'SegmentationModel'}, image=array([[[118, 120, 128],\n        [115, 117, 125],\n        [114, 116, 124],\n        ...,\n        [215, 219, 220],\n        [209, 213, 214],\n        [206, 210, 211]],\n\n       [[110, 112, 120],\n        [110, 112, 120],\n        [110, 112, 120],\n        ...,\n        [211, 215, 216],\n        [207, 211, 212],\n        [209, 213, 214]],\n\n       [[109, 112, 120],\n        [109, 112, 120],\n        [104, 107, 115],\n        ...,\n        [207, 211, 212],\n        [205, 209, 210],\n        [209, 213, 214]],\n\n       ...,\n\n       [[146, 152, 151],\n        [147, 153, 152],\n        [147, 153, 152],\n        ...,\n        [212, 218, 213],\n        [214, 222, 211],\n        [211, 221, 204]],\n\n       [[144, 150, 149],\n        [146, 152, 151],\n        [148, 154, 153],\n        ...,\n        [217, 223, 212],\n        [220, 231, 205],\n        [216, 234, 187]],\n\n       [[147, 153, 152],\n        [149, 155, 154],\n        [151, 157, 156],\n        ...,\n        [214, 221, 208],\n        [214, 228, 194],\n        [208, 231, 169]]], dtype=uint8), segments=[Segment(bbox=(345, 751, 11, 167), mask=array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), polygon=array([[345,  85],\n       [356, 115],\n       [393, 140],\n       [527, 167],\n       [672, 151],\n       [726, 127],\n       [751,  93],\n       [740,  63],\n       [703,  38],\n       [570,  11],\n       [417,  29],\n       [365,  55]], dtype=int32), score=0.7689563885870707, class_label='region')])\n</pre> <p>The results are a list of <code>SegmentationResult</code>. To apply the results to the input images, we pass them back to the volume with its <code>update</code> method. It returns the new regions as a list of images.</p> In\u00a0[5]: Copied! <pre>regions = volume.update(results)\n</pre> regions = volume.update(results) <p>The volume tree has now grown:</p> In\u00a0[6]: Copied! <pre>print(volume)\n</pre> print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u2514\u2500\u2500156x406 region at (345, 11)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500117x406 region at (17, 0)\n    \u2502   \u251c\u2500\u2500156x406 region at (948, 262)\n    \u2502   \u2514\u2500\u2500156x309 region at (0, 85)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500156x406 region at (480, 173)\n    \u2502   \u251c\u2500\u2500156x406 region at (690, 11)\n    \u2502   \u251c\u2500\u2500149x406 region at (570, 0)\n    \u2502   \u251c\u2500\u2500156x332 region at (1296, 381)\n    \u2502   \u2514\u2500\u2500156x292 region at (0, 16)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u250099x213 region at (1415, 0)\n    \u2502   \u2514\u2500\u2500116x406 region at (678, 509)\n    \u2514\u2500\u2500626x1629 image demo_image\n        \u251c\u2500\u2500156x278 region at (0, 234)\n        \u251c\u2500\u2500156x406 region at (786, 133)\n        \u251c\u2500\u2500156x406 region at (1105, 461)\n        \u2514\u2500\u250090x406 region at (442, 0)\n</pre> <p>The new regions can be passed through a segmentation model (such as a line model) again. The <code>update</code> method always updates the leaves of the tree.</p> In\u00a0[7]: Copied! <pre>results = model(volume.segments())\nvolume.update(results)\nprint(volume)\n</pre> results = model(volume.segments()) volume.update(results) print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u2514\u2500\u2500156x406 region at (345, 11)\n    \u2502       \u251c\u2500\u250037x100 region at (517, 129)\n    \u2502       \u251c\u2500\u250022x100 region at (636, 144)\n    \u2502       \u251c\u2500\u250038x100 region at (543, 125)\n    \u2502       \u251c\u2500\u250038x100 region at (486, 122)\n    \u2502       \u2514\u2500\u250038x69 region at (681, 38)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500117x406 region at (17, 0)\n    \u2502   \u2502   \u2514\u2500\u250028x100 region at (216, 70)\n    \u2502   \u251c\u2500\u2500156x406 region at (948, 262)\n    \u2502   \u2502   \u251c\u2500\u250033x100 region at (1070, 384)\n    \u2502   \u2502   \u251c\u2500\u250038x87 region at (948, 359)\n    \u2502   \u2502   \u2514\u2500\u250038x57 region at (1296, 329)\n    \u2502   \u2514\u2500\u2500156x309 region at (0, 85)\n    \u2502       \u251c\u2500\u250038x76 region at (7, 159)\n    \u2502       \u251c\u2500\u250038x76 region at (142, 124)\n    \u2502       \u251c\u2500\u250034x76 region at (218, 85)\n    \u2502       \u251c\u2500\u250038x76 region at (215, 125)\n    \u2502       \u2514\u2500\u250038x76 region at (52, 105)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500156x406 region at (480, 173)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (623, 272)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (498, 270)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (561, 244)\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (652, 261)\n    \u2502   \u251c\u2500\u2500156x406 region at (690, 11)\n    \u2502   \u2502   \u251c\u2500\u250038x82 region at (690, 122)\n    \u2502   \u2502   \u251c\u2500\u250038x95 region at (690, 13)\n    \u2502   \u2502   \u251c\u2500\u250037x54 region at (690, 129)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (919, 95)\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (805, 59)\n    \u2502   \u251c\u2500\u2500149x406 region at (570, 0)\n    \u2502   \u2502   \u2514\u2500\u250023x71 region at (904, 125)\n    \u2502   \u251c\u2500\u2500156x332 region at (1296, 381)\n    \u2502   \u2502   \u251c\u2500\u250038x53 region at (1296, 403)\n    \u2502   \u2502   \u251c\u2500\u250035x82 region at (1469, 381)\n    \u2502   \u2502   \u2514\u2500\u250038x82 region at (1328, 457)\n    \u2502   \u2514\u2500\u2500156x292 region at (0, 16)\n    \u2502       \u2514\u2500\u250038x65 region at (0, 129)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u250099x213 region at (1415, 0)\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1426, 71)\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1463, 37)\n    \u2502   \u2502   \u2514\u2500\u250024x52 region at (1525, 31)\n    \u2502   \u2514\u2500\u2500116x406 region at (678, 509)\n    \u2502       \u251c\u2500\u250028x100 region at (929, 544)\n    \u2502       \u2514\u2500\u250028x76 region at (1007, 512)\n    \u2514\u2500\u2500626x1629 image demo_image\n        \u251c\u2500\u2500156x278 region at (0, 234)\n        \u2502   \u2514\u2500\u250038x68 region at (144, 330)\n        \u251c\u2500\u2500156x406 region at (786, 133)\n        \u2502   \u251c\u2500\u250038x100 region at (891, 223)\n        \u2502   \u251c\u2500\u250038x64 region at (786, 154)\n        \u2502   \u251c\u2500\u250038x100 region at (1000, 245)\n        \u2502   \u2514\u2500\u250038x100 region at (911, 242)\n        \u251c\u2500\u2500156x406 region at (1105, 461)\n        \u2502   \u251c\u2500\u250029x100 region at (1170, 587)\n        \u2502   \u251c\u2500\u250038x100 region at (1194, 571)\n        \u2502   \u2514\u2500\u250038x100 region at (1219, 509)\n        \u2514\u2500\u250090x406 region at (442, 0)\n            \u251c\u2500\u250022x91 region at (442, 14)\n            \u251c\u2500\u250013x67 region at (780, 0)\n            \u251c\u2500\u250022x100 region at (681, 18)\n            \u251c\u2500\u250021x100 region at (554, 0)\n            \u2514\u2500\u250022x100 region at (667, 6)\n</pre> <p>When the segmentation is done, the segments can be passed to a text recognition model. The results are passed to the workbench in the same manner as before:</p> In\u00a0[8]: Copied! <pre>from htrflow_core.models.dummy_models import RecognitionModel\n\n\nrecognition_model = RecognitionModel()\nresults = recognition_model(volume.segments())\nvolume.update(results)\nprint(volume)\n</pre> from htrflow_core.models.dummy_models import RecognitionModel   recognition_model = RecognitionModel() results = recognition_model(volume.segments()) volume.update(results) print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u2514\u2500\u2500156x406 region at (345, 11)\n    \u2502       \u251c\u2500\u250037x100 region at (517, 129) \"Dolor velit non non tempora magnam ut adipisci.\"\n    \u2502       \u251c\u2500\u250022x100 region at (636, 144) \"Dolor quiquia quisquam adipisci velit velit quiquia quiquia.\"\n    \u2502       \u251c\u2500\u250038x100 region at (543, 125) \"Ipsum labore dolorem ut neque ipsum velit.\"\n    \u2502       \u251c\u2500\u250038x100 region at (486, 122) \"Consectetur est numquam voluptatem quiquia ipsum.\"\n    \u2502       \u2514\u2500\u250038x69 region at (681, 38) \"Magnam etincidunt consectetur neque quaerat ut sit ipsum.\"\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500117x406 region at (17, 0)\n    \u2502   \u2502   \u2514\u2500\u250028x100 region at (216, 70) \"Modi sed non tempora.\"\n    \u2502   \u251c\u2500\u2500156x406 region at (948, 262)\n    \u2502   \u2502   \u251c\u2500\u250033x100 region at (1070, 384) \"Numquam quiquia ut etincidunt sit quaerat adipisci.\"\n    \u2502   \u2502   \u251c\u2500\u250038x87 region at (948, 359) \"Est etincidunt dolore modi.\"\n    \u2502   \u2502   \u2514\u2500\u250038x57 region at (1296, 329) \"Dolore ut tempora numquam voluptatem dolorem etincidunt non.\"\n    \u2502   \u2514\u2500\u2500156x309 region at (0, 85)\n    \u2502       \u251c\u2500\u250038x76 region at (7, 159) \"Numquam amet quisquam magnam modi.\"\n    \u2502       \u251c\u2500\u250038x76 region at (142, 124) \"Dolorem dolorem eius aliquam eius.\"\n    \u2502       \u251c\u2500\u250034x76 region at (218, 85) \"Eius tempora modi sit.\"\n    \u2502       \u251c\u2500\u250038x76 region at (215, 125) \"Tempora labore velit dolor.\"\n    \u2502       \u2514\u2500\u250038x76 region at (52, 105) \"Consectetur neque labore porro quiquia.\"\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500156x406 region at (480, 173)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (623, 272) \"Quaerat sed ipsum tempora.\"\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (498, 270) \"Ipsum aliquam consectetur dolor.\"\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (561, 244) \"Sed magnam aliquam aliquam dolor.\"\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (652, 261) \"Sed dolor amet sed adipisci etincidunt.\"\n    \u2502   \u251c\u2500\u2500156x406 region at (690, 11)\n    \u2502   \u2502   \u251c\u2500\u250038x82 region at (690, 122) \"Voluptatem aliquam aliquam porro amet.\"\n    \u2502   \u2502   \u251c\u2500\u250038x95 region at (690, 13) \"Modi aliquam quiquia etincidunt labore.\"\n    \u2502   \u2502   \u251c\u2500\u250037x54 region at (690, 129) \"Tempora dolore quiquia ipsum neque consectetur tempora.\"\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (919, 95) \"Tempora labore modi ut non.\"\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (805, 59) \"Ut dolorem labore dolore consectetur.\"\n    \u2502   \u251c\u2500\u2500149x406 region at (570, 0)\n    \u2502   \u2502   \u2514\u2500\u250023x71 region at (904, 125) \"Est labore dolor est.\"\n    \u2502   \u251c\u2500\u2500156x332 region at (1296, 381)\n    \u2502   \u2502   \u251c\u2500\u250038x53 region at (1296, 403) \"Neque eius adipisci amet voluptatem consectetur.\"\n    \u2502   \u2502   \u251c\u2500\u250035x82 region at (1469, 381) \"Voluptatem magnam voluptatem labore sed dolore voluptatem.\"\n    \u2502   \u2502   \u2514\u2500\u250038x82 region at (1328, 457) \"Dolore ut magnam voluptatem etincidunt amet adipisci.\"\n    \u2502   \u2514\u2500\u2500156x292 region at (0, 16)\n    \u2502       \u2514\u2500\u250038x65 region at (0, 129) \"Etincidunt etincidunt quiquia porro velit.\"\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u250099x213 region at (1415, 0)\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1426, 71) \"Etincidunt etincidunt dolorem modi dolorem.\"\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1463, 37) \"Neque quaerat dolorem magnam.\"\n    \u2502   \u2502   \u2514\u2500\u250024x52 region at (1525, 31) \"Sed aliquam dolor quisquam numquam.\"\n    \u2502   \u2514\u2500\u2500116x406 region at (678, 509)\n    \u2502       \u251c\u2500\u250028x100 region at (929, 544) \"Velit tempora non quiquia magnam ipsum sed.\"\n    \u2502       \u2514\u2500\u250028x76 region at (1007, 512) \"Dolor sed velit quisquam dolor.\"\n    \u2514\u2500\u2500626x1629 image demo_image\n        \u251c\u2500\u2500156x278 region at (0, 234)\n        \u2502   \u2514\u2500\u250038x68 region at (144, 330) \"Amet adipisci quaerat quiquia sit dolor numquam ut.\"\n        \u251c\u2500\u2500156x406 region at (786, 133)\n        \u2502   \u251c\u2500\u250038x100 region at (891, 223) \"Etincidunt velit ut neque labore quisquam.\"\n        \u2502   \u251c\u2500\u250038x64 region at (786, 154) \"Aliquam labore aliquam quaerat consectetur.\"\n        \u2502   \u251c\u2500\u250038x100 region at (1000, 245) \"Ut non numquam ut.\"\n        \u2502   \u2514\u2500\u250038x100 region at (911, 242) \"Ipsum sed non dolore eius consectetur.\"\n        \u251c\u2500\u2500156x406 region at (1105, 461)\n        \u2502   \u251c\u2500\u250029x100 region at (1170, 587) \"Sed sed magnam tempora velit.\"\n        \u2502   \u251c\u2500\u250038x100 region at (1194, 571) \"Numquam quisquam dolore ut non.\"\n        \u2502   \u2514\u2500\u250038x100 region at (1219, 509) \"Sit amet ipsum neque neque adipisci consectetur.\"\n        \u2514\u2500\u250090x406 region at (442, 0)\n            \u251c\u2500\u250022x91 region at (442, 14) \"Ipsum ut eius sit porro sit.\"\n            \u251c\u2500\u250013x67 region at (780, 0) \"Dolorem voluptatem sed voluptatem non modi quisquam.\"\n            \u251c\u2500\u250022x100 region at (681, 18) \"Sed amet labore dolorem velit aliquam.\"\n            \u251c\u2500\u250021x100 region at (554, 0) \"Sit non amet velit dolorem dolore labore.\"\n            \u2514\u2500\u250022x100 region at (667, 6) \"Dolorem amet amet modi voluptatem.\"\n</pre> In\u00a0[9]: Copied! <pre># Access image 0, region 0, subregion 0\nvolume[0, 0, 0]\n\n# Access image 0, region 0\nvolume[0, 0]\n</pre> # Access image 0, region 0, subregion 0 volume[0, 0, 0]  # Access image 0, region 0 volume[0, 0] Out[9]: <pre>&lt;htrflow_core.volume.RegionNode at 0x7f5a496ef1f0&gt;</pre> <p>The image associated with each node is accessed through the <code>image</code> attribute. The image isn't stored directly in the node, instead, the node refers to the parent image, and crops it according to its box:</p> <pre>class BaseImageNode:\n\n    @property\n    def image(self):\n        x1, x2, y1, y2 = self.box\n        return self.parent.image[y1:y2, x1:x2]\n\n    ...\n</pre> In\u00a0[10]: Copied! <pre>volume[0, 0, 0].image\n</pre> volume[0, 0, 0].image Out[10]: <pre>array([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       ...,\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]]], dtype=uint8)</pre> In\u00a0[11]: Copied! <pre>print(volume[0].coordinate)\n</pre> print(volume[0].coordinate) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 print(volume[0].coordinate)\n\nAttributeError: 'PageNode' object has no attribute 'coordinate'</pre> <p>For first-level regions <code>coordinate</code> is the same as the corner of the segment bounding box.</p> In\u00a0[12]: Copied! <pre>print(\"Coordinate:\", volume[0, 0].coordinate)\nprint(\"Bounding box:\", volume[0, 0].data[\"segment\"].box, \"(x1, x2, y1, y2)\")\n</pre> print(\"Coordinate:\", volume[0, 0].coordinate) print(\"Bounding box:\", volume[0, 0].data[\"segment\"].box, \"(x1, x2, y1, y2)\") <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 print('Coordinate:', volume[0, 0].coordinate)\n      2 print('Bounding box:', volume[0, 0].data['segment'].box, '(x1, x2, y1, y2)')\n\nAttributeError: 'RegionNode' object has no attribute 'coordinate'</pre> <p>But for nested regions the two differ, because <code>coordinate</code> is relative to the original image, while the segment bounding box is relative to the parent region.</p> In\u00a0[13]: Copied! <pre>print(\"Global coordinate:\", volume[0, 0, 0].coordinate)\nprint(\"Local bounding box:\", volume[0, 0, 0].data[\"segment\"].box)\n</pre> print(\"Global coordinate:\", volume[0, 0, 0].coordinate) print(\"Local bounding box:\", volume[0, 0, 0].data[\"segment\"].box) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 print('Global coordinate:', volume[0, 0, 0].coordinate)\n      2 print('Local bounding box:', volume[0, 0, 0].data['segment'].box)\n\nAttributeError: 'RegionNode' object has no attribute 'coordinate'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/demo.html#models-inferencers","title":"Models / inferencers\u00b6","text":"<p>Models &amp; inferencers accept lists of images, and return lists of results (either segmentation or recognition results)</p> <p>I have made a dummy <code>SegmentationModel</code> and <code>RecognitionModel</code> in <code>models.py</code>. These do the same thing as the current inferencers.</p> <pre>class SegmentationModel:\n    def __call__(self, images: list[np.ndarray]) -&gt; list[SegmentationResult]:\n        ...\n\n\n@dataclass\nclass SegmentationResult:\n    boxes: np.ndarray\n    masks: np.ndarray\n    scores: np.ndarray\n    labels: np.ndarray\n</pre> <p>(It would be nice to wrap all models in a \"batching\" function, which divides an input list into chunks if it is too long) -&gt; This is a card in DevOps</p>"},{"location":"notebooks/demo.html#using-the-volume-class","title":"Using the Volume class\u00b6","text":"<p>To load images, create a <code>Volume</code>. The name of this class is not set in stone... It represents what Catrin called a \"batch\", a divison of an archive volume, but I don't want to use \"batch\" because of potential confusion with a model's batch (the number of inputs it operates on simultaneously).</p>"},{"location":"notebooks/demo.html#accessing-nodes","title":"Accessing nodes\u00b6","text":"<p>Specific nodes are accessed by tuple indexing. Here we extract the first line of the first region of the first image:</p>"},{"location":"notebooks/demo.html#coordinates","title":"Coordinates\u00b6","text":"<p>All nodes have a <code>coordinate</code> attribute. This is the location of the node's top-left corner relative to the original image. The base image node's coordinate is thus (0,0):</p>"},{"location":"resources/index.html","title":"Open Source","text":""},{"location":"resources/index.html#hugging-face","title":"Hugging Face","text":""},{"location":"resources/cases.html","title":"Projects","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p>"},{"location":"resources/cases.html#indexing-projects","title":"Indexing projects","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p>"},{"location":"resources/cases.html#seamless-access-to-transcribed-text-swedish-national-archives-universalviewer-customization","title":"Seamless Access to Transcribed Text: Swedish National Archives' UniversalViewer Customization","text":"<p>Work in progress</p> <p>Riksarkivet's UniversalViewer is still under development, with an estimated finish the end of this year.</p> <p>This project aims to make the archival documents held by the Swedish National Archives, transcribed using Htrflow, accessible to everyone in a convenient and effortless way. To achieve this, the National Archives is utilizing the community-developed, open-source media viewer UniversalViewer, capable of displaying images, audio, video, and even 3D versions of digitized and born-digital cultural heritage collections. UniversalViewer is built on the IIIF (International Image Interoperability Framework) standard for interoperable images. By utilizing a IIIF Content Search API endpoint, the National Archives make the transcribed text from Htrflow accessible for users through UniversalViewer. In October 2024, the National Archives are launching a new customized version of UniversalViewer 4 (UV4), which includes a function to search within transcriptions of archival documents displayed in the viewer. This function enables researchers, students, and the general public to interact with the documents held by the National Archives in a new and efficient way \u2013 without needing to decipher old handwriting. Users can search the archival documents and view the image of the original document next to the transcribed text.</p> <ul> <li>The National Archives are contributing these new functions to the main UV4 GitHub repository. Key features of the National Archives UV4 include:</li> <li>Search Pane: A new search pane on the left to search and display hits, with text snippets, within the archival document.</li> <li>Transcribed Text Pane: A new pane on the right displaying the transcribed text next to the image of the original archival document.</li> <li>Highlighting Function: A function that highlights the search hits both in the image and in the text snippets in the search pane.</li> <li>Image Adjustments: Tools for users to adjust image settings (saturation, contrast, light) within the viewer.</li> <li>User-Friendly Features: Options to print, download, and copy files, transcribed text, and metadata in user-friendly ways within the viewer.</li> </ul>"},{"location":"resources/datasets.html","title":"Datasets","text":""},{"location":"resources/datasets.html#datasets","title":"Datasets","text":"Datasets Description Link Svea hovr\u00e4tt Svea hovr\u00e4tt (Renskrivna protokoll) - 1713\u20131735 https://sok.riksarkivet.se/arkiv/H2hpDbNn14scxjzdWqAaJ1 Bergm\u00e4staren Bergm\u00e4staren i Nora m fl bergslag (Hammartingsprotokoll) - 1698\u20131765 https://sok.riksarkivet.se/arkiv/M5Fe2TT9rH6cxG02H087k3 Trolldomskommissionen Trolldomskommissionen - mainly 1670s https://sok.riksarkivet.se/trolldomskommissionen Bergskollegium Bergskollegium - 1718\u20131758 https://sok.riksarkivet.se/arkiv/SMFky31ekQ80Qsk0UCZZE2 J\u00e4mtlands J\u00e4mtlands domsaga - 1647\u20131688 https://sok.riksarkivet.se/arkiv/2l4NYFT8rH6cxG02H087k3 Stockholms domkapitel Stockholms domkapitel - 1728\u20131759 https://sok.riksarkivet.se/arkiv/etg1tyeEaIPMBzKbUKTjw1 Politikollegiet Politikollegiet - 1729\u20131759 https://sok.riksarkivet.se/arkiv/1lQnXIDiKaYxRLBlK1dGF3 Poliskammaren G\u00f6teborgs poliskammare (Detektiva polisens rapportb\u00f6cker) -  1868\u20131901 https://sok.riksarkivet.se/arkiv/oLTOi9yxweZJUG018W43t3 Court Records Renovated Court Records (the National Archives of Finland) - 1800s https://tuomiokirjat.kansallisarkisto.fi/"},{"location":"resources/datasets.html#dataset-contributions","title":"dataset \u2013 Contributions","text":"<p>The AI models used in HTRFLOW is the result of a collaborative effort, involving the National Archives in both Sweden and Finland, in partnership with the Stockholm City Archives, J\u00e4mtlands l\u00e4ns fornskrifts\u00e4llskap, citizen science volunteers and researchers from Stockholm and Uppsala Universities.</p> <p>Several datasets have been created by participants through Citizen Science using the Handwritten Text Recognition (HTR) software, Transkribus, provided by READ-COOP SCE .</p>"},{"location":"resources/datasets.html#ongoing-research-collaborations","title":"Ongoing research collaborations","text":"<p>Transcription node Sweden \u2013 machine interpretation and citizen research combined, Swedish National Archives and University of Gothenburg, funded by the Swedish National Heritage Board.</p> <p>Mapping the geographies of early modern mining knowledge. A digital history of the study tours of the Swedish Bureau of Mines, 1691\u20131826, Uppsala University and Stockholm University, funded by the Swedish Research Council.</p> <p>The Swedish National Archives' research and development on HTR is part of the Swedish national infrastructure Huminfra. Click here for more information.</p>"},{"location":"resources/models.html","title":"Models","text":""},{"location":"resources/models.html#models","title":"Models","text":"<p>The models used in this demo are very much a work in progress, and as more data, and new architectures, becomes available, they will be retrained and reevaluated. For more information about the models, please refer to their model-cards on Huggingface.</p> Model Name Description Task Riksarkivet/rtmdet_regions nan instance segmentation and object detection Riksarkivet/rtmdet_lines nan instance segmentation and object detection Riksarkivet/satrn_htr nan image-to-text"}]}