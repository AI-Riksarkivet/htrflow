{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#htrflow_core","title":"htrflow_core","text":"<p>[!NOTE] This repo is an work in progress \u26a0\ufe0f</p> <p>htrflow_core is a part of the htrflow suite, which is Riksarkivets open source project for handwritten text recogntion.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p>Installation can be done using pypi:</p> <pre><code>pip install htrflow_core\n</code></pre> <p>In order to use different models within htrflow_core, you have to install more dependices. Current possible installations are:</p> <pre><code># Different model frameworks\npip install htrflow_core[huggingface, openmmlab, ultralytics]\n</code></pre>"},{"location":"index.html#usage","title":"Usage","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p>"},{"location":"api/image/image.html","title":"Image","text":"<p>Module containing utilities related to images and geometries</p>"},{"location":"api/image/image.html#htrflow_core.image.Colors","title":"<code>Colors</code>","text":"<p>Color constants in BGR</p> Source code in <code>src/htrflow_core/image.py</code> <pre><code>class Colors:\n    \"\"\"Color constants in BGR\"\"\"\n\n    RED = (0, 0, 255)\n    GREEN = (0, 255, 0)\n    BLUE = (255, 0, 0)\n    WHITE = (255, 255, 255)\n</code></pre>"},{"location":"api/image/image.html#htrflow_core.image.bbox2polygon","title":"<code>bbox2polygon(bbox)</code>","text":"<p>Convert bounding box to polygon</p> Source code in <code>src/htrflow_core/image.py</code> <pre><code>def bbox2polygon(bbox: Bbox) -&gt; Polygon:\n    \"\"\"Convert bounding box to polygon\"\"\"\n    x1, x2, y1, y2 = bbox\n    return np.array([[x1, y1], [x1, y2], [x2, y2], [x2, y1]])\n</code></pre>"},{"location":"api/image/image.html#htrflow_core.image.binarize","title":"<code>binarize(image)</code>","text":"<p>Binarize image</p> Source code in <code>src/htrflow_core/image.py</code> <pre><code>def binarize(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Binarize image\"\"\"\n    # Moved from binarize.py\n    # TODO: Double check color space conversions (other functions in this module operate on BGR)\n    img_ori = cv2.cvtColor(image.astype(\"uint8\"), cv2.COLOR_RGB2BGR)\n    img_gray = cv2.cvtColor(img_ori, cv2.COLOR_BGR2GRAY)\n    dst = cv2.fastNlMeansDenoising(img_gray, h=31, templateWindowSize=7, searchWindowSize=21)\n    img_blur = cv2.medianBlur(dst, 3).astype(\"uint8\")\n    threshold = cv2.adaptiveThreshold(img_blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n    img_binarized = cv2.cvtColor(threshold, cv2.COLOR_BGR2RGB)\n    return img_binarized\n</code></pre>"},{"location":"api/image/image.html#htrflow_core.image.crop","title":"<code>crop(image, bbox)</code>","text":"<p>Crop image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image</p> required <code>bbox</code> <code>Bbox</code> <p>The bounding box</p> required Source code in <code>src/htrflow_core/image.py</code> <pre><code>def crop(image: np.ndarray, bbox: Bbox) -&gt; np.ndarray:\n    \"\"\"Crop image\n\n    Args:\n        image: The input image\n        bbox: The bounding box\n    \"\"\"\n    x1, x2, y1, y2 = bbox\n    return image[y1:y2, x1:x2]\n</code></pre>"},{"location":"api/image/image.html#htrflow_core.image.draw_bboxes","title":"<code>draw_bboxes(image, bboxes, color=Colors.BLUE, thickness=3, alpha=0.2)</code>","text":"<p>Draw bounding boxes on image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image</p> required <code>bboxes</code> <code>Iterable[Bbox]</code> <p>List of bounding boxes to draw</p> required <code>color</code> <code>Color</code> <p>Box border color</p> <code>BLUE</code> <code>thickness</code> <code>int</code> <p>Box border thickness</p> <code>3</code> <p>Returns:     A copy of the input image with the bounding boxes drawn.</p> Source code in <code>src/htrflow_core/image.py</code> <pre><code>def draw_bboxes(\n    image: np.ndarray,\n    bboxes: Iterable[Bbox],\n    color: Color = Colors.BLUE,\n    thickness: int = 3,\n    alpha: float = 0.2,\n) -&gt; np.ndarray:\n    \"\"\"Draw bounding boxes on image\n\n    Args:\n        image: The input image\n        bboxes: List of bounding boxes to draw\n        color: Box border color\n        thickness: Box border thickness\n    Returns:\n        A copy of the input image with the bounding boxes drawn.\n    \"\"\"\n    polygons = [bbox2polygon(bbox) for bbox in bboxes]\n    return draw_polygons(image, polygons, color, thickness, alpha)\n</code></pre>"},{"location":"api/image/image.html#htrflow_core.image.draw_masks","title":"<code>draw_masks(image, masks, color=Colors.BLUE, alpha=0.2)</code>","text":"<p>Draw masks on image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image</p> required <code>masks</code> <code>Iterable[Mask]</code> <p>The masks</p> required <code>color</code> <code>Color</code> <p>Mask color</p> <code>BLUE</code> <code>alpha</code> <code>float</code> <p>Mask opacity</p> <code>0.2</code> <p>Returns:     A copy of the input image with the masked areas colored.</p> Source code in <code>src/htrflow_core/image.py</code> <pre><code>def draw_masks(\n    image: np.ndarray,\n    masks: Iterable[Mask],\n    color: Color = Colors.BLUE,\n    alpha: float = 0.2,\n) -&gt; np.ndarray:\n    \"\"\"Draw masks on image\n\n    Args:\n        image: The input image\n        masks: The masks\n        color: Mask color\n        alpha: Mask opacity\n    Returns:\n        A copy of the input image with the masked areas colored.\n    \"\"\"\n    for mask_ in masks:\n        masked = mask(image, mask_, inverse=True, fill=color)\n        image = image * (1 - alpha) + masked * alpha\n    return image\n</code></pre>"},{"location":"api/image/image.html#htrflow_core.image.draw_polygons","title":"<code>draw_polygons(image, polygons, color=Colors.BLUE, thickness=3, alpha=0.2)</code>","text":"<p>Draw polygons on image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image</p> required <code>polygons</code> <code>Iterable[Polygon]</code> <p>The polygons</p> required <code>color</code> <code>Color</code> <p>Fill and border color</p> <code>BLUE</code> <code>alpha</code> <code>float</code> <p>Opacity of the fill</p> <code>0.2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A copy of the input image with polygons drawn on it.</p> Source code in <code>src/htrflow_core/image.py</code> <pre><code>def draw_polygons(\n    image: np.ndarray,\n    polygons: Iterable[Polygon],\n    color: Color = Colors.BLUE,\n    thickness: int = 3,\n    alpha: float = 0.2,\n) -&gt; np.ndarray:\n    \"\"\"Draw polygons on image\n\n    Args:\n        image: The input image\n        polygons: The polygons\n        color: Fill and border color\n        alpha: Opacity of the fill\n\n    Returns:\n        A copy of the input image with polygons drawn on it.\n    \"\"\"\n    image = image.copy()\n    cv2.polylines(image, polygons, isClosed=True, color=color, thickness=thickness)\n    if alpha &gt; 0:\n        for polygon in polygons:\n            filled = cv2.fillPoly(image.copy(), [polygon], color=color)\n            image = image * (1 - alpha) + filled * alpha\n    return image\n</code></pre>"},{"location":"api/image/image.html#htrflow_core.image.mask","title":"<code>mask(image, mask, fill=Colors.WHITE, inverse=False)</code>","text":"<p>Apply mask to image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image</p> required <code>mask</code> <code>Mask</code> <p>The mask, a binary array, of the same shape as <code>image</code></p> required <code>fill</code> <code>Color</code> <p>The color value to fill the masked areas with, default white</p> <code>WHITE</code> <code>inverse</code> <code>bool</code> <p>Invert the mask before applying</p> <code>False</code> Source code in <code>src/htrflow_core/image.py</code> <pre><code>def mask(\n    image: np.ndarray,\n    mask: Mask,\n    fill: Color = Colors.WHITE,\n    inverse: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Apply mask to image\n\n    Args:\n        image: The input image\n        mask: The mask, a binary array, of the same shape as `image`\n        fill: The color value to fill the masked areas with, default white\n        inverse: Invert the mask before applying\n    \"\"\"\n    image = image.copy()\n    idx = (mask != 0) if inverse else (mask == 0)\n    image[idx] = fill\n    return image\n</code></pre>"},{"location":"api/image/image.html#htrflow_core.image.mask2bbox","title":"<code>mask2bbox(mask)</code>","text":"<p>Convert mask to bounding box</p> Source code in <code>src/htrflow_core/image.py</code> <pre><code>def mask2bbox(mask: Mask) -&gt; Bbox:\n    \"\"\"Convert mask to bounding box\"\"\"\n    y, x = np.where(mask != 0)\n    return np.min(x), np.max(x), np.min(y), np.max(y)\n</code></pre>"},{"location":"api/image/image.html#htrflow_core.image.mask2polygon","title":"<code>mask2polygon(mask, epsilon=0.01)</code>","text":"<p>Convert mask to polygon</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Mask</code> <p>The input mask</p> required <code>epsilon</code> <code>float</code> <p>A tolerance parameter. Smaller epsilon will result in a higher-resolution polygon.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Polygon</code> <p>A list of coordinate tuples</p> Source code in <code>src/htrflow_core/image.py</code> <pre><code>def mask2polygon(mask: Mask, epsilon: float = 0.01) -&gt; Polygon:\n    \"\"\"Convert mask to polygon\n\n    Args:\n        mask: The input mask\n        epsilon: A tolerance parameter. Smaller epsilon will result in\n            a higher-resolution polygon.\n\n    Returns:\n        A list of coordinate tuples\n    \"\"\"\n    contours, _ = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n\n    # `contours` is a high-resolution contour, but we need a simple polygon, so\n    # we approximate it with the DP algorithm, which removes as many points as\n    # possible while still keeping the original shape.\n\n    # Adjust the tolerance parameter `epsilon` relative to the size of the mask\n    epsilon *= cv2.arcLength(contours[0], closed=True)\n    approx = cv2.approxPolyDP(contours[0], epsilon, closed=True)\n    return np.squeeze(approx)\n</code></pre>"},{"location":"api/image/image.html#htrflow_core.image.masks2polygons","title":"<code>masks2polygons(masks, epsilon=0.01)</code>","text":"<p>Convert masks to polygons</p> Source code in <code>src/htrflow_core/image.py</code> <pre><code>def masks2polygons(masks: Iterable[Mask], epsilon=0.01) -&gt; Iterable[Polygon]:\n    \"\"\"Convert masks to polygons\"\"\"\n    return [mask2polygon(mask, epsilon) for mask in masks]\n</code></pre>"},{"location":"api/results/results.html","title":"Results","text":""},{"location":"api/results/results.html#htrflow_core.results.RecognizedText","title":"<code>RecognizedText</code>  <code>dataclass</code>","text":"<p>Recognized text class</p> <p>This class represents a result from a text recognition model.</p> <p>Attributes:</p> Name Type Description <code>texts</code> <code>Sequence[str]</code> <p>A sequence of candidate texts</p> <code>scores</code> <code>Sequence[float]</code> <p>The scores of the candidate texts</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@dataclass\nclass RecognizedText:\n    \"\"\"Recognized text class\n\n    This class represents a result from a text recognition model.\n\n    Attributes:\n        texts: A sequence of candidate texts\n        scores: The scores of the candidate texts\n    \"\"\"\n    texts: Sequence[str]\n    scores: Sequence[float]\n\n    def top_candidate(self):\n        \"\"\"The best candidate text\"\"\"\n        return self.texts[self.scores.index(self.top_score())]\n\n    def top_score(self):\n        \"\"\"The highest score\"\"\"\n        return max(self.scores)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.RecognizedText.top_candidate","title":"<code>top_candidate()</code>","text":"<p>The best candidate text</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def top_candidate(self):\n    \"\"\"The best candidate text\"\"\"\n    return self.texts[self.scores.index(self.top_score())]\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.RecognizedText.top_score","title":"<code>top_score()</code>","text":"<p>The highest score</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def top_score(self):\n    \"\"\"The highest score\"\"\"\n    return max(self.scores)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result","title":"<code>Result</code>  <code>dataclass</code>","text":"<p>Result class</p> <p>This class bundles segmentation and text recognition results</p> <p>Returns:</p> Name Type Description <code>image</code> <p>The original imaage</p> <code>metadata</code> <p>Metadata associated with the result</p> <code>segments</code> <p>Segments (may be empty)</p> <code>texts</code> <p>Texts (may be empty)</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@dataclass\nclass Result:\n    \"\"\"Result class\n\n    This class bundles segmentation and text recognition results\n\n    Returns:\n        image: The original imaage\n        metadata: Metadata associated with the result\n        segments: Segments (may be empty)\n        texts: Texts (may be empty)\n    \"\"\"\n\n    image: np.ndarray\n    metadata: dict\n    segments: Sequence[Segment] = field(default_factory=list)\n    texts: Sequence[RecognizedText] = field(default_factory=list)\n\n    @classmethod\n    def text_recognition_result(cls, image: np.ndarray, metadata: dict, text: RecognizedText) -&gt; \"Result\":\n        \"\"\"Create a text recognition result\n\n        Arguments:\n            image: The original image\n            metadata: Result metadata\n            text: The recognized text\n\n        Returns:\n            A Result instance with the specified data and no segments.\n        \"\"\"\n        return cls(image, metadata, texts=[text])\n\n    @classmethod\n    def segmentation_result(cls, image: np.ndarray, metadata: dict, segments: Sequence[Segment]) -&gt; \"Result\":\n        \"\"\"Create a segmentation result\n\n        Arguments:\n            image: The original image\n            metadata: Result metadata\n            segments: The segments\n\n        Returns:\n            A Result instance with the specified data and no texts.\n        \"\"\"\n        return cls(image, metadata, segments=segments)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.segmentation_result","title":"<code>segmentation_result(image, metadata, segments)</code>  <code>classmethod</code>","text":"<p>Create a segmentation result</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The original image</p> required <code>metadata</code> <code>dict</code> <p>Result metadata</p> required <code>segments</code> <code>Sequence[Segment]</code> <p>The segments</p> required <p>Returns:</p> Type Description <code>Result</code> <p>A Result instance with the specified data and no texts.</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@classmethod\ndef segmentation_result(cls, image: np.ndarray, metadata: dict, segments: Sequence[Segment]) -&gt; \"Result\":\n    \"\"\"Create a segmentation result\n\n    Arguments:\n        image: The original image\n        metadata: Result metadata\n        segments: The segments\n\n    Returns:\n        A Result instance with the specified data and no texts.\n    \"\"\"\n    return cls(image, metadata, segments=segments)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.text_recognition_result","title":"<code>text_recognition_result(image, metadata, text)</code>  <code>classmethod</code>","text":"<p>Create a text recognition result</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The original image</p> required <code>metadata</code> <code>dict</code> <p>Result metadata</p> required <code>text</code> <code>RecognizedText</code> <p>The recognized text</p> required <p>Returns:</p> Type Description <code>Result</code> <p>A Result instance with the specified data and no segments.</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@classmethod\ndef text_recognition_result(cls, image: np.ndarray, metadata: dict, text: RecognizedText) -&gt; \"Result\":\n    \"\"\"Create a text recognition result\n\n    Arguments:\n        image: The original image\n        metadata: Result metadata\n        text: The recognized text\n\n    Returns:\n        A Result instance with the specified data and no segments.\n    \"\"\"\n    return cls(image, metadata, texts=[text])\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment","title":"<code>Segment</code>  <code>dataclass</code>","text":"<p>Segment class</p> <p>Attributes:</p> Name Type Description <code>bbox</code> <code>tuple[int, int, int, int]</code> <p>The segment's bounding box as a tuple of coordinates (x1, x2, y1, y2)</p> <code>mask</code> <code>Optional[ndarray]</code> <p>The segment's mask, if available. The mask is relative to the bounding box.</p> <code>score</code> <code>Optional[float]</code> <p>Segment confidence score</p> <code>class_label</code> <code>Optional[str]</code> <p>Segment label, if available</p> <code>polygon</code> <code>list[tuple]</code> <p>An approximation of the segment mask, !relative to the parent!</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@dataclass\nclass Segment:\n    \"\"\"Segment class\n\n    Attributes:\n        bbox: The segment's bounding box as a tuple of coordinates (x1, x2, y1, y2)\n        mask: The segment's mask, if available. The mask is relative to the bounding box.\n        score: Segment confidence score\n        class_label: Segment label, if available\n        polygon: An approximation of the segment mask, !relative to the parent!\n    \"\"\"\n\n    bbox: tuple[int, int, int, int]\n    mask: Optional[np.ndarray]\n    polygon: list[tuple] = None\n    score: Optional[float] = None\n    class_label: Optional[str] = None\n    # baseline: list[tuple] ?\n\n    @classmethod\n    def from_bbox(cls, bbox, **kwargs):\n        \"\"\"Create a segment from a bounding box\"\"\"\n        mask = None\n        polygon = image.bbox2polygon(bbox)\n        return cls(bbox, mask, polygon, **kwargs)\n\n    @classmethod\n    def from_mask(cls, mask, **kwargs):\n        \"\"\"Create a segment from a mask\n\n        Args:\n            mask: A binary mask, of same shape as original image.\n        \"\"\"\n        bbox = image.mask2bbox(mask)\n        polygon = image.mask2polygon(mask)\n        cropped_mask = image.crop(mask, bbox)\n        return cls(bbox, cropped_mask, polygon, **kwargs)\n\n    @classmethod\n    def from_baseline(cls, baseline, **kwargs):\n        \"\"\"Create a segment from a baseline\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment.from_baseline","title":"<code>from_baseline(baseline, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a segment from a baseline</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@classmethod\ndef from_baseline(cls, baseline, **kwargs):\n    \"\"\"Create a segment from a baseline\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment.from_bbox","title":"<code>from_bbox(bbox, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a segment from a bounding box</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@classmethod\ndef from_bbox(cls, bbox, **kwargs):\n    \"\"\"Create a segment from a bounding box\"\"\"\n    mask = None\n    polygon = image.bbox2polygon(bbox)\n    return cls(bbox, mask, polygon, **kwargs)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment.from_mask","title":"<code>from_mask(mask, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a segment from a mask</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <p>A binary mask, of same shape as original image.</p> required Source code in <code>src/htrflow_core/results.py</code> <pre><code>@classmethod\ndef from_mask(cls, mask, **kwargs):\n    \"\"\"Create a segment from a mask\n\n    Args:\n        mask: A binary mask, of same shape as original image.\n    \"\"\"\n    bbox = image.mask2bbox(mask)\n    polygon = image.mask2polygon(mask)\n    cropped_mask = image.crop(mask, bbox)\n    return cls(bbox, cropped_mask, polygon, **kwargs)\n</code></pre>"},{"location":"api/volume/node.html","title":"Noe","text":"<p>Node class</p> Source code in <code>src/htrflow_core/volume.py</code> <pre><code>class Node:\n    \"\"\"Node class\"\"\"\n\n    parent: Optional[\"Node\"]\n    children: Sequence[\"Node\"]\n    depth: int\n\n    def __init__(self, parent: Optional[\"Node\"] = None):\n        self.parent = parent\n        self.children = []\n        self.depth = parent.depth + 1 if parent else 0\n\n    def __getitem__(self, i):\n        if isinstance(i, int):\n            return self.children[i]\n        i, *rest = i\n        return self.children[i][rest] if rest else self.children[i]\n\n    def leaves(self):  # -&gt; Sequence[Self]:\n        \"\"\"Return the leaf nodes attached to this node\"\"\"\n        nodes = [] if self.children else [self]\n        for child in self.children:\n            nodes.extend(child.leaves())\n        return nodes\n\n    def traverse(self, filter: Optional[Callable[[\"Node\"], bool]] = None) -&gt; Sequence[\"Node\"]:\n        \"\"\"Return all nodes attached to this node\"\"\"\n        nodes = [self] if (filter is None or filter(self)) else []\n        for child in self.children:\n            nodes.extend(child.traverse(filter=filter))\n        return nodes\n\n    def tree2str(self, sep: str = \"\", is_last: bool = True) -&gt; str:\n        \"\"\"Return a string representation of this node and its decendants\"\"\"\n        lines = [sep + (\"\u2514\u2500\u2500\" if is_last else \"\u251c\u2500\u2500\") + str(self)]\n        sep += \"    \" if is_last else \"\u2502   \"\n        for child in self.children:\n            lines.append(child.tree2str(sep, child == self.children[-1]))\n        return \"\\n\".join(lines).strip(\"\u2514\u2500\u2500\")\n\n    def is_leaf(self) -&gt; bool:\n        return not self.children\n</code></pre>"},{"location":"api/volume/node.html#htrflow_core.volume.Node.leaves","title":"<code>leaves()</code>","text":"<p>Return the leaf nodes attached to this node</p> Source code in <code>src/htrflow_core/volume.py</code> <pre><code>def leaves(self):  # -&gt; Sequence[Self]:\n    \"\"\"Return the leaf nodes attached to this node\"\"\"\n    nodes = [] if self.children else [self]\n    for child in self.children:\n        nodes.extend(child.leaves())\n    return nodes\n</code></pre>"},{"location":"api/volume/node.html#htrflow_core.volume.Node.traverse","title":"<code>traverse(filter=None)</code>","text":"<p>Return all nodes attached to this node</p> Source code in <code>src/htrflow_core/volume.py</code> <pre><code>def traverse(self, filter: Optional[Callable[[\"Node\"], bool]] = None) -&gt; Sequence[\"Node\"]:\n    \"\"\"Return all nodes attached to this node\"\"\"\n    nodes = [self] if (filter is None or filter(self)) else []\n    for child in self.children:\n        nodes.extend(child.traverse(filter=filter))\n    return nodes\n</code></pre>"},{"location":"api/volume/node.html#htrflow_core.volume.Node.tree2str","title":"<code>tree2str(sep='', is_last=True)</code>","text":"<p>Return a string representation of this node and its decendants</p> Source code in <code>src/htrflow_core/volume.py</code> <pre><code>def tree2str(self, sep: str = \"\", is_last: bool = True) -&gt; str:\n    \"\"\"Return a string representation of this node and its decendants\"\"\"\n    lines = [sep + (\"\u2514\u2500\u2500\" if is_last else \"\u251c\u2500\u2500\") + str(self)]\n    sep += \"    \" if is_last else \"\u2502   \"\n    for child in self.children:\n        lines.append(child.tree2str(sep, child == self.children[-1]))\n    return \"\\n\".join(lines).strip(\"\u2514\u2500\u2500\")\n</code></pre>"},{"location":"api/volume/volume.html","title":"Volume","text":"<p>Class representing a collection of input images</p> <p>Examples:</p> <pre><code>from htrflow_core.volume import Volume\n\nimages = ['../assets/demo_image.jpg'] * 5\n\nvolume = Volume(images)\n</code></pre> Source code in <code>src/htrflow_core/volume.py</code> <pre><code>class Volume:\n\n    \"\"\"Class representing a collection of input images\n\n    Examples:\n\n    ```python\n    from htrflow_core.volume import Volume\n\n    images = ['../assets/demo_image.jpg'] * 5\n\n    volume = Volume(images)\n    ```\n\n    \"\"\"\n\n    def __init__(self, paths: Iterable[str], label: str = \"untitled_volume\"):\n        \"\"\"Initialize volume\n\n        Arguments:\n            paths: A list of paths to images\n            label: A label describing the volume (optional)\n        \"\"\"\n        self.pages = [PageNode(path) for path in paths]\n        self.label = label\n\n    @classmethod\n    def from_directory(cls, path: str) -&gt; \"Volume\":\n        \"\"\"Initialize a volume from a directory\n\n        Sets the volume label to the directory name.\n\n        Arguments:\n            path: A path to a directory of images.\n        \"\"\"\n        files = (os.path.join(path, file) for file in os.listdir(path))\n        label = os.path.basename(path)\n        return cls(files, label)\n\n    @classmethod\n    def from_pickle(cls, path: str) -&gt; \"Volume\":\n        \"\"\"Initialize a volume from a pickle file\n\n        Arguments:\n            path: A path to a previously pickled volume instance\n        \"\"\"\n        with open(path, \"rb\") as f:\n            vol = pickle.load(f)\n        return vol\n\n    def pickle(self, directory: str = \".cache\", filename: Optional[str] = None):\n        \"\"\"Pickle volume\n\n        Arguments:\n            directory: Where to save the pickle file\n            filename: Name of pickle file, optional. Defaults to\n                \"volume_{volume label}.pickle\" if left as None\n\n        Returns:\n            The path to the pickled file.\n        \"\"\"\n        os.makedirs(directory, exist_ok=True)\n        filename = f\"volume_{self.label}.pickle\" if filename is None else filename\n        path = os.path.join(directory, filename)\n        with open(path, \"wb\") as f:\n            pickle.dump(self, f)\n        return path\n\n    def __getitem__(self, i):\n        if isinstance(i, Iterable):\n            i, *rest = i\n            return self.pages[i].__getitem__(rest)\n        return self.pages[i]\n\n    def __iter__(self):\n        return self.pages.__iter__()\n\n    def __str__(self):\n        return f\"Volume label: {self.label}\\nVolume tree:\\n\" + \"\\n\".join(page.tree2str() for page in self.pages)\n\n    def images(self):  # -&gt; Generator[np.ndarray]:\n        \"\"\"Yields the volume's original input images\"\"\"\n        for page in self.pages:\n            yield page.image\n\n    def leaves(self):  # -&gt; Iterable[BaseDocumentNode]:\n        return chain.from_iterable(page.leaves() for page in self.pages)\n\n    def traverse(self):\n        return chain.from_iterable(page.traverse() for page in self.pages)\n\n    def segments(self, depth: Optional[int] = None):  # -&gt; Iterable[np.ndarray]:\n        \"\"\"Yields the volume's segments at `depth`\n\n        Args:\n            depth (int | None): Which depth segments to yield. Defaults to None, which\n                returns the leaf nodes (maximum depth).\n        \"\"\"\n\n        if depth is None:\n            for node in self.leaves():\n                yield node.image\n        else:\n            for page in self.pages:\n                for node in page.traverse(lambda node: node.depth == depth):\n                    yield node.image\n\n    def update(self, results: list[Result]) -&gt; None:\n        \"\"\"\n        Update the volume with model results\n\n        Arguments:\n            results: A list of results where the i:th result corresponds\n                to the volume's i:th leaf node.\n        \"\"\"\n        leaves = list(self.leaves())\n        if len(leaves) != len(results):\n            raise ValueError(f\"Size of input ({len(results)}) does not match \" f\"the size of the tree ({len(leaves)})\")\n\n        # Update the leaves of the tree\n        for leaf, result in zip(leaves, results):\n            # If the result has segments, segment the leaf\n            if result.segments:\n                leaf.segment(result.segments)\n\n            # If the result has texts, add them to the new leaves (which\n            # may be other than `leaves` if the result also had a segmentation)\n            if result.texts:\n                for new_leaf, text in zip(leaf.leaves(), result.texts):\n                    new_leaf.add_text(text)\n\n    def save(self, directory: str = \"outputs\", format_: Literal[\"alto\", \"page\", \"txt\"] = \"alto\") -&gt; None:\n        \"\"\"Save volume\n\n        Arguments:\n            directory: Output directory\n            format_: Output format\n        \"\"\"\n        serialization.save_volume(self, format_, directory)\n</code></pre>"},{"location":"api/volume/volume.html#htrflow_core.volume.Volume.__init__","title":"<code>__init__(paths, label='untitled_volume')</code>","text":"<p>Initialize volume</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Iterable[str]</code> <p>A list of paths to images</p> required <code>label</code> <code>str</code> <p>A label describing the volume (optional)</p> <code>'untitled_volume'</code> Source code in <code>src/htrflow_core/volume.py</code> <pre><code>def __init__(self, paths: Iterable[str], label: str = \"untitled_volume\"):\n    \"\"\"Initialize volume\n\n    Arguments:\n        paths: A list of paths to images\n        label: A label describing the volume (optional)\n    \"\"\"\n    self.pages = [PageNode(path) for path in paths]\n    self.label = label\n</code></pre>"},{"location":"api/volume/volume.html#htrflow_core.volume.Volume.from_directory","title":"<code>from_directory(path)</code>  <code>classmethod</code>","text":"<p>Initialize a volume from a directory</p> <p>Sets the volume label to the directory name.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>A path to a directory of images.</p> required Source code in <code>src/htrflow_core/volume.py</code> <pre><code>@classmethod\ndef from_directory(cls, path: str) -&gt; \"Volume\":\n    \"\"\"Initialize a volume from a directory\n\n    Sets the volume label to the directory name.\n\n    Arguments:\n        path: A path to a directory of images.\n    \"\"\"\n    files = (os.path.join(path, file) for file in os.listdir(path))\n    label = os.path.basename(path)\n    return cls(files, label)\n</code></pre>"},{"location":"api/volume/volume.html#htrflow_core.volume.Volume.from_pickle","title":"<code>from_pickle(path)</code>  <code>classmethod</code>","text":"<p>Initialize a volume from a pickle file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>A path to a previously pickled volume instance</p> required Source code in <code>src/htrflow_core/volume.py</code> <pre><code>@classmethod\ndef from_pickle(cls, path: str) -&gt; \"Volume\":\n    \"\"\"Initialize a volume from a pickle file\n\n    Arguments:\n        path: A path to a previously pickled volume instance\n    \"\"\"\n    with open(path, \"rb\") as f:\n        vol = pickle.load(f)\n    return vol\n</code></pre>"},{"location":"api/volume/volume.html#htrflow_core.volume.Volume.images","title":"<code>images()</code>","text":"<p>Yields the volume's original input images</p> Source code in <code>src/htrflow_core/volume.py</code> <pre><code>def images(self):  # -&gt; Generator[np.ndarray]:\n    \"\"\"Yields the volume's original input images\"\"\"\n    for page in self.pages:\n        yield page.image\n</code></pre>"},{"location":"api/volume/volume.html#htrflow_core.volume.Volume.pickle","title":"<code>pickle(directory='.cache', filename=None)</code>","text":"<p>Pickle volume</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Where to save the pickle file</p> <code>'.cache'</code> <code>filename</code> <code>Optional[str]</code> <p>Name of pickle file, optional. Defaults to \"volume_{volume label}.pickle\" if left as None</p> <code>None</code> <p>Returns:</p> Type Description <p>The path to the pickled file.</p> Source code in <code>src/htrflow_core/volume.py</code> <pre><code>def pickle(self, directory: str = \".cache\", filename: Optional[str] = None):\n    \"\"\"Pickle volume\n\n    Arguments:\n        directory: Where to save the pickle file\n        filename: Name of pickle file, optional. Defaults to\n            \"volume_{volume label}.pickle\" if left as None\n\n    Returns:\n        The path to the pickled file.\n    \"\"\"\n    os.makedirs(directory, exist_ok=True)\n    filename = f\"volume_{self.label}.pickle\" if filename is None else filename\n    path = os.path.join(directory, filename)\n    with open(path, \"wb\") as f:\n        pickle.dump(self, f)\n    return path\n</code></pre>"},{"location":"api/volume/volume.html#htrflow_core.volume.Volume.save","title":"<code>save(directory='outputs', format_='alto')</code>","text":"<p>Save volume</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Output directory</p> <code>'outputs'</code> <code>format_</code> <code>Literal['alto', 'page', 'txt']</code> <p>Output format</p> <code>'alto'</code> Source code in <code>src/htrflow_core/volume.py</code> <pre><code>def save(self, directory: str = \"outputs\", format_: Literal[\"alto\", \"page\", \"txt\"] = \"alto\") -&gt; None:\n    \"\"\"Save volume\n\n    Arguments:\n        directory: Output directory\n        format_: Output format\n    \"\"\"\n    serialization.save_volume(self, format_, directory)\n</code></pre>"},{"location":"api/volume/volume.html#htrflow_core.volume.Volume.segments","title":"<code>segments(depth=None)</code>","text":"<p>Yields the volume's segments at <code>depth</code></p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int | None</code> <p>Which depth segments to yield. Defaults to None, which returns the leaf nodes (maximum depth).</p> <code>None</code> Source code in <code>src/htrflow_core/volume.py</code> <pre><code>def segments(self, depth: Optional[int] = None):  # -&gt; Iterable[np.ndarray]:\n    \"\"\"Yields the volume's segments at `depth`\n\n    Args:\n        depth (int | None): Which depth segments to yield. Defaults to None, which\n            returns the leaf nodes (maximum depth).\n    \"\"\"\n\n    if depth is None:\n        for node in self.leaves():\n            yield node.image\n    else:\n        for page in self.pages:\n            for node in page.traverse(lambda node: node.depth == depth):\n                yield node.image\n</code></pre>"},{"location":"api/volume/volume.html#htrflow_core.volume.Volume.update","title":"<code>update(results)</code>","text":"<p>Update the volume with model results</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[Result]</code> <p>A list of results where the i:th result corresponds to the volume's i:th leaf node.</p> required Source code in <code>src/htrflow_core/volume.py</code> <pre><code>def update(self, results: list[Result]) -&gt; None:\n    \"\"\"\n    Update the volume with model results\n\n    Arguments:\n        results: A list of results where the i:th result corresponds\n            to the volume's i:th leaf node.\n    \"\"\"\n    leaves = list(self.leaves())\n    if len(leaves) != len(results):\n        raise ValueError(f\"Size of input ({len(results)}) does not match \" f\"the size of the tree ({len(leaves)})\")\n\n    # Update the leaves of the tree\n    for leaf, result in zip(leaves, results):\n        # If the result has segments, segment the leaf\n        if result.segments:\n            leaf.segment(result.segments)\n\n        # If the result has texts, add them to the new leaves (which\n        # may be other than `leaves` if the result also had a segmentation)\n        if result.texts:\n            for new_leaf, text in zip(leaf.leaves(), result.texts):\n                new_leaf.add_text(text)\n</code></pre>"},{"location":"getting_started/data_structure.html","title":"Data structure","text":""},{"location":"getting_started/pipeline.html","title":"Pipeline","text":""},{"location":"getting_started/quick_start.html","title":"Quickstart","text":""},{"location":"getting_started/serialization.html","title":"Serialization","text":""},{"location":"getting_started/usage_of_model.html","title":"Usage of models","text":""},{"location":"notebooks/demo.html","title":"The internals","text":"In\u00a0[1]: Copied! <pre>import random\nrandom.seed(123)\n</pre> import random random.seed(123) In\u00a0[2]: Copied! <pre>from htrflow_core.volume import Volume\n\nimages = ['../assets/demo_image.jpg'] * 5 \n\nvolume = Volume(images)\n</pre> from htrflow_core.volume import Volume  images = ['../assets/demo_image.jpg'] * 5   volume = Volume(images) <p>The <code>Volume</code> instance holds a tree. We see the root <code>node</code> and its five children, each representing one input image:</p> In\u00a0[3]: Copied! <pre>print(volume)\n</pre> print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2514\u2500\u2500626x1629 image demo_image\n</pre> <p>The images are available through <code>volume.images()</code>. We pass them through a segmentation model:</p> In\u00a0[4]: Copied! <pre>from htrflow_core.models.dummy_models import SegmentationModel\n\nmodel = SegmentationModel()\nresults = model(volume.images())\nprint(results[0])\n</pre> from htrflow_core.models.dummy_models import SegmentationModel  model = SegmentationModel() results = model(volume.images()) print(results[0]) <pre>SegmentationResult(metadata={'model_name': 'SegmentationModel'}, image=array([[[118, 120, 128],\n        [115, 117, 125],\n        [114, 116, 124],\n        ...,\n        [215, 219, 220],\n        [209, 213, 214],\n        [206, 210, 211]],\n\n       [[110, 112, 120],\n        [110, 112, 120],\n        [110, 112, 120],\n        ...,\n        [211, 215, 216],\n        [207, 211, 212],\n        [209, 213, 214]],\n\n       [[109, 112, 120],\n        [109, 112, 120],\n        [104, 107, 115],\n        ...,\n        [207, 211, 212],\n        [205, 209, 210],\n        [209, 213, 214]],\n\n       ...,\n\n       [[146, 152, 151],\n        [147, 153, 152],\n        [147, 153, 152],\n        ...,\n        [212, 218, 213],\n        [214, 222, 211],\n        [211, 221, 204]],\n\n       [[144, 150, 149],\n        [146, 152, 151],\n        [148, 154, 153],\n        ...,\n        [217, 223, 212],\n        [220, 231, 205],\n        [216, 234, 187]],\n\n       [[147, 153, 152],\n        [149, 155, 154],\n        [151, 157, 156],\n        ...,\n        [214, 221, 208],\n        [214, 228, 194],\n        [208, 231, 169]]], dtype=uint8), segments=[Segment(bbox=(345, 751, 11, 167), mask=array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), polygon=array([[345,  85],\n       [356, 115],\n       [393, 140],\n       [527, 167],\n       [672, 151],\n       [726, 127],\n       [751,  93],\n       [740,  63],\n       [703,  38],\n       [570,  11],\n       [417,  29],\n       [365,  55]], dtype=int32), score=0.7689563885870707, class_label='region')])\n</pre> <p>The results are a list of <code>SegmentationResult</code>. To apply the results to the input images, we pass them back to the volume with its <code>update</code> method. It returns the new regions as a list of images.</p> In\u00a0[5]: Copied! <pre>regions = volume.update(results)\n</pre> regions = volume.update(results) <p>The volume tree has now grown:</p> In\u00a0[6]: Copied! <pre>print(volume)\n</pre> print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u2514\u2500\u2500156x406 region at (345, 11)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500117x406 region at (17, 0)\n    \u2502   \u251c\u2500\u2500156x406 region at (948, 262)\n    \u2502   \u2514\u2500\u2500156x309 region at (0, 85)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500156x406 region at (480, 173)\n    \u2502   \u251c\u2500\u2500156x406 region at (690, 11)\n    \u2502   \u251c\u2500\u2500149x406 region at (570, 0)\n    \u2502   \u251c\u2500\u2500156x332 region at (1296, 381)\n    \u2502   \u2514\u2500\u2500156x292 region at (0, 16)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u250099x213 region at (1415, 0)\n    \u2502   \u2514\u2500\u2500116x406 region at (678, 509)\n    \u2514\u2500\u2500626x1629 image demo_image\n        \u251c\u2500\u2500156x278 region at (0, 234)\n        \u251c\u2500\u2500156x406 region at (786, 133)\n        \u251c\u2500\u2500156x406 region at (1105, 461)\n        \u2514\u2500\u250090x406 region at (442, 0)\n</pre> <p>The new regions can be passed through a segmentation model (such as a line model) again. The <code>update</code> method always updates the leaves of the tree.</p> In\u00a0[7]: Copied! <pre>results = model(volume.segments())\nvolume.update(results)\nprint(volume)\n</pre> results = model(volume.segments()) volume.update(results) print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u2514\u2500\u2500156x406 region at (345, 11)\n    \u2502       \u251c\u2500\u250037x100 region at (517, 129)\n    \u2502       \u251c\u2500\u250022x100 region at (636, 144)\n    \u2502       \u251c\u2500\u250038x100 region at (543, 125)\n    \u2502       \u251c\u2500\u250038x100 region at (486, 122)\n    \u2502       \u2514\u2500\u250038x69 region at (681, 38)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500117x406 region at (17, 0)\n    \u2502   \u2502   \u2514\u2500\u250028x100 region at (216, 70)\n    \u2502   \u251c\u2500\u2500156x406 region at (948, 262)\n    \u2502   \u2502   \u251c\u2500\u250033x100 region at (1070, 384)\n    \u2502   \u2502   \u251c\u2500\u250038x87 region at (948, 359)\n    \u2502   \u2502   \u2514\u2500\u250038x57 region at (1296, 329)\n    \u2502   \u2514\u2500\u2500156x309 region at (0, 85)\n    \u2502       \u251c\u2500\u250038x76 region at (7, 159)\n    \u2502       \u251c\u2500\u250038x76 region at (142, 124)\n    \u2502       \u251c\u2500\u250034x76 region at (218, 85)\n    \u2502       \u251c\u2500\u250038x76 region at (215, 125)\n    \u2502       \u2514\u2500\u250038x76 region at (52, 105)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500156x406 region at (480, 173)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (623, 272)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (498, 270)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (561, 244)\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (652, 261)\n    \u2502   \u251c\u2500\u2500156x406 region at (690, 11)\n    \u2502   \u2502   \u251c\u2500\u250038x82 region at (690, 122)\n    \u2502   \u2502   \u251c\u2500\u250038x95 region at (690, 13)\n    \u2502   \u2502   \u251c\u2500\u250037x54 region at (690, 129)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (919, 95)\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (805, 59)\n    \u2502   \u251c\u2500\u2500149x406 region at (570, 0)\n    \u2502   \u2502   \u2514\u2500\u250023x71 region at (904, 125)\n    \u2502   \u251c\u2500\u2500156x332 region at (1296, 381)\n    \u2502   \u2502   \u251c\u2500\u250038x53 region at (1296, 403)\n    \u2502   \u2502   \u251c\u2500\u250035x82 region at (1469, 381)\n    \u2502   \u2502   \u2514\u2500\u250038x82 region at (1328, 457)\n    \u2502   \u2514\u2500\u2500156x292 region at (0, 16)\n    \u2502       \u2514\u2500\u250038x65 region at (0, 129)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u250099x213 region at (1415, 0)\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1426, 71)\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1463, 37)\n    \u2502   \u2502   \u2514\u2500\u250024x52 region at (1525, 31)\n    \u2502   \u2514\u2500\u2500116x406 region at (678, 509)\n    \u2502       \u251c\u2500\u250028x100 region at (929, 544)\n    \u2502       \u2514\u2500\u250028x76 region at (1007, 512)\n    \u2514\u2500\u2500626x1629 image demo_image\n        \u251c\u2500\u2500156x278 region at (0, 234)\n        \u2502   \u2514\u2500\u250038x68 region at (144, 330)\n        \u251c\u2500\u2500156x406 region at (786, 133)\n        \u2502   \u251c\u2500\u250038x100 region at (891, 223)\n        \u2502   \u251c\u2500\u250038x64 region at (786, 154)\n        \u2502   \u251c\u2500\u250038x100 region at (1000, 245)\n        \u2502   \u2514\u2500\u250038x100 region at (911, 242)\n        \u251c\u2500\u2500156x406 region at (1105, 461)\n        \u2502   \u251c\u2500\u250029x100 region at (1170, 587)\n        \u2502   \u251c\u2500\u250038x100 region at (1194, 571)\n        \u2502   \u2514\u2500\u250038x100 region at (1219, 509)\n        \u2514\u2500\u250090x406 region at (442, 0)\n            \u251c\u2500\u250022x91 region at (442, 14)\n            \u251c\u2500\u250013x67 region at (780, 0)\n            \u251c\u2500\u250022x100 region at (681, 18)\n            \u251c\u2500\u250021x100 region at (554, 0)\n            \u2514\u2500\u250022x100 region at (667, 6)\n</pre> <p>When the segmentation is done, the segments can be passed to a text recognition model. The results are passed to the workbench in the same manner as before:</p> In\u00a0[8]: Copied! <pre>from htrflow_core.models.dummy_models import RecognitionModel\n\nrecognition_model = RecognitionModel()\nresults = recognition_model(volume.segments())\nvolume.update(results)\nprint(volume)\n</pre> from htrflow_core.models.dummy_models import RecognitionModel  recognition_model = RecognitionModel() results = recognition_model(volume.segments()) volume.update(results) print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u2514\u2500\u2500156x406 region at (345, 11)\n    \u2502       \u251c\u2500\u250037x100 region at (517, 129) \"Dolor velit non non tempora magnam ut adipisci.\"\n    \u2502       \u251c\u2500\u250022x100 region at (636, 144) \"Dolor quiquia quisquam adipisci velit velit quiquia quiquia.\"\n    \u2502       \u251c\u2500\u250038x100 region at (543, 125) \"Ipsum labore dolorem ut neque ipsum velit.\"\n    \u2502       \u251c\u2500\u250038x100 region at (486, 122) \"Consectetur est numquam voluptatem quiquia ipsum.\"\n    \u2502       \u2514\u2500\u250038x69 region at (681, 38) \"Magnam etincidunt consectetur neque quaerat ut sit ipsum.\"\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500117x406 region at (17, 0)\n    \u2502   \u2502   \u2514\u2500\u250028x100 region at (216, 70) \"Modi sed non tempora.\"\n    \u2502   \u251c\u2500\u2500156x406 region at (948, 262)\n    \u2502   \u2502   \u251c\u2500\u250033x100 region at (1070, 384) \"Numquam quiquia ut etincidunt sit quaerat adipisci.\"\n    \u2502   \u2502   \u251c\u2500\u250038x87 region at (948, 359) \"Est etincidunt dolore modi.\"\n    \u2502   \u2502   \u2514\u2500\u250038x57 region at (1296, 329) \"Dolore ut tempora numquam voluptatem dolorem etincidunt non.\"\n    \u2502   \u2514\u2500\u2500156x309 region at (0, 85)\n    \u2502       \u251c\u2500\u250038x76 region at (7, 159) \"Numquam amet quisquam magnam modi.\"\n    \u2502       \u251c\u2500\u250038x76 region at (142, 124) \"Dolorem dolorem eius aliquam eius.\"\n    \u2502       \u251c\u2500\u250034x76 region at (218, 85) \"Eius tempora modi sit.\"\n    \u2502       \u251c\u2500\u250038x76 region at (215, 125) \"Tempora labore velit dolor.\"\n    \u2502       \u2514\u2500\u250038x76 region at (52, 105) \"Consectetur neque labore porro quiquia.\"\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500156x406 region at (480, 173)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (623, 272) \"Quaerat sed ipsum tempora.\"\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (498, 270) \"Ipsum aliquam consectetur dolor.\"\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (561, 244) \"Sed magnam aliquam aliquam dolor.\"\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (652, 261) \"Sed dolor amet sed adipisci etincidunt.\"\n    \u2502   \u251c\u2500\u2500156x406 region at (690, 11)\n    \u2502   \u2502   \u251c\u2500\u250038x82 region at (690, 122) \"Voluptatem aliquam aliquam porro amet.\"\n    \u2502   \u2502   \u251c\u2500\u250038x95 region at (690, 13) \"Modi aliquam quiquia etincidunt labore.\"\n    \u2502   \u2502   \u251c\u2500\u250037x54 region at (690, 129) \"Tempora dolore quiquia ipsum neque consectetur tempora.\"\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (919, 95) \"Tempora labore modi ut non.\"\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (805, 59) \"Ut dolorem labore dolore consectetur.\"\n    \u2502   \u251c\u2500\u2500149x406 region at (570, 0)\n    \u2502   \u2502   \u2514\u2500\u250023x71 region at (904, 125) \"Est labore dolor est.\"\n    \u2502   \u251c\u2500\u2500156x332 region at (1296, 381)\n    \u2502   \u2502   \u251c\u2500\u250038x53 region at (1296, 403) \"Neque eius adipisci amet voluptatem consectetur.\"\n    \u2502   \u2502   \u251c\u2500\u250035x82 region at (1469, 381) \"Voluptatem magnam voluptatem labore sed dolore voluptatem.\"\n    \u2502   \u2502   \u2514\u2500\u250038x82 region at (1328, 457) \"Dolore ut magnam voluptatem etincidunt amet adipisci.\"\n    \u2502   \u2514\u2500\u2500156x292 region at (0, 16)\n    \u2502       \u2514\u2500\u250038x65 region at (0, 129) \"Etincidunt etincidunt quiquia porro velit.\"\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u250099x213 region at (1415, 0)\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1426, 71) \"Etincidunt etincidunt dolorem modi dolorem.\"\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1463, 37) \"Neque quaerat dolorem magnam.\"\n    \u2502   \u2502   \u2514\u2500\u250024x52 region at (1525, 31) \"Sed aliquam dolor quisquam numquam.\"\n    \u2502   \u2514\u2500\u2500116x406 region at (678, 509)\n    \u2502       \u251c\u2500\u250028x100 region at (929, 544) \"Velit tempora non quiquia magnam ipsum sed.\"\n    \u2502       \u2514\u2500\u250028x76 region at (1007, 512) \"Dolor sed velit quisquam dolor.\"\n    \u2514\u2500\u2500626x1629 image demo_image\n        \u251c\u2500\u2500156x278 region at (0, 234)\n        \u2502   \u2514\u2500\u250038x68 region at (144, 330) \"Amet adipisci quaerat quiquia sit dolor numquam ut.\"\n        \u251c\u2500\u2500156x406 region at (786, 133)\n        \u2502   \u251c\u2500\u250038x100 region at (891, 223) \"Etincidunt velit ut neque labore quisquam.\"\n        \u2502   \u251c\u2500\u250038x64 region at (786, 154) \"Aliquam labore aliquam quaerat consectetur.\"\n        \u2502   \u251c\u2500\u250038x100 region at (1000, 245) \"Ut non numquam ut.\"\n        \u2502   \u2514\u2500\u250038x100 region at (911, 242) \"Ipsum sed non dolore eius consectetur.\"\n        \u251c\u2500\u2500156x406 region at (1105, 461)\n        \u2502   \u251c\u2500\u250029x100 region at (1170, 587) \"Sed sed magnam tempora velit.\"\n        \u2502   \u251c\u2500\u250038x100 region at (1194, 571) \"Numquam quisquam dolore ut non.\"\n        \u2502   \u2514\u2500\u250038x100 region at (1219, 509) \"Sit amet ipsum neque neque adipisci consectetur.\"\n        \u2514\u2500\u250090x406 region at (442, 0)\n            \u251c\u2500\u250022x91 region at (442, 14) \"Ipsum ut eius sit porro sit.\"\n            \u251c\u2500\u250013x67 region at (780, 0) \"Dolorem voluptatem sed voluptatem non modi quisquam.\"\n            \u251c\u2500\u250022x100 region at (681, 18) \"Sed amet labore dolorem velit aliquam.\"\n            \u251c\u2500\u250021x100 region at (554, 0) \"Sit non amet velit dolorem dolore labore.\"\n            \u2514\u2500\u250022x100 region at (667, 6) \"Dolorem amet amet modi voluptatem.\"\n</pre> In\u00a0[9]: Copied! <pre># Access image 0, region 0, subregion 0\nvolume[0, 0, 0]\n\n# Access image 0, region 0\nvolume[0, 0]\n</pre> # Access image 0, region 0, subregion 0 volume[0, 0, 0]  # Access image 0, region 0 volume[0, 0] Out[9]: <pre>&lt;htrflow_core.volume.RegionNode at 0x7f5a496ef1f0&gt;</pre> <p>The image associated with each node is accessed through the <code>image</code> attribute. The image isn't stored directly in the node, instead, the node refers to the parent image, and crops it according to its box:</p> <pre>class BaseImageNode:\n    \n    @property\n    def image(self):\n        x1, x2, y1, y2 = self.box\n        return self.parent.image[y1:y2, x1:x2]\n    \n    ...\n</pre> In\u00a0[10]: Copied! <pre>volume[0, 0, 0].image\n</pre> volume[0, 0, 0].image Out[10]: <pre>array([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       ...,\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]]], dtype=uint8)</pre> In\u00a0[11]: Copied! <pre>print(volume[0].coordinate)\n</pre> print(volume[0].coordinate) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 print(volume[0].coordinate)\n\nAttributeError: 'PageNode' object has no attribute 'coordinate'</pre> <p>For first-level regions <code>coordinate</code> is the same as the corner of the segment bounding box.</p> In\u00a0[12]: Copied! <pre>print('Coordinate:', volume[0, 0].coordinate)\nprint('Bounding box:', volume[0, 0].data['segment'].box, '(x1, x2, y1, y2)')\n</pre> print('Coordinate:', volume[0, 0].coordinate) print('Bounding box:', volume[0, 0].data['segment'].box, '(x1, x2, y1, y2)') <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 print('Coordinate:', volume[0, 0].coordinate)\n      2 print('Bounding box:', volume[0, 0].data['segment'].box, '(x1, x2, y1, y2)')\n\nAttributeError: 'RegionNode' object has no attribute 'coordinate'</pre> <p>But for nested regions the two differ, because <code>coordinate</code> is relative to the original image, while the segment bounding box is relative to the parent region.</p> In\u00a0[13]: Copied! <pre>print('Global coordinate:', volume[0, 0, 0].coordinate)\nprint('Local bounding box:', volume[0, 0, 0].data['segment'].box)\n</pre> print('Global coordinate:', volume[0, 0, 0].coordinate) print('Local bounding box:', volume[0, 0, 0].data['segment'].box) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 print('Global coordinate:', volume[0, 0, 0].coordinate)\n      2 print('Local bounding box:', volume[0, 0, 0].data['segment'].box)\n\nAttributeError: 'RegionNode' object has no attribute 'coordinate'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/demo.html#models-inferencers","title":"Models / inferencers\u00b6","text":"<p>Models &amp; inferencers accept lists of images, and return lists of results (either segmentation or recognition results)</p> <p>I have made a dummy <code>SegmentationModel</code> and <code>RecognitionModel</code> in <code>models.py</code>. These do the same thing as the current inferencers.</p> <pre>class SegmentationModel:\n    def __call__(self, images: list[np.ndarray]) -&gt; list[SegmentationResult]:\n        ...\n\n\n@dataclass\nclass SegmentationResult:\n    boxes: np.ndarray\n    masks: np.ndarray\n    scores: np.ndarray\n    labels: np.ndarray\n</pre> <p>(It would be nice to wrap all models in a \"batching\" function, which divides an input list into chunks if it is too long) -&gt; This is a card in DevOps</p>"},{"location":"notebooks/demo.html#using-the-volume-class","title":"Using the Volume class\u00b6","text":"<p>To load images, create a <code>Volume</code>. The name of this class is not set in stone... It represents what Catrin called a \"batch\", a divison of an archive volume, but I don't want to use \"batch\" because of potential confusion with a model's batch (the number of inputs it operates on simultaneously).</p>"},{"location":"notebooks/demo.html#accessing-nodes","title":"Accessing nodes\u00b6","text":"<p>Specific nodes are accessed by tuple indexing. Here we extract the first line of the first region of the first image:</p>"},{"location":"notebooks/demo.html#coordinates","title":"Coordinates\u00b6","text":"<p>All nodes have a <code>coordinate</code> attribute. This is the location of the node's top-left corner relative to the original image. The base image node's coordinate is thus (0,0):</p>"},{"location":"notebooks/demo2.html","title":"Simple workflow","text":"In\u00a0[\u00a0]: Copied! <pre>from htrflow_core.models.huggingface.trocr import TrOCR\nfrom htrflow_core.models.ultralytics.yolo import YOLO\nfrom htrflow_core.volume import Volume\n\nimg = '../assets/demo_image.jpg'\nvol = Volume([img, img])\n\nseg_model = YOLO('ultralyticsplus/yolov8s')\nres = seg_model(vol.images())  # vol.segments() is also possible since it points to the images\nvol.update(res)\n\nrec_model = TrOCR()\nres = rec_model(vol.segments())\nvol.update(res)\n\n# The final volume\nprint(vol)\n\n# Saves at outputs/&lt;image_name&gt;.xml, since the two demo images are called the same, we get only one output file \nvol.save('outputs', 'alto')\n</pre> from htrflow_core.models.huggingface.trocr import TrOCR from htrflow_core.models.ultralytics.yolo import YOLO from htrflow_core.volume import Volume  img = '../assets/demo_image.jpg' vol = Volume([img, img])  seg_model = YOLO('ultralyticsplus/yolov8s') res = seg_model(vol.images())  # vol.segments() is also possible since it points to the images vol.update(res)  rec_model = TrOCR() res = rec_model(vol.segments()) vol.update(res)  # The final volume print(vol)  # Saves at outputs/.xml, since the two demo images are called the same, we get only one output file  vol.save('outputs', 'alto')"}]}