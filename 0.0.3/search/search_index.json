{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"htrflow","text":""},{"location":"index.html#htrflow_core","title":"htrflow_core","text":"<p>htrflow_core is a part of the htrflow suite, which is Riksarkivets open source project for handwritten text recogntion. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p>"},{"location":"index.html#the-swedish-national-archives-ai-lab","title":"The Swedish National Archives AI-Lab","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p>"},{"location":"index.html#why-should-i-use-htrflow","title":"Why should I use htrflow?","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p>"},{"location":"index.html#quick-tour","title":"Quick tour","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p> <ul> <li> <p> Set up in 5 minutes</p> <p>Install <code>mkdocs-material</code> with <code>pip</code> and get up and running in minutes</p> <p> Getting started</p> </li> <li> <p> It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site</p> <p> Reference</p> </li> <li> <p> Made to measure</p> <p>Change the colors, fonts, language, icons, logo and more with a few lines</p> <p> Customization</p> </li> <li> <p> Open Source, MIT</p> <p>Material for MkDocs is licensed under MIT and available on [GitHub]</p> <p> License</p> </li> </ul>"},{"location":"index.html#usage","title":"Usage","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"Riksarkivet/Trolldomkomission\")[\"train\"]\n\nimages = dataset[\"image\"]\n</code></pre> <p>Release Notes</p> <p>Releases on Github </p>"},{"location":"index.html#installation","title":"Installation","text":"InstallationDevelopment <p>You can install <code>htrflow_core</code> with pypi in a Python&gt;=3.10 environment.</p> <p>pip install (recommended)</p> coremodels <p>The core installation of <code>htrflow_core</code> install everything you need to get you started with structuring output in your htr workflow.</p> <pre><code>pip install htrflow_core\n</code></pre> <p>This installation add support f\u00f6r models we have implemented.</p> <pre><code>pip install \"htrflow_core[models]\"\n</code></pre> <p>git clone (for development)</p> virtualenvpoetry <pre><code># clone repository and navigate to root directory\ngit clone https://github.com/Swedish-National-Archives-AI-lab/htrflow_core\ncd htrflow_core\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\n\n# core install\npip install -e \".\"\n\n# all models install\npip install -e \".[huggingface, openmmlab, ultralytics]\"\n</code></pre> <pre><code># clone repository and navigate to root directory\ngit clone https://github.com/Swedish-National-Archives-AI-lab/htrflow_core\ncd htrflow_core\n\n# setup python environment and activate it\npoetry env use python3.10\npoetry shell\n\n# core install\npoetry install\n\n# all models install\npoetry install --all-extras\n\n# or specific framework\npoetry install --extras huggingface\n</code></pre>"},{"location":"index.html#info","title":"INFO","text":"LicenseContactCitation"},{"location":"index.html#license","title":"License","text":"<p>INFO</p> <p>dsda</p>"},{"location":"index.html#contact","title":"Contact","text":"<p>INFO</p> <p>dsda</p>"},{"location":"index.html#citation","title":"Citation","text":"<p>INFO</p> <p>dsda</p>"},{"location":"contributing.html","title":"Contributing to htrflow_core \ud83d\udee0\ufe0f","text":"<p>Thank you for your interest in contributing to htrflow_core! We appreciate contributions in the following areas:</p> <ol> <li>New Features: Enhance the library by adding new functionality. Refer to the section below for guidelines.</li> <li>Documentation: Help us improve our documentation with clear examples demonstrating how to use htrflow_core.</li> <li>Bug Reports: Identify and report any issues in the project.</li> <li>Feature Requests: Suggest new features or improvements.</li> </ol>"},{"location":"contributing.html#contributing-features","title":"Contributing Features \u2728","text":"<p>htrflow_core aims to provide versatile tools applicable across a broad range of projects. We value contributions that offer generic solutions to common problems. Before proposing a new feature, please open an issue to discuss your idea with the community. This encourages feedback and support.</p>"},{"location":"contributing.html#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Fork the htrflow_core repository to your GitHub account by clicking \"fork\" at the top right of the repository page.</li> <li>Clone your fork locally and create a new branch for your changes:</li> </ol> <pre><code>git clone https://github.com/yourusername/htrflow_core.git\ncd htrflow_core\ngit checkout -b &lt;your_branch_name&gt;\n</code></pre> <ol> <li>Develop your feature, fix, or documentation update on your branch.</li> </ol>"},{"location":"contributing.html#code-quality","title":"Code Quality \ud83c\udfa8","text":"<p>Ensure your code adheres to our quality standards using tools like:</p> <ul> <li>ruff</li> <li>mypy</li> </ul>"},{"location":"contributing.html#documentation","title":"Documentation \ud83d\udcdd","text":"<p>Our documentation utilizes docstrings combined with type hinting from mypy. Update or add necessary documentation in the <code>docs/</code> directory and test it locally with:</p> <pre><code>mkdocs serve -v\n</code></pre>"},{"location":"contributing.html#tests","title":"Tests \ud83e\uddea","text":"<p>We employ pytest for testing. Ensure you add tests for your changes and run:</p> <pre><code>pytest\n</code></pre>"},{"location":"contributing.html#making-a-pull-request","title":"Making a Pull Request","text":"<p>After pushing your changes to GitHub, initiate a pull request from your fork to the main <code>htrflow_core</code> repository:</p> <ol> <li>Push your branch:</li> </ol> <pre><code>git push -u origin &lt;your_branch_name&gt;\n</code></pre> <ol> <li>Visit the repository on GitHub and click \"New Pull Request.\" Set the base branch to <code>develop</code> and describe your changes.</li> </ol> <p>Ensure all tests pass before requesting a review.</p>"},{"location":"contributing.html#license","title":"License \ud83d\udcc4","text":"<p>By contributing to htrflow_core, you agree that your contributions will be licensed under the EUPL-1.2 license.</p> <p>Thank you for contributing to htrflow_core!</p>"},{"location":"api/index.html","title":"Test","text":""},{"location":"api/image/image.html","title":"Image","text":""},{"location":"api/results/results.html","title":"Results","text":""},{"location":"api/results/results.html#htrflow_core.results.RecognizedText","title":"<code>RecognizedText</code>  <code>dataclass</code>","text":"<p>Recognized text class</p> <p>This class represents a result from a text recognition model.</p> <p>Attributes:</p> Name Type Description <code>texts</code> <code>list[str]</code> <p>A sequence of candidate texts</p> <code>scores</code> <code>list[float]</code> <p>The scores of the candidate texts</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@dataclass\nclass RecognizedText:\n    \"\"\"Recognized text class\n\n    This class represents a result from a text recognition model.\n\n    Attributes:\n        texts: A sequence of candidate texts\n        scores: The scores of the candidate texts\n    \"\"\"\n\n    texts: list[str]\n    scores: list[float]\n\n    def __post_init__(self):\n        if not isinstance(self.texts, list):\n            self.texts = [self.texts]\n        if not isinstance(self.scores, list):\n            self.scores = [self.scores]\n\n    def top_candidate(self) -&gt; str:\n        \"\"\"The candidate with the highest confidence score\"\"\"\n        return self.texts[self.scores.index(self.top_score())]\n\n    def top_score(self):\n        \"\"\"The highest confidence score\"\"\"\n        return max(self.scores)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.RecognizedText.top_candidate","title":"<code>top_candidate()</code>","text":"<p>The candidate with the highest confidence score</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def top_candidate(self) -&gt; str:\n    \"\"\"The candidate with the highest confidence score\"\"\"\n    return self.texts[self.scores.index(self.top_score())]\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.RecognizedText.top_score","title":"<code>top_score()</code>","text":"<p>The highest confidence score</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def top_score(self):\n    \"\"\"The highest confidence score\"\"\"\n    return max(self.scores)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result","title":"<code>Result</code>","text":"<p>A result from an arbitrary model (or process)</p> <p>One result instance corresponds to one input image.</p> <p>Attributes:</p> Name Type Description <code>metadata</code> <p>Metadata regarding the result, model-dependent.</p> <code>segments</code> <p><code>Segment</code> instances representing results from an object detection or instance segmentation model, or similar. May be empty if not applicable.</p> <code>data</code> <p>Any other data associated with the result, stored as a sequence of dictionaries. Is assumed to correspond one-to-one with <code>segments</code> when <code>segments</code> is non-empty. If <code>segments</code> is empty, the first entry in <code>data</code> is assumed to apply for the entire input image.</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>class Result:\n    \"\"\"\n    A result from an arbitrary model (or process)\n\n    One result instance corresponds to one input image.\n\n    Attributes:\n        metadata: Metadata regarding the result, model-dependent.\n        segments: `Segment` instances representing results from an object\n            detection or instance segmentation model, or similar. May\n            be empty if not applicable.\n        data: Any other data associated with the result, stored as a\n            sequence of dictionaries. Is assumed to correspond one-to-one\n            with `segments` when `segments` is non-empty. If `segments`\n            is empty, the first entry in `data` is assumed to apply for\n            the entire input image.\n    \"\"\"\n\n    def __init__(\n        self,\n        metadata: dict[str, str] | None = None,\n        segments: Sequence[Segment] | None = None,\n        data: Sequence[dict[str, Any]] | None = None,\n        texts: Sequence[RecognizedText] | None = None,\n    ):\n        self.metadata = metadata or {}\n        self.segments = segments or []\n        self.data = []\n        for entry, text in _zip_longest_none(data, texts, fillvalue={}):\n            self.data.append(entry | {TEXT_RESULT_KEY: text})\n\n    def rescale(self, factor: float):\n        \"\"\"Rescale the Result's segments\"\"\"\n        for segment in self.segments:\n            segment.rescale(factor)\n\n    @property\n    def bboxes(self) -&gt; Sequence[Bbox]:\n        \"\"\"Bounding boxes relative to input image\"\"\"\n        return [segment.bbox for segment in self.segments]\n\n    @property\n    def global_masks(self) -&gt; Sequence[Mask | None]:\n        \"\"\"Global masks relative to input image\"\"\"\n        return [segment.global_mask for segment in self.segments]\n\n    @property\n    def local_mask(self) -&gt; Sequence[Mask | None]:\n        \"\"\"Local masks relative to bounding boxes\"\"\"\n        return [segment.local_mask for segment in self.segments]\n\n    @property\n    def polygons(self) -&gt; Sequence[Polygon | None]:\n        \"\"\"Polygons relative to input image\"\"\"\n        return [segment.polygon for segment in self.segments]\n\n    @property\n    def class_labels(self) -&gt; Sequence[str | None]:\n        \"\"\"Class labels of segments\"\"\"\n        return [segment.class_label for segment in self.segments]\n\n    @classmethod\n    def text_recognition_result(cls, metadata: dict[str, Any], texts: list[str], scores: list[float]) -&gt; \"Result\":\n        \"\"\"Create a text recognition result\n\n        Arguments:\n            metadata: Result metadata\n            text: The recognized text\n\n        Returns:\n            A Result instance with the specified data and no segments.\n        \"\"\"\n        return cls(metadata, texts=[RecognizedText(texts, scores)])\n\n    @classmethod\n    def segmentation_result(\n        cls,\n        orig_shape: tuple[int, int],\n        metadata: dict[str, Any],\n        bboxes: Sequence[Bbox | Iterable[int]] | None = None,\n        masks: Sequence[Mask] | None = None,\n        polygons: Sequence[Polygon] | None = None,\n        scores: Iterable[float] | None = None,\n        labels: Iterable[str] | None = None,\n    ) -&gt; \"Result\":\n        \"\"\"Create a segmentation result\n\n        Arguments:\n            image: The original image\n            metadata: Result metadata\n            segments: The segments\n\n        Returns:\n            A Result instance with the specified data and no texts.\n        \"\"\"\n        segments = []\n        for item in _zip_longest_none(bboxes, masks, scores, labels, polygons):\n            segments.append(Segment(*item, orig_shape=orig_shape))\n        return cls(metadata, segments=segments)\n\n    def reorder(self, index: Sequence[int]) -&gt; None:\n        \"\"\"Reorder result\n\n        Example: Given a `Result` with three segments s0, s1 and s2,\n        index = [2, 0, 1] will put the segments in order [s2, s0, s1].\n        Any indices not in `index` will be dropped from the result.\n\n        Arguments:\n            index: A list of indices representing the new ordering.\n        \"\"\"\n        if self.segments:\n            self.segments = [self.segments[i] for i in index]\n        if self.data:\n            self.data = [self.data[i] for i in index]\n\n    def drop_indices(self, index: Sequence[int]) -&gt; None:\n        \"\"\"Drop segments from result\n\n        Example: Given a `Result` with three segments s0, s1 and s2,\n        index = [0, 2] will drop segments s0 and s2.\n\n        Arguments:\n            index: Indices of segments to drop\n        \"\"\"\n        keep = [i for i in range(len(self.segments)) if i not in index]\n        self.reorder(keep)\n\n    def filter(self, key: str, predicate: Callable[[Any], bool]) -&gt; None:\n        \"\"\"Filter segments and data based on a predicate applied to a specified key.\n\n        Args:\n            key: The key in the data dictionary to test the predicate against.\n            predicate [Callable]: A function that takes a value associated with the key\n            and returns True if the segment should be kept.\n\n        Example:\n        ```\n        &gt;&gt;&gt; def remove_certain_text(text_results):\n        &gt;&gt;&gt;    return text_results != 'lorem'\n        &gt;&gt;&gt; result.filter('text_results', remove_certain_text)\n        True\n        ```\n        \"\"\"\n        keep = [i for i, item in enumerate(self.data) if predicate(item.get(key, None))]\n        self.reorder(keep)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.bboxes","title":"<code>bboxes: Sequence[Bbox]</code>  <code>property</code>","text":"<p>Bounding boxes relative to input image</p>"},{"location":"api/results/results.html#htrflow_core.results.Result.class_labels","title":"<code>class_labels: Sequence[str | None]</code>  <code>property</code>","text":"<p>Class labels of segments</p>"},{"location":"api/results/results.html#htrflow_core.results.Result.global_masks","title":"<code>global_masks: Sequence[Mask | None]</code>  <code>property</code>","text":"<p>Global masks relative to input image</p>"},{"location":"api/results/results.html#htrflow_core.results.Result.local_mask","title":"<code>local_mask: Sequence[Mask | None]</code>  <code>property</code>","text":"<p>Local masks relative to bounding boxes</p>"},{"location":"api/results/results.html#htrflow_core.results.Result.polygons","title":"<code>polygons: Sequence[Polygon | None]</code>  <code>property</code>","text":"<p>Polygons relative to input image</p>"},{"location":"api/results/results.html#htrflow_core.results.Result.drop_indices","title":"<code>drop_indices(index)</code>","text":"<p>Drop segments from result</p> <p>Example: Given a <code>Result</code> with three segments s0, s1 and s2, index = [0, 2] will drop segments s0 and s2.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Sequence[int]</code> <p>Indices of segments to drop</p> required Source code in <code>src/htrflow_core/results.py</code> <pre><code>def drop_indices(self, index: Sequence[int]) -&gt; None:\n    \"\"\"Drop segments from result\n\n    Example: Given a `Result` with three segments s0, s1 and s2,\n    index = [0, 2] will drop segments s0 and s2.\n\n    Arguments:\n        index: Indices of segments to drop\n    \"\"\"\n    keep = [i for i in range(len(self.segments)) if i not in index]\n    self.reorder(keep)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.filter","title":"<code>filter(key, predicate)</code>","text":"<p>Filter segments and data based on a predicate applied to a specified key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key in the data dictionary to test the predicate against.</p> required <code>predicate</code> <code>[Callable]</code> <p>A function that takes a value associated with the key</p> required <p>Example: <pre><code>&gt;&gt;&gt; def remove_certain_text(text_results):\n&gt;&gt;&gt;    return text_results != 'lorem'\n&gt;&gt;&gt; result.filter('text_results', remove_certain_text)\nTrue\n</code></pre></p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def filter(self, key: str, predicate: Callable[[Any], bool]) -&gt; None:\n    \"\"\"Filter segments and data based on a predicate applied to a specified key.\n\n    Args:\n        key: The key in the data dictionary to test the predicate against.\n        predicate [Callable]: A function that takes a value associated with the key\n        and returns True if the segment should be kept.\n\n    Example:\n    ```\n    &gt;&gt;&gt; def remove_certain_text(text_results):\n    &gt;&gt;&gt;    return text_results != 'lorem'\n    &gt;&gt;&gt; result.filter('text_results', remove_certain_text)\n    True\n    ```\n    \"\"\"\n    keep = [i for i, item in enumerate(self.data) if predicate(item.get(key, None))]\n    self.reorder(keep)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.reorder","title":"<code>reorder(index)</code>","text":"<p>Reorder result</p> <p>Example: Given a <code>Result</code> with three segments s0, s1 and s2, index = [2, 0, 1] will put the segments in order [s2, s0, s1]. Any indices not in <code>index</code> will be dropped from the result.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Sequence[int]</code> <p>A list of indices representing the new ordering.</p> required Source code in <code>src/htrflow_core/results.py</code> <pre><code>def reorder(self, index: Sequence[int]) -&gt; None:\n    \"\"\"Reorder result\n\n    Example: Given a `Result` with three segments s0, s1 and s2,\n    index = [2, 0, 1] will put the segments in order [s2, s0, s1].\n    Any indices not in `index` will be dropped from the result.\n\n    Arguments:\n        index: A list of indices representing the new ordering.\n    \"\"\"\n    if self.segments:\n        self.segments = [self.segments[i] for i in index]\n    if self.data:\n        self.data = [self.data[i] for i in index]\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.rescale","title":"<code>rescale(factor)</code>","text":"<p>Rescale the Result's segments</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def rescale(self, factor: float):\n    \"\"\"Rescale the Result's segments\"\"\"\n    for segment in self.segments:\n        segment.rescale(factor)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.segmentation_result","title":"<code>segmentation_result(orig_shape, metadata, bboxes=None, masks=None, polygons=None, scores=None, labels=None)</code>  <code>classmethod</code>","text":"<p>Create a segmentation result</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <p>The original image</p> required <code>metadata</code> <code>dict[str, Any]</code> <p>Result metadata</p> required <code>segments</code> <p>The segments</p> required <p>Returns:</p> Type Description <code>Result</code> <p>A Result instance with the specified data and no texts.</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@classmethod\ndef segmentation_result(\n    cls,\n    orig_shape: tuple[int, int],\n    metadata: dict[str, Any],\n    bboxes: Sequence[Bbox | Iterable[int]] | None = None,\n    masks: Sequence[Mask] | None = None,\n    polygons: Sequence[Polygon] | None = None,\n    scores: Iterable[float] | None = None,\n    labels: Iterable[str] | None = None,\n) -&gt; \"Result\":\n    \"\"\"Create a segmentation result\n\n    Arguments:\n        image: The original image\n        metadata: Result metadata\n        segments: The segments\n\n    Returns:\n        A Result instance with the specified data and no texts.\n    \"\"\"\n    segments = []\n    for item in _zip_longest_none(bboxes, masks, scores, labels, polygons):\n        segments.append(Segment(*item, orig_shape=orig_shape))\n    return cls(metadata, segments=segments)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Result.text_recognition_result","title":"<code>text_recognition_result(metadata, texts, scores)</code>  <code>classmethod</code>","text":"<p>Create a text recognition result</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[str, Any]</code> <p>Result metadata</p> required <code>text</code> <p>The recognized text</p> required <p>Returns:</p> Type Description <code>Result</code> <p>A Result instance with the specified data and no segments.</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>@classmethod\ndef text_recognition_result(cls, metadata: dict[str, Any], texts: list[str], scores: list[float]) -&gt; \"Result\":\n    \"\"\"Create a text recognition result\n\n    Arguments:\n        metadata: Result metadata\n        text: The recognized text\n\n    Returns:\n        A Result instance with the specified data and no segments.\n    \"\"\"\n    return cls(metadata, texts=[RecognizedText(texts, scores)])\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment","title":"<code>Segment</code>","text":"<p>Segment class</p> <p>Class representing a segment of an image, typically a result from a segmentation model or a detection model.</p> <p>Attributes:</p> Name Type Description <code>bbox</code> <code>Bbox</code> <p>The bounding box of the segment</p> <code>mask</code> <code>Mask | None</code> <p>The segment's mask, if available. The mask is stored relative to the bounding box. Use the <code>global_mask()</code> method to retrieve the mask relative to the original image.</p> <code>score</code> <code>float | None</code> <p>Segment confidence score, if available.</p> <code>class_label</code> <code>str | None</code> <p>Segment class label, if available.</p> <code>polygon</code> <code>Polygon | None</code> <p>An approximation of the segment mask, relative to the original image. If no mask is available, <code>polygon</code> defaults to a polygon representation of the segment's bounding box.</p> <code>orig_shape</code> <code>tuple[int, int] | None</code> <p>The shape of the orginal input image.</p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>class Segment:\n    \"\"\"Segment class\n\n    Class representing a segment of an image, typically a result from\n    a segmentation model or a detection model.\n\n    Attributes:\n        bbox: The bounding box of the segment\n        mask: The segment's mask, if available. The mask is stored\n            relative to the bounding box. Use the `global_mask()`\n            method to retrieve the mask relative to the original image.\n        score: Segment confidence score, if available.\n        class_label: Segment class label, if available.\n        polygon: An approximation of the segment mask, relative to the\n            original image. If no mask is available, `polygon` defaults\n            to a polygon representation of the segment's bounding box.\n        orig_shape: The shape of the orginal input image.\n    \"\"\"\n\n    bbox: Bbox\n    mask: Mask | None\n    score: float | None\n    class_label: str | None\n    polygon: Polygon | None\n    orig_shape: tuple[int, int] | None\n\n    def __init__(\n        self,\n        bbox: tuple[int, int, int, int] | Bbox | None = None,\n        mask: Mask | None = None,\n        score: float | None = None,\n        class_label: str | None = None,\n        polygon: Polygon | Sequence[tuple[int, int]] | None = None,\n        orig_shape: tuple[int, int] | None = None,\n    ):\n        \"\"\"Create a `Segment` instance\n\n        A segment can be created from a bounding box, a polygon, a mask\n        or any combination of the three.\n\n        Arguments:\n            bbox: The segment's bounding box, as either a `geometry.Bbox`\n                instance or as a (xmin, ymin, xmax, ymax) tuple. Required\n                if `mask` and `polygon` are None. Defaults to None.\n            mask: The segment's mask relative to the original input image.\n                Required if both `polygon` and `bbox` are None. Defaults\n                to None.\n            score: Segment confidence score. Defaults to None.\n            class_label: Segment class label. Defaults to None.\n            polygon: A polygon defining the segment, relative to the input\n                image. Defaults to None. Required if both `mask` and `bbox`\n                are None.\n            orig_shape: The shape of the orginal input image. Defaults to\n                None.\n        \"\"\"\n        if all(item is None for item in (bbox, mask, polygon)):\n            raise ValueError(\"Cannot create a Segment without bbox, mask or polygon\")\n\n        # Mask (and possibly bbox) is given: The mask is assumed to be aligned\n        # with the original image. The bounding box is discarded (if given) and\n        # recomputed from the mask. A polygon is also inferred from the mask.\n        # The mask is then converted to a local mask.\n        if mask is not None:\n            bbox = geometry.mask2bbox(mask)\n            polygon = geometry.mask2polygon(mask)\n            mask = imgproc.crop(mask, bbox)\n\n        if polygon is not None:\n            polygon = geometry.Polygon(polygon)\n\n            # Use the polygon's bounding box if no other bounding box was provided\n            if bbox is None:\n                bbox = polygon.bbox()\n\n        self.bbox = geometry.Bbox(*bbox)\n        self.polygon = polygon\n        self.mask = mask\n        self.score = score\n        self.class_label = class_label\n        self.orig_shape = orig_shape\n\n    def __str__(self):\n        return f\"Segment(class_label={self.class_label}, score={self.score}, bbox={self.bbox}, polygon={self.polygon}, mask={self.mask})\"  # noqa: E501\n\n    @property\n    def global_mask(self, orig_shape: tuple[int, int] | None = None) -&gt; Mask | None:\n        \"\"\"\n        The segment mask relative to the original input image.\n\n        Arguments:\n            orig_shape: Pass this argument to use another original shape\n                than the segment's `orig_shape` attribute. Defaults to None.\n        \"\"\"\n        if self.mask is None:\n            return None\n\n        orig_shape = self.orig_shape if orig_shape is None else orig_shape\n        if orig_shape is None:\n            raise ValueError(\"Cannot compute the global mask without knowing the original shape.\")\n\n        x1, y1, x2, y2 = self.bbox\n        mask = np.zeros(orig_shape, dtype=np.uint8)\n        mask[y1:y2, x1:x2] = self.mask\n        return mask\n\n    def approximate_mask(self, ratio: float) -&gt; Mask | None:\n        \"\"\"A lower resolution version of the global mask\n\n        Arguments:\n            ratio: Size of approximate mask relative to the original.\n        \"\"\"\n        global_mask = self.global_mask\n        if global_mask is None:\n            return None\n        return imgproc.rescale(global_mask, ratio)\n\n    @property\n    def local_mask(self):\n        \"\"\"The segment mask relative to the bounding box (alias for self.mask)\"\"\"\n        return self.mask\n\n    def rescale(self, factor: float) -&gt; None:\n        \"\"\"Rescale the segment's mask, bounding box and polygon by `factor`\"\"\"\n        if self.mask is not None:\n            self.mask = imgproc.rescale_linear(self.mask, factor)\n        self.bbox = self.bbox.rescale(factor)\n        if self.polygon is not None:\n            self.polygon = self.polygon.rescale(factor)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment.global_mask","title":"<code>global_mask: Mask | None</code>  <code>property</code>","text":"<p>The segment mask relative to the original input image.</p> <p>Parameters:</p> Name Type Description Default <code>orig_shape</code> <p>Pass this argument to use another original shape than the segment's <code>orig_shape</code> attribute. Defaults to None.</p> required"},{"location":"api/results/results.html#htrflow_core.results.Segment.local_mask","title":"<code>local_mask</code>  <code>property</code>","text":"<p>The segment mask relative to the bounding box (alias for self.mask)</p>"},{"location":"api/results/results.html#htrflow_core.results.Segment.__init__","title":"<code>__init__(bbox=None, mask=None, score=None, class_label=None, polygon=None, orig_shape=None)</code>","text":"<p>Create a <code>Segment</code> instance</p> <p>A segment can be created from a bounding box, a polygon, a mask or any combination of the three.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>tuple[int, int, int, int] | Bbox | None</code> <p>The segment's bounding box, as either a <code>geometry.Bbox</code> instance or as a (xmin, ymin, xmax, ymax) tuple. Required if <code>mask</code> and <code>polygon</code> are None. Defaults to None.</p> <code>None</code> <code>mask</code> <code>Mask | None</code> <p>The segment's mask relative to the original input image. Required if both <code>polygon</code> and <code>bbox</code> are None. Defaults to None.</p> <code>None</code> <code>score</code> <code>float | None</code> <p>Segment confidence score. Defaults to None.</p> <code>None</code> <code>class_label</code> <code>str | None</code> <p>Segment class label. Defaults to None.</p> <code>None</code> <code>polygon</code> <code>Polygon | Sequence[tuple[int, int]] | None</code> <p>A polygon defining the segment, relative to the input image. Defaults to None. Required if both <code>mask</code> and <code>bbox</code> are None.</p> <code>None</code> <code>orig_shape</code> <code>tuple[int, int] | None</code> <p>The shape of the orginal input image. Defaults to None.</p> <code>None</code> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def __init__(\n    self,\n    bbox: tuple[int, int, int, int] | Bbox | None = None,\n    mask: Mask | None = None,\n    score: float | None = None,\n    class_label: str | None = None,\n    polygon: Polygon | Sequence[tuple[int, int]] | None = None,\n    orig_shape: tuple[int, int] | None = None,\n):\n    \"\"\"Create a `Segment` instance\n\n    A segment can be created from a bounding box, a polygon, a mask\n    or any combination of the three.\n\n    Arguments:\n        bbox: The segment's bounding box, as either a `geometry.Bbox`\n            instance or as a (xmin, ymin, xmax, ymax) tuple. Required\n            if `mask` and `polygon` are None. Defaults to None.\n        mask: The segment's mask relative to the original input image.\n            Required if both `polygon` and `bbox` are None. Defaults\n            to None.\n        score: Segment confidence score. Defaults to None.\n        class_label: Segment class label. Defaults to None.\n        polygon: A polygon defining the segment, relative to the input\n            image. Defaults to None. Required if both `mask` and `bbox`\n            are None.\n        orig_shape: The shape of the orginal input image. Defaults to\n            None.\n    \"\"\"\n    if all(item is None for item in (bbox, mask, polygon)):\n        raise ValueError(\"Cannot create a Segment without bbox, mask or polygon\")\n\n    # Mask (and possibly bbox) is given: The mask is assumed to be aligned\n    # with the original image. The bounding box is discarded (if given) and\n    # recomputed from the mask. A polygon is also inferred from the mask.\n    # The mask is then converted to a local mask.\n    if mask is not None:\n        bbox = geometry.mask2bbox(mask)\n        polygon = geometry.mask2polygon(mask)\n        mask = imgproc.crop(mask, bbox)\n\n    if polygon is not None:\n        polygon = geometry.Polygon(polygon)\n\n        # Use the polygon's bounding box if no other bounding box was provided\n        if bbox is None:\n            bbox = polygon.bbox()\n\n    self.bbox = geometry.Bbox(*bbox)\n    self.polygon = polygon\n    self.mask = mask\n    self.score = score\n    self.class_label = class_label\n    self.orig_shape = orig_shape\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment.approximate_mask","title":"<code>approximate_mask(ratio)</code>","text":"<p>A lower resolution version of the global mask</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>Size of approximate mask relative to the original.</p> required Source code in <code>src/htrflow_core/results.py</code> <pre><code>def approximate_mask(self, ratio: float) -&gt; Mask | None:\n    \"\"\"A lower resolution version of the global mask\n\n    Arguments:\n        ratio: Size of approximate mask relative to the original.\n    \"\"\"\n    global_mask = self.global_mask\n    if global_mask is None:\n        return None\n    return imgproc.rescale(global_mask, ratio)\n</code></pre>"},{"location":"api/results/results.html#htrflow_core.results.Segment.rescale","title":"<code>rescale(factor)</code>","text":"<p>Rescale the segment's mask, bounding box and polygon by <code>factor</code></p> Source code in <code>src/htrflow_core/results.py</code> <pre><code>def rescale(self, factor: float) -&gt; None:\n    \"\"\"Rescale the segment's mask, bounding box and polygon by `factor`\"\"\"\n    if self.mask is not None:\n        self.mask = imgproc.rescale_linear(self.mask, factor)\n    self.bbox = self.bbox.rescale(factor)\n    if self.polygon is not None:\n        self.polygon = self.polygon.rescale(factor)\n</code></pre>"},{"location":"api/volume/node.html","title":"Noe","text":""},{"location":"api/volume/volume.html","title":"Volume","text":""},{"location":"core/design.html","title":"Htrflow Design","text":"<pre><code>graph TD\n    A[htrflow_core] --&gt; B(dummies)\n    A --&gt; C(image)\n    A --&gt; E(logging)\n    A --&gt; F(models)\n    A --&gt; G(overlapping_masks)\n\n    A --&gt; J(reading_order)\n    A --&gt; K(results)\n    A --&gt; L(serialization)\n    A --&gt; M(templates)\n    A --&gt; N(volume)\n\n\n    F --&gt; F3(huggingface)\n    F --&gt; F5(openmmlab)\n    F --&gt; F7(ultralytics)\n\n    M --&gt; M1(alto)\n    M --&gt; M2(page)</code></pre>"},{"location":"core/design.html#sequence-diagram-workflow","title":"Sequence Diagram workflow","text":"<p>The Swedish National Archives introduces a...</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p>"},{"location":"core/design.html#data-strucutre","title":"Data strucutre","text":"<pre><code>graph TD\n    A(\"Image.jpg\") --&gt; B(\"Region\")\n    B --&gt; C1(\"Line - Text: 'Lorem non ipsum dolor.'\")\n    C1 --&gt; D1(\"Word: 'Lorem'\")\n    C1 --&gt; D2(\"Word: 'non'\")\n    C1 --&gt; D3(\"Word: 'ipsum'\")\n    C1 --&gt; D4(\"Word: 'dolor.'\")\n    B --&gt; C2(\"Line - Text: 'Numquam consectetur ut'\")\n    C2 --&gt; D5(\"Word: 'Numquam'\")\n    C2 --&gt; D6(\"Word: 'consectetur'\")\n    C2 --&gt; D7(\"Word: 'ut'\")</code></pre>"},{"location":"core/ecosystem.html","title":"Htrflow Ecosystem","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p> <pre><code>mindmap\n  root((htrlow))\n    htrflow_core\n      Core\n      CLI\n    sandbox\n      Fasttrack\n      Stepwise\n      More..\n    As-a-service?\n      Rest API?\n      Infrastructure?</code></pre>"},{"location":"core/ecosystem.html#haas","title":"HaaS?","text":""},{"location":"getting_started/index.html","title":"Getting Started","text":""},{"location":"getting_started/data_structure.html","title":"Working with the data","text":""},{"location":"getting_started/models.html","title":"Adding new models","text":""},{"location":"getting_started/pipeline.html","title":"Building your pipeline","text":""},{"location":"getting_started/quick_start.html","title":"Quickstart","text":"<p> - Quickstart</p>"},{"location":"getting_started/quick_start.html#data","title":"Data","text":"<p>Load dataset from huggingface</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"Riksarkivet/Trolldomkomission\")[\"train\"]\n\nimages = dataset[\"image\"]\n</code></pre>"},{"location":"getting_started/quick_start.html#volume","title":"Volume","text":"<pre><code>from htrflow_core.volume import Volume\n\nvol = Volume([images])\n</code></pre>"},{"location":"getting_started/quick_start.html#segment-images","title":"Segment Images","text":"<pre><code>from htrflow_core.models.ultralytics.yolo import YOLO\n\nseg_model = YOLO('ultralyticsplus/yolov8s')\nres = seg_model(vol.images()) # vol.segments() is also possible since it points to the images\n</code></pre>"},{"location":"getting_started/quick_start.html#update-volume","title":"Update Volume","text":"<pre><code>vol.update(res)\n</code></pre>"},{"location":"getting_started/quick_start.html#htr","title":"HTR","text":"<pre><code>from htrflow_core.models.huggingface.trocr import TrOCR\n\nrec_model = TrOCR()\nres = rec_model(vol.segments())\n\nvol.update(res)\n</code></pre> <p>Note</p> <p>The final volume <pre><code>    print(vol)\n</code></pre></p>"},{"location":"getting_started/quick_start.html#serialize","title":"Serialize","text":"<p>Saves at outputs/.xml, since the two demo images are called the same, we get only one output file</p> <pre><code>vol.save('outputs', 'alto')\n</code></pre> <p>..</p> <p>Whenever you have large documents, you typicall ...</p> <pre><code># Something here\n</code></pre>"},{"location":"getting_started/serialization.html","title":"Serialization","text":""},{"location":"getting_started/serialization.html#support-output-formats","title":"Support output formats..","text":""},{"location":"hf/datasets.html","title":"Datasets","text":""},{"location":"hf/datasets.html#htrflow-contributions","title":"HTRFLOW \u2013 Contributions","text":"<p>The AI models used in HTRFLOW is the result of a collaborative effort, involving the National Archives in both Sweden and Finland, in partnership with the Stockholm City Archives, J\u00e4mtlands l\u00e4ns fornskrifts\u00e4llskap, citizen science volunteers and researchers from Stockholm and Uppsala Universities.</p> <p>Several datasets have been created by participants through Citizen Science using the Handwritten Text Recognition (HTR) software, Transkribus, provided by READ-COOP SCE .</p>"},{"location":"hf/datasets.html#archives-used-to-train-models-for-htrflow","title":"Archives used to train models for HTRFLOW","text":"<p>The datasets will be avaliable here soon:</p> <p>Train and testsets created by the Swedish National Archives will be released here:</p> <ul> <li>Riksarkivet/placeholder_region_segmentation</li> <li>Riksarkivet/placeholder_line_segmentation</li> <li>Riksarkivet/placeholder_htr</li> </ul> Datasets Description Link Svea hovr\u00e4tt Svea hovr\u00e4tt (Renskrivna protokoll), 1713\u20131735 Link Bergm\u00e4staren Bergm\u00e4staren i Nora m fl bergslag (Hammartingsprotokoll), 1698\u20131765 Link Trolldomskommissionen Trolldomskommissionen, mainly 1670s Link Bergskollegium Bergskollegium, 1718\u20131758 Link J\u00e4mtlands J\u00e4mtlands domsaga, 1647\u20131688 Link Stockholms domkapitel Stockholms domkapitel, 1728\u20131759 Link Politikollegiet Politikollegiet, 1729\u20131759 Link Poliskammaren G\u00f6teborgs poliskammare f\u00f6re 1900 (Detektiva polisens rapportb\u00f6cker), 1868\u20131901 Link Court Records Renovated Court Records, the National Archives of Finland, 1800s Link"},{"location":"hf/datasets.html#ongoing-research-collaborations","title":"Ongoing research collaborations","text":"<p>Transcription node Sweden \u2013 machine interpretation and citizen research combined, Swedish National Archives and University of Gothenburg, funded by the Swedish National Heritage Board.</p> <p>Mapping the geographies of early modern mining knowledge. A digital history of the study tours of the Swedish Bureau of Mines, 1691\u20131826, Uppsala University and Stockholm University, funded by the Swedish Research Council.</p> <p>The Swedish National Archives' research and development on HTR is part of the Swedish national infrastructure Huminfra. Click here for more information.</p>"},{"location":"hf/models.html","title":"Models","text":""},{"location":"hf/models.html#models","title":"Models","text":"<p>The models used in this demo are very much a work in progress, and as more data, and new architectures, becomes available, they will be retrained and reevaluated. For more information about the models, please refer to their model-cards on Huggingface.</p> <ul> <li>Riksarkivet/rtmdet_regions</li> <li>Riksarkivet/rtmdet_lines</li> <li>Riksarkivet/satrn_htr</li> </ul> Method Description <code>GET</code>      Fetch resource <code>PUT</code>  Update resource <code>DELETE</code>      Delete resource <p>Riksarkivet/rtmdet_regions</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo</p> <p>Riksarkivet/rtmdet_regions</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo</p> <p>Riksarkivet/rtmdet_regions</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo</p>"},{"location":"hf/spaces.html","title":"Spaces","text":""},{"location":"hf/spaces.html#fasttrack-stepwsie","title":"Fasttrack &amp; Stepwsie","text":"<p>htrflow_app</p> <p>htrflow_app is designed to provide users with a step-by-step visualization of the HTR-process, and offer non-expert users an inside look into the workings of an AI-transcription pipeline. At the moment htrflow_app is mainly a demo-application. It\u2019s not intended for production, but instead to showcase the immense possibilities that HTR-technology is opening up for cultural heritage institutions around the world.</p> <p>All code is open-source, all our models are on Hugging Face and are free to use, and all data will be made available for download and use on Hugging Face as well.</p> <p>Note</p> <p>Note</p> <p>The backend (src) for the app will be rewritten and packaged to be more optimized under the project name htrflow_core.</p> <p> </p>"},{"location":"notebooks/demo.html","title":"The internals","text":"In\u00a0[1]: Copied! <pre>import random\n\n\nrandom.seed(123)\n</pre> import random   random.seed(123) In\u00a0[2]: Copied! <pre>from htrflow_core.volume import Volume\n\n\nimages = [\"../assets/demo_image.jpg\"] * 5\n\nvolume = Volume(images)\n</pre> from htrflow_core.volume import Volume   images = [\"../assets/demo_image.jpg\"] * 5  volume = Volume(images) <p>The <code>Volume</code> instance holds a tree. We see the root <code>node</code> and its five children, each representing one input image:</p> In\u00a0[3]: Copied! <pre>print(volume)\n</pre> print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2514\u2500\u2500626x1629 image demo_image\n</pre> <p>The images are available through <code>volume.images()</code>. We pass them through a segmentation model:</p> In\u00a0[4]: Copied! <pre>from htrflow_core.models.dummy_models import SegmentationModel\n\n\nmodel = SegmentationModel()\nresults = model(volume.images())\nprint(results[0])\n</pre> from htrflow_core.models.dummy_models import SegmentationModel   model = SegmentationModel() results = model(volume.images()) print(results[0]) <pre>SegmentationResult(metadata={'model_name': 'SegmentationModel'}, image=array([[[118, 120, 128],\n        [115, 117, 125],\n        [114, 116, 124],\n        ...,\n        [215, 219, 220],\n        [209, 213, 214],\n        [206, 210, 211]],\n\n       [[110, 112, 120],\n        [110, 112, 120],\n        [110, 112, 120],\n        ...,\n        [211, 215, 216],\n        [207, 211, 212],\n        [209, 213, 214]],\n\n       [[109, 112, 120],\n        [109, 112, 120],\n        [104, 107, 115],\n        ...,\n        [207, 211, 212],\n        [205, 209, 210],\n        [209, 213, 214]],\n\n       ...,\n\n       [[146, 152, 151],\n        [147, 153, 152],\n        [147, 153, 152],\n        ...,\n        [212, 218, 213],\n        [214, 222, 211],\n        [211, 221, 204]],\n\n       [[144, 150, 149],\n        [146, 152, 151],\n        [148, 154, 153],\n        ...,\n        [217, 223, 212],\n        [220, 231, 205],\n        [216, 234, 187]],\n\n       [[147, 153, 152],\n        [149, 155, 154],\n        [151, 157, 156],\n        ...,\n        [214, 221, 208],\n        [214, 228, 194],\n        [208, 231, 169]]], dtype=uint8), segments=[Segment(bbox=(345, 751, 11, 167), mask=array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), polygon=array([[345,  85],\n       [356, 115],\n       [393, 140],\n       [527, 167],\n       [672, 151],\n       [726, 127],\n       [751,  93],\n       [740,  63],\n       [703,  38],\n       [570,  11],\n       [417,  29],\n       [365,  55]], dtype=int32), score=0.7689563885870707, class_label='region')])\n</pre> <p>The results are a list of <code>SegmentationResult</code>. To apply the results to the input images, we pass them back to the volume with its <code>update</code> method. It returns the new regions as a list of images.</p> In\u00a0[5]: Copied! <pre>regions = volume.update(results)\n</pre> regions = volume.update(results) <p>The volume tree has now grown:</p> In\u00a0[6]: Copied! <pre>print(volume)\n</pre> print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u2514\u2500\u2500156x406 region at (345, 11)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500117x406 region at (17, 0)\n    \u2502   \u251c\u2500\u2500156x406 region at (948, 262)\n    \u2502   \u2514\u2500\u2500156x309 region at (0, 85)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500156x406 region at (480, 173)\n    \u2502   \u251c\u2500\u2500156x406 region at (690, 11)\n    \u2502   \u251c\u2500\u2500149x406 region at (570, 0)\n    \u2502   \u251c\u2500\u2500156x332 region at (1296, 381)\n    \u2502   \u2514\u2500\u2500156x292 region at (0, 16)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u250099x213 region at (1415, 0)\n    \u2502   \u2514\u2500\u2500116x406 region at (678, 509)\n    \u2514\u2500\u2500626x1629 image demo_image\n        \u251c\u2500\u2500156x278 region at (0, 234)\n        \u251c\u2500\u2500156x406 region at (786, 133)\n        \u251c\u2500\u2500156x406 region at (1105, 461)\n        \u2514\u2500\u250090x406 region at (442, 0)\n</pre> <p>The new regions can be passed through a segmentation model (such as a line model) again. The <code>update</code> method always updates the leaves of the tree.</p> In\u00a0[7]: Copied! <pre>results = model(volume.segments())\nvolume.update(results)\nprint(volume)\n</pre> results = model(volume.segments()) volume.update(results) print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u2514\u2500\u2500156x406 region at (345, 11)\n    \u2502       \u251c\u2500\u250037x100 region at (517, 129)\n    \u2502       \u251c\u2500\u250022x100 region at (636, 144)\n    \u2502       \u251c\u2500\u250038x100 region at (543, 125)\n    \u2502       \u251c\u2500\u250038x100 region at (486, 122)\n    \u2502       \u2514\u2500\u250038x69 region at (681, 38)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500117x406 region at (17, 0)\n    \u2502   \u2502   \u2514\u2500\u250028x100 region at (216, 70)\n    \u2502   \u251c\u2500\u2500156x406 region at (948, 262)\n    \u2502   \u2502   \u251c\u2500\u250033x100 region at (1070, 384)\n    \u2502   \u2502   \u251c\u2500\u250038x87 region at (948, 359)\n    \u2502   \u2502   \u2514\u2500\u250038x57 region at (1296, 329)\n    \u2502   \u2514\u2500\u2500156x309 region at (0, 85)\n    \u2502       \u251c\u2500\u250038x76 region at (7, 159)\n    \u2502       \u251c\u2500\u250038x76 region at (142, 124)\n    \u2502       \u251c\u2500\u250034x76 region at (218, 85)\n    \u2502       \u251c\u2500\u250038x76 region at (215, 125)\n    \u2502       \u2514\u2500\u250038x76 region at (52, 105)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500156x406 region at (480, 173)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (623, 272)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (498, 270)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (561, 244)\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (652, 261)\n    \u2502   \u251c\u2500\u2500156x406 region at (690, 11)\n    \u2502   \u2502   \u251c\u2500\u250038x82 region at (690, 122)\n    \u2502   \u2502   \u251c\u2500\u250038x95 region at (690, 13)\n    \u2502   \u2502   \u251c\u2500\u250037x54 region at (690, 129)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (919, 95)\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (805, 59)\n    \u2502   \u251c\u2500\u2500149x406 region at (570, 0)\n    \u2502   \u2502   \u2514\u2500\u250023x71 region at (904, 125)\n    \u2502   \u251c\u2500\u2500156x332 region at (1296, 381)\n    \u2502   \u2502   \u251c\u2500\u250038x53 region at (1296, 403)\n    \u2502   \u2502   \u251c\u2500\u250035x82 region at (1469, 381)\n    \u2502   \u2502   \u2514\u2500\u250038x82 region at (1328, 457)\n    \u2502   \u2514\u2500\u2500156x292 region at (0, 16)\n    \u2502       \u2514\u2500\u250038x65 region at (0, 129)\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u250099x213 region at (1415, 0)\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1426, 71)\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1463, 37)\n    \u2502   \u2502   \u2514\u2500\u250024x52 region at (1525, 31)\n    \u2502   \u2514\u2500\u2500116x406 region at (678, 509)\n    \u2502       \u251c\u2500\u250028x100 region at (929, 544)\n    \u2502       \u2514\u2500\u250028x76 region at (1007, 512)\n    \u2514\u2500\u2500626x1629 image demo_image\n        \u251c\u2500\u2500156x278 region at (0, 234)\n        \u2502   \u2514\u2500\u250038x68 region at (144, 330)\n        \u251c\u2500\u2500156x406 region at (786, 133)\n        \u2502   \u251c\u2500\u250038x100 region at (891, 223)\n        \u2502   \u251c\u2500\u250038x64 region at (786, 154)\n        \u2502   \u251c\u2500\u250038x100 region at (1000, 245)\n        \u2502   \u2514\u2500\u250038x100 region at (911, 242)\n        \u251c\u2500\u2500156x406 region at (1105, 461)\n        \u2502   \u251c\u2500\u250029x100 region at (1170, 587)\n        \u2502   \u251c\u2500\u250038x100 region at (1194, 571)\n        \u2502   \u2514\u2500\u250038x100 region at (1219, 509)\n        \u2514\u2500\u250090x406 region at (442, 0)\n            \u251c\u2500\u250022x91 region at (442, 14)\n            \u251c\u2500\u250013x67 region at (780, 0)\n            \u251c\u2500\u250022x100 region at (681, 18)\n            \u251c\u2500\u250021x100 region at (554, 0)\n            \u2514\u2500\u250022x100 region at (667, 6)\n</pre> <p>When the segmentation is done, the segments can be passed to a text recognition model. The results are passed to the workbench in the same manner as before:</p> In\u00a0[8]: Copied! <pre>from htrflow_core.models.dummy_models import RecognitionModel\n\n\nrecognition_model = RecognitionModel()\nresults = recognition_model(volume.segments())\nvolume.update(results)\nprint(volume)\n</pre> from htrflow_core.models.dummy_models import RecognitionModel   recognition_model = RecognitionModel() results = recognition_model(volume.segments()) volume.update(results) print(volume) <pre>\u2514\u2500\u2500&lt;htrflow_core.volume.Node object at 0x7f5aa834c1c0&gt;\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u2514\u2500\u2500156x406 region at (345, 11)\n    \u2502       \u251c\u2500\u250037x100 region at (517, 129) \"Dolor velit non non tempora magnam ut adipisci.\"\n    \u2502       \u251c\u2500\u250022x100 region at (636, 144) \"Dolor quiquia quisquam adipisci velit velit quiquia quiquia.\"\n    \u2502       \u251c\u2500\u250038x100 region at (543, 125) \"Ipsum labore dolorem ut neque ipsum velit.\"\n    \u2502       \u251c\u2500\u250038x100 region at (486, 122) \"Consectetur est numquam voluptatem quiquia ipsum.\"\n    \u2502       \u2514\u2500\u250038x69 region at (681, 38) \"Magnam etincidunt consectetur neque quaerat ut sit ipsum.\"\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500117x406 region at (17, 0)\n    \u2502   \u2502   \u2514\u2500\u250028x100 region at (216, 70) \"Modi sed non tempora.\"\n    \u2502   \u251c\u2500\u2500156x406 region at (948, 262)\n    \u2502   \u2502   \u251c\u2500\u250033x100 region at (1070, 384) \"Numquam quiquia ut etincidunt sit quaerat adipisci.\"\n    \u2502   \u2502   \u251c\u2500\u250038x87 region at (948, 359) \"Est etincidunt dolore modi.\"\n    \u2502   \u2502   \u2514\u2500\u250038x57 region at (1296, 329) \"Dolore ut tempora numquam voluptatem dolorem etincidunt non.\"\n    \u2502   \u2514\u2500\u2500156x309 region at (0, 85)\n    \u2502       \u251c\u2500\u250038x76 region at (7, 159) \"Numquam amet quisquam magnam modi.\"\n    \u2502       \u251c\u2500\u250038x76 region at (142, 124) \"Dolorem dolorem eius aliquam eius.\"\n    \u2502       \u251c\u2500\u250034x76 region at (218, 85) \"Eius tempora modi sit.\"\n    \u2502       \u251c\u2500\u250038x76 region at (215, 125) \"Tempora labore velit dolor.\"\n    \u2502       \u2514\u2500\u250038x76 region at (52, 105) \"Consectetur neque labore porro quiquia.\"\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u2500156x406 region at (480, 173)\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (623, 272) \"Quaerat sed ipsum tempora.\"\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (498, 270) \"Ipsum aliquam consectetur dolor.\"\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (561, 244) \"Sed magnam aliquam aliquam dolor.\"\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (652, 261) \"Sed dolor amet sed adipisci etincidunt.\"\n    \u2502   \u251c\u2500\u2500156x406 region at (690, 11)\n    \u2502   \u2502   \u251c\u2500\u250038x82 region at (690, 122) \"Voluptatem aliquam aliquam porro amet.\"\n    \u2502   \u2502   \u251c\u2500\u250038x95 region at (690, 13) \"Modi aliquam quiquia etincidunt labore.\"\n    \u2502   \u2502   \u251c\u2500\u250037x54 region at (690, 129) \"Tempora dolore quiquia ipsum neque consectetur tempora.\"\n    \u2502   \u2502   \u251c\u2500\u250038x100 region at (919, 95) \"Tempora labore modi ut non.\"\n    \u2502   \u2502   \u2514\u2500\u250038x100 region at (805, 59) \"Ut dolorem labore dolore consectetur.\"\n    \u2502   \u251c\u2500\u2500149x406 region at (570, 0)\n    \u2502   \u2502   \u2514\u2500\u250023x71 region at (904, 125) \"Est labore dolor est.\"\n    \u2502   \u251c\u2500\u2500156x332 region at (1296, 381)\n    \u2502   \u2502   \u251c\u2500\u250038x53 region at (1296, 403) \"Neque eius adipisci amet voluptatem consectetur.\"\n    \u2502   \u2502   \u251c\u2500\u250035x82 region at (1469, 381) \"Voluptatem magnam voluptatem labore sed dolore voluptatem.\"\n    \u2502   \u2502   \u2514\u2500\u250038x82 region at (1328, 457) \"Dolore ut magnam voluptatem etincidunt amet adipisci.\"\n    \u2502   \u2514\u2500\u2500156x292 region at (0, 16)\n    \u2502       \u2514\u2500\u250038x65 region at (0, 129) \"Etincidunt etincidunt quiquia porro velit.\"\n    \u251c\u2500\u2500626x1629 image demo_image\n    \u2502   \u251c\u2500\u250099x213 region at (1415, 0)\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1426, 71) \"Etincidunt etincidunt dolorem modi dolorem.\"\n    \u2502   \u2502   \u251c\u2500\u250024x52 region at (1463, 37) \"Neque quaerat dolorem magnam.\"\n    \u2502   \u2502   \u2514\u2500\u250024x52 region at (1525, 31) \"Sed aliquam dolor quisquam numquam.\"\n    \u2502   \u2514\u2500\u2500116x406 region at (678, 509)\n    \u2502       \u251c\u2500\u250028x100 region at (929, 544) \"Velit tempora non quiquia magnam ipsum sed.\"\n    \u2502       \u2514\u2500\u250028x76 region at (1007, 512) \"Dolor sed velit quisquam dolor.\"\n    \u2514\u2500\u2500626x1629 image demo_image\n        \u251c\u2500\u2500156x278 region at (0, 234)\n        \u2502   \u2514\u2500\u250038x68 region at (144, 330) \"Amet adipisci quaerat quiquia sit dolor numquam ut.\"\n        \u251c\u2500\u2500156x406 region at (786, 133)\n        \u2502   \u251c\u2500\u250038x100 region at (891, 223) \"Etincidunt velit ut neque labore quisquam.\"\n        \u2502   \u251c\u2500\u250038x64 region at (786, 154) \"Aliquam labore aliquam quaerat consectetur.\"\n        \u2502   \u251c\u2500\u250038x100 region at (1000, 245) \"Ut non numquam ut.\"\n        \u2502   \u2514\u2500\u250038x100 region at (911, 242) \"Ipsum sed non dolore eius consectetur.\"\n        \u251c\u2500\u2500156x406 region at (1105, 461)\n        \u2502   \u251c\u2500\u250029x100 region at (1170, 587) \"Sed sed magnam tempora velit.\"\n        \u2502   \u251c\u2500\u250038x100 region at (1194, 571) \"Numquam quisquam dolore ut non.\"\n        \u2502   \u2514\u2500\u250038x100 region at (1219, 509) \"Sit amet ipsum neque neque adipisci consectetur.\"\n        \u2514\u2500\u250090x406 region at (442, 0)\n            \u251c\u2500\u250022x91 region at (442, 14) \"Ipsum ut eius sit porro sit.\"\n            \u251c\u2500\u250013x67 region at (780, 0) \"Dolorem voluptatem sed voluptatem non modi quisquam.\"\n            \u251c\u2500\u250022x100 region at (681, 18) \"Sed amet labore dolorem velit aliquam.\"\n            \u251c\u2500\u250021x100 region at (554, 0) \"Sit non amet velit dolorem dolore labore.\"\n            \u2514\u2500\u250022x100 region at (667, 6) \"Dolorem amet amet modi voluptatem.\"\n</pre> In\u00a0[9]: Copied! <pre># Access image 0, region 0, subregion 0\nvolume[0, 0, 0]\n\n# Access image 0, region 0\nvolume[0, 0]\n</pre> # Access image 0, region 0, subregion 0 volume[0, 0, 0]  # Access image 0, region 0 volume[0, 0] Out[9]: <pre>&lt;htrflow_core.volume.RegionNode at 0x7f5a496ef1f0&gt;</pre> <p>The image associated with each node is accessed through the <code>image</code> attribute. The image isn't stored directly in the node, instead, the node refers to the parent image, and crops it according to its box:</p> <pre>class BaseImageNode:\n\n    @property\n    def image(self):\n        x1, x2, y1, y2 = self.box\n        return self.parent.image[y1:y2, x1:x2]\n\n    ...\n</pre> In\u00a0[10]: Copied! <pre>volume[0, 0, 0].image\n</pre> volume[0, 0, 0].image Out[10]: <pre>array([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       ...,\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]]], dtype=uint8)</pre> In\u00a0[11]: Copied! <pre>print(volume[0].coordinate)\n</pre> print(volume[0].coordinate) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 print(volume[0].coordinate)\n\nAttributeError: 'PageNode' object has no attribute 'coordinate'</pre> <p>For first-level regions <code>coordinate</code> is the same as the corner of the segment bounding box.</p> In\u00a0[12]: Copied! <pre>print(\"Coordinate:\", volume[0, 0].coordinate)\nprint(\"Bounding box:\", volume[0, 0].data[\"segment\"].box, \"(x1, x2, y1, y2)\")\n</pre> print(\"Coordinate:\", volume[0, 0].coordinate) print(\"Bounding box:\", volume[0, 0].data[\"segment\"].box, \"(x1, x2, y1, y2)\") <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 print('Coordinate:', volume[0, 0].coordinate)\n      2 print('Bounding box:', volume[0, 0].data['segment'].box, '(x1, x2, y1, y2)')\n\nAttributeError: 'RegionNode' object has no attribute 'coordinate'</pre> <p>But for nested regions the two differ, because <code>coordinate</code> is relative to the original image, while the segment bounding box is relative to the parent region.</p> In\u00a0[13]: Copied! <pre>print(\"Global coordinate:\", volume[0, 0, 0].coordinate)\nprint(\"Local bounding box:\", volume[0, 0, 0].data[\"segment\"].box)\n</pre> print(\"Global coordinate:\", volume[0, 0, 0].coordinate) print(\"Local bounding box:\", volume[0, 0, 0].data[\"segment\"].box) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 print('Global coordinate:', volume[0, 0, 0].coordinate)\n      2 print('Local bounding box:', volume[0, 0, 0].data['segment'].box)\n\nAttributeError: 'RegionNode' object has no attribute 'coordinate'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/demo.html#models-inferencers","title":"Models / inferencers\u00b6","text":"<p>Models &amp; inferencers accept lists of images, and return lists of results (either segmentation or recognition results)</p> <p>I have made a dummy <code>SegmentationModel</code> and <code>RecognitionModel</code> in <code>models.py</code>. These do the same thing as the current inferencers.</p> <pre>class SegmentationModel:\n    def __call__(self, images: list[np.ndarray]) -&gt; list[SegmentationResult]:\n        ...\n\n\n@dataclass\nclass SegmentationResult:\n    boxes: np.ndarray\n    masks: np.ndarray\n    scores: np.ndarray\n    labels: np.ndarray\n</pre> <p>(It would be nice to wrap all models in a \"batching\" function, which divides an input list into chunks if it is too long) -&gt; This is a card in DevOps</p>"},{"location":"notebooks/demo.html#using-the-volume-class","title":"Using the Volume class\u00b6","text":"<p>To load images, create a <code>Volume</code>. The name of this class is not set in stone... It represents what Catrin called a \"batch\", a divison of an archive volume, but I don't want to use \"batch\" because of potential confusion with a model's batch (the number of inputs it operates on simultaneously).</p>"},{"location":"notebooks/demo.html#accessing-nodes","title":"Accessing nodes\u00b6","text":"<p>Specific nodes are accessed by tuple indexing. Here we extract the first line of the first region of the first image:</p>"},{"location":"notebooks/demo.html#coordinates","title":"Coordinates\u00b6","text":"<p>All nodes have a <code>coordinate</code> attribute. This is the location of the node's top-left corner relative to the original image. The base image node's coordinate is thus (0,0):</p>"}]}