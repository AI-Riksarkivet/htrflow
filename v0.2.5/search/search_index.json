{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"HTRflow","text":"Htrflow"},{"location":"index.html#htrflow","title":"HTRflow","text":"<p>HTRflow is an open source tool for HTR and OCR developed by the AI lab at the National Archives of Sweden (Riksarkivet).</p>"},{"location":"index.html#key-features","title":"Key features","text":"<ul> <li>Flexibility: Customize the HTR/OCR process for different kinds of materials.</li> <li>Compatibility: HTRflow supports all models trained by the AI lab - and more!</li> <li>YAML pipelines: HTRflow YAML pipelines are easy to create, modify and share.</li> <li>Export: Export results as Alto XML, Page XML, plain text or JSON.</li> <li>Evaluation: Compare results from different pipelines with ground truth.</li> </ul>"},{"location":"index.html#installation","title":"Installation","text":"<p>Install HTRflow with pip: <pre><code>pip install htrflow\n</code></pre> For more details, see the Installation guide.</p>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>Ready to build your own pipeline for your documents? Head over to the Quickstart guide to get started with HTRflow.</p> <p>The guide will walk you through setting up your first pipeline, utilizing pre-trained models, and seamlessly running HTR/OCR tasks. With the HTRflow CLI, you can quickly set up pipelines using <code>pipeline.yaml</code> files as your \"blueprints\".</p>"},{"location":"api/index.html","title":"Test","text":"<p>Release Notes</p> <p>Releases on Github </p>"},{"location":"api/cli/cli.html","title":"Command Line Interface (CLI)","text":""},{"location":"api/cli/cli.html#htrflow.cli","title":"<code>cli</code>","text":""},{"location":"api/cli/cli.html#htrflow.cli.HTRFLOWLoggingFormatter","title":"<code>HTRFLOWLoggingFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Logging formatter for HTRFLOW</p> Source code in <code>src/htrflow/cli.py</code> <pre><code>def __init__(self):\n    datefmt = \"%Y-%m-%d %H:%M:%S\"\n    hostname = socket.gethostname()\n    fmt = f\"{hostname} - %(asctime)s UTC - %(levelname)s - %(message)s\"\n    super().__init__(fmt, datefmt)\n</code></pre>"},{"location":"api/cli/cli.html#htrflow.cli.get_inputs","title":"<code>get_inputs</code>","text":"<p>Get inputs from the CLI arguments <code>inputs</code> and <code>inputs_file</code></p> <p>The HTRflow CLI accepts inputs in two formats: Either as a list of paths, given as positional arguments:</p> <pre><code>$ htrflow pipeline pipeline.yaml image1.jpg image2.jpg image3.jpg\n</code></pre> <p>or as a path to a text file containing the same information. In this case, the file is given with the argument --inputs-file:</p> <pre><code>$ htrflow pipeline pipeline.yaml --inputs-file=inputs.txt\n</code></pre> <p>This function parses the two arguments and returns a list of input paths.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>list[str] | None</code> <p>The list of inputs given as positional arguments</p> required <code>inputs_file</code> <code>str | None</code> <p>Path to a text file containing inputs</p> required Source code in <code>src/htrflow/cli.py</code> <pre><code>def get_inputs(inputs: list[str] | None, inputs_file: str | None) -&gt; Iterable[str]:\n    \"\"\"\n    Get inputs from the CLI arguments `inputs` and `inputs_file`\n\n    The HTRflow CLI accepts inputs in two formats: Either as a list of paths,\n    given as positional arguments:\n\n        $ htrflow pipeline pipeline.yaml image1.jpg image2.jpg image3.jpg\n\n    or as a path to a text file containing the same information. In this case,\n    the file is given with the argument --inputs-file:\n\n        $ htrflow pipeline pipeline.yaml --inputs-file=inputs.txt\n\n    This function parses the two arguments and returns a list of input paths.\n\n    Arguments:\n        inputs: The list of inputs given as positional arguments\n        inputs_file: Path to a text file containing inputs\n\n    Raises:\n        typer.BadParameter if both `inputs` and `inputs_file` are None, or\n        if both are given.\n    \"\"\"\n    if inputs is not None:\n        if inputs_file:\n            raise typer.BadParameter(\n                f\"Please provide only one of INPUTS and --inputs-file (got INPUTS={inputs} and --inputs-file={inputs_file})\"\n            )\n        return inputs\n\n    if inputs_file:\n        with open(inputs_file, \"r\") as f:\n            inputs = map(str.strip, f.readlines())\n        return inputs\n\n    raise typer.BadParameter(\"Missing input files. Please provide either INPUTS or --inputs-file.\")\n</code></pre>"},{"location":"api/cli/cli.html#htrflow.cli.run_evaluation","title":"<code>run_evaluation</code>","text":"<p>Evaluate HTR transcriptions against ground truth</p> Source code in <code>src/htrflow/cli.py</code> <pre><code>@app.command(\"evaluate\")\ndef run_evaluation(\n    gt: Annotated[\n        str,\n        typer.Argument(\n            help=\"Path to directory with ground truth files. Should have two subdirectories `images` and `xmls`.\"\n        ),\n    ],\n    candidates: Annotated[\n        list[str],\n        typer.Argument(help=\"Paths to pipelines or directories containing already generated Page XMLs.\"),\n    ],\n):\n    \"\"\"\n    Evaluate HTR transcriptions against ground truth\n    \"\"\"\n\n    from htrflow.evaluate import evaluate\n    from htrflow.pipeline.pipeline import Pipeline\n    from htrflow.pipeline.steps import Export\n\n    run_name = datetime.now().strftime(\"run_%Y%m%d_%H%M%S\")\n    run_dir = os.path.join(\"evaluation\", run_name)\n    os.makedirs(run_dir)\n\n    # inputs are pipelines -&gt; run the pipelines before evaluation\n    if all(os.path.isfile(candidate) for candidate in candidates):\n        images = os.path.join(gt, \"images\")\n        pipelines = candidates\n        candidates = []\n        for i, pipe in enumerate(pipelines):\n            # Create a directory under `run_dir` to save pipeline results,\n            # logs and a copy of the pipeline yaml to.\n            pipeline_name = f\"pipeline{i}_{os.path.splitext(os.path.basename(pipe))[0]}\"\n            pipeline_dir = os.path.join(run_dir, pipeline_name)\n            os.mkdir(pipeline_dir)\n            shutil.copy(pipe, os.path.join(pipeline_dir, pipeline_name + \".yaml\"))\n\n            with open(pipe, \"r\") as file:\n                pipeline = Pipeline.from_config(yaml.safe_load(file))\n            pipeline.steps.append(Export(run_dir, \"page\"))\n\n            # Run the pipeline\n            run_pipeline(pipeline, images, logfile=os.path.join(pipeline_dir, \"htrflow.log\"), label=pipeline_name)\n            candidates.append(os.path.join(run_dir, pipeline_name))\n\n    df = evaluate(gt, *candidates)\n    df.to_csv(os.path.join(run_dir, \"evaluation_results.csv\"))\n</code></pre>"},{"location":"api/cli/cli.html#htrflow.cli.run_pipeline","title":"<code>run_pipeline</code>","text":"<p>Run a HTRflow pipeline</p> Source code in <code>src/htrflow/cli.py</code> <pre><code>@app.command(\"pipeline\")\ndef run_pipeline(\n    pipeline: Annotated[str, typer.Argument(help=\"Path to a HTRflow pipeline YAML file\")],\n    inputs: Annotated[\n        list[str] | None,\n        typer.Argument(help=\"Paths to input images. May be paths to directories of images or paths to single images.\"),\n    ] = None,\n    logfile: Annotated[\n        str,\n        typer.Option(help=\"Where to write logs to. If not provided, logs will be printed to the standard output.\"),\n    ] = None,\n    loglevel: Annotated[LogLevel, typer.Option(help=\"Loglevel\", case_sensitive=False)] = LogLevel.info,\n    backup: Annotated[bool, typer.Option(help=\"Save a pickled backup after each pipeline step.\")] = False,\n    batch_output: Annotated[\n        int | None,\n        typer.Option(help=\"Write continuous output in batches of this size (number of images).\"),\n    ] = 1,\n    label: Annotated[\n        str | None,\n        typer.Option(help=\"Collection label\"),\n    ] = None,\n    inputs_file: Annotated[\n        str | None,\n        typer.Option(\n            help=\"A text file containing newline-separated paths to input images. Requires INPUTS to be empty.\"\n        ),\n    ] = None,\n):\n    \"\"\"Run a HTRflow pipeline\"\"\"\n\n    logger = setup_pipeline_logging(logfile, loglevel)\n    inputs = get_inputs(inputs, inputs_file)\n\n    # Slow imports! Only import after all CLI arguments have been resolved.\n    from htrflow.models import hf_utils\n    from htrflow.pipeline.pipeline import Pipeline\n    from htrflow.pipeline.steps import auto_import\n\n    if isinstance(pipeline, Pipeline):\n        pipe = pipeline\n        config = {}\n    else:\n        with open(pipeline, \"r\") as file:\n            config = yaml.safe_load(file)\n        pipe = Pipeline.from_config(config)\n\n    hf_utils.HF_CONFIG |= config.get(\"huggingface_config\", {})\n    pipe.do_backup = backup\n\n    tic = time.time()\n    collections = auto_import(inputs, max_size=batch_output)\n    n_pages = 0\n    for collection in collections:\n        if \"labels\" in config:\n            collection.set_label_format(**config[\"labels\"])\n        if label:\n            collection.label = label\n        collection = pipe.run(collection)\n        n_pages += len(collection.pages)\n    toc = time.time()\n\n    total_time = toc - tic\n    logger.info(\n        \"Processed %d pages in %d seconds (average %.3f seconds per page)\",\n        n_pages,\n        total_time,\n        total_time / n_pages if n_pages &gt; 0 else -1.0,\n    )\n</code></pre>"},{"location":"api/results/results.html","title":"Results","text":""},{"location":"api/results/results.html#htrflow.results","title":"<code>results</code>","text":""},{"location":"api/results/results.html#htrflow.results.RecognizedText","title":"<code>RecognizedText</code>  <code>dataclass</code>","text":"<p>Recognized text class</p> <p>This class represents a result from a text recognition model.</p> <p>Attributes:</p> Name Type Description <code>texts</code> <code>list[str]</code> <p>A sequence of candidate texts</p> <code>scores</code> <code>list[float]</code> <p>The scores of the candidate texts</p>"},{"location":"api/results/results.html#htrflow.results.RecognizedText.top_candidate","title":"<code>top_candidate</code>","text":"<p>The candidate with the highest confidence score</p> Source code in <code>src/htrflow/results.py</code> <pre><code>def top_candidate(self) -&gt; str:\n    \"\"\"The candidate with the highest confidence score\"\"\"\n    return self.texts[self.scores.index(self.top_score())]\n</code></pre>"},{"location":"api/results/results.html#htrflow.results.RecognizedText.top_score","title":"<code>top_score</code>","text":"<p>The highest confidence score</p> Source code in <code>src/htrflow/results.py</code> <pre><code>def top_score(self):\n    \"\"\"The highest confidence score\"\"\"\n    return max(self.scores)\n</code></pre>"},{"location":"api/results/results.html#htrflow.results.Result","title":"<code>Result</code>","text":"<p>A result from an arbitrary model (or process)</p> <p>One result instance corresponds to one input image.</p> <p>Attributes:</p> Name Type Description <code>metadata</code> <p>Metadata regarding the result, model-dependent.</p> <code>segments</code> <p><code>Segment</code> instances representing results from an object detection or instance segmentation model, or similar. May be empty if not applicable.</p> <code>data</code> <p>Any other data associated with the result.</p> <p>Create a Result</p> <p>See also the alternative constructors Result.text_recognition_result, Result.segmentation_result and Result.word_segmentation_result.</p> Source code in <code>src/htrflow/results.py</code> <pre><code>def __init__(\n    self,\n    metadata: dict[str, str] | None = None,\n    segments: Sequence[Segment] | None = None,\n    data: dict[str, Any] = None,\n    text: RecognizedText | None = None,\n):\n    \"\"\"Create a Result\n\n    See also the alternative constructors Result.text_recognition_result,\n    Result.segmentation_result and Result.word_segmentation_result.\n    \"\"\"\n    self.metadata = metadata or {}\n    self.segments = segments or []\n    self.data = data or {}\n    if text is not None:\n        self.data.update({TEXT_RESULT_KEY: text})\n</code></pre>"},{"location":"api/results/results.html#htrflow.results.Result.bboxes","title":"<code>bboxes</code>  <code>property</code>","text":"<p>Bounding boxes relative to input image</p>"},{"location":"api/results/results.html#htrflow.results.Result.class_labels","title":"<code>class_labels</code>  <code>property</code>","text":"<p>Class labels of segments</p>"},{"location":"api/results/results.html#htrflow.results.Result.global_masks","title":"<code>global_masks</code>  <code>property</code>","text":"<p>Global masks relative to input image</p>"},{"location":"api/results/results.html#htrflow.results.Result.local_mask","title":"<code>local_mask</code>  <code>property</code>","text":"<p>Local masks relative to bounding boxes</p>"},{"location":"api/results/results.html#htrflow.results.Result.polygons","title":"<code>polygons</code>  <code>property</code>","text":"<p>Polygons relative to input image</p>"},{"location":"api/results/results.html#htrflow.results.Result.drop_indices","title":"<code>drop_indices</code>","text":"<p>Drop segments from result</p> <p>Example: Given a <code>Result</code> with three segments s0, s1 and s2, index = [0, 2] will drop segments s0 and s2.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Sequence[int]</code> <p>Indices of segments to drop</p> required Source code in <code>src/htrflow/results.py</code> <pre><code>def drop_indices(self, index: Sequence[int]) -&gt; None:\n    \"\"\"Drop segments from result\n\n    Example: Given a `Result` with three segments s0, s1 and s2,\n    index = [0, 2] will drop segments s0 and s2.\n\n    Arguments:\n        index: Indices of segments to drop\n    \"\"\"\n    keep = [i for i in range(len(self.segments)) if i not in index]\n    self.reorder(keep)\n</code></pre>"},{"location":"api/results/results.html#htrflow.results.Result.filter","title":"<code>filter</code>","text":"<p>Filter segments and data based on a predicate applied to a specified key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key in the data dictionary to test the predicate against.</p> required <code>predicate</code> <code>[Callable]</code> <p>A function that takes a value associated with the key</p> required <p>Example: <pre><code>&gt;&gt;&gt; def remove_certain_text(text_results):\n&gt;&gt;&gt;    return text_results != 'lorem'\n&gt;&gt;&gt; result.filter('text_results', remove_certain_text)\nTrue\n</code></pre></p> Source code in <code>src/htrflow/results.py</code> <pre><code>def filter(self, key: str, predicate: Callable[[Any], bool]) -&gt; None:\n    \"\"\"Filter segments and data based on a predicate applied to a specified key.\n\n    Args:\n        key: The key in the data dictionary to test the predicate against.\n        predicate [Callable]: A function that takes a value associated with the key\n        and returns True if the segment should be kept.\n\n    Example:\n    ```\n    &gt;&gt;&gt; def remove_certain_text(text_results):\n    &gt;&gt;&gt;    return text_results != 'lorem'\n    &gt;&gt;&gt; result.filter('text_results', remove_certain_text)\n    True\n    ```\n    \"\"\"\n    keep = [i for i, item in enumerate(self.data) if predicate(item.get(key, None))]\n    self.reorder(keep)\n</code></pre>"},{"location":"api/results/results.html#htrflow.results.Result.reorder","title":"<code>reorder</code>","text":"<p>Reorder result</p> <p>Example: Given a <code>Result</code> with three segments s0, s1 and s2, index = [2, 0, 1] will put the segments in order [s2, s0, s1]. Any indices not in <code>index</code> will be dropped from the result.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Sequence[int]</code> <p>A list of indices representing the new ordering.</p> required Source code in <code>src/htrflow/results.py</code> <pre><code>def reorder(self, index: Sequence[int]) -&gt; None:\n    \"\"\"Reorder result\n\n    Example: Given a `Result` with three segments s0, s1 and s2,\n    index = [2, 0, 1] will put the segments in order [s2, s0, s1].\n    Any indices not in `index` will be dropped from the result.\n\n    Arguments:\n        index: A list of indices representing the new ordering.\n    \"\"\"\n    if self.segments:\n        self.segments = [self.segments[i] for i in index]\n</code></pre>"},{"location":"api/results/results.html#htrflow.results.Result.rescale","title":"<code>rescale</code>","text":"<p>Rescale the Result's segments</p> Source code in <code>src/htrflow/results.py</code> <pre><code>def rescale(self, factor: float):\n    \"\"\"Rescale the Result's segments\"\"\"\n    for segment in self.segments:\n        segment.rescale(factor)\n</code></pre>"},{"location":"api/results/results.html#htrflow.results.Result.segmentation_result","title":"<code>segmentation_result</code>  <code>classmethod</code>","text":"<p>Create a segmentation result</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <p>The original image</p> required <code>metadata</code> <code>dict[str, Any]</code> <p>Result metadata</p> required <code>segments</code> <p>The segments</p> required <p>Returns:</p> Type Description <code>Result</code> <p>A Result instance with the specified data and no texts.</p> Source code in <code>src/htrflow/results.py</code> <pre><code>@classmethod\ndef segmentation_result(\n    cls,\n    orig_shape: tuple[int, int],\n    metadata: dict[str, Any],\n    bboxes: Sequence[Bbox | Iterable[int]] | None = None,\n    masks: Sequence[Mask] | None = None,\n    polygons: Sequence[Polygon] | None = None,\n    scores: Iterable[float] | None = None,\n    labels: Iterable[str] | None = None,\n) -&gt; \"Result\":\n    \"\"\"Create a segmentation result\n\n    Arguments:\n        image: The original image\n        metadata: Result metadata\n        segments: The segments\n\n    Returns:\n        A Result instance with the specified data and no texts.\n    \"\"\"\n    segments = []\n    for item in _zip_longest_none(bboxes, masks, scores, labels, polygons):\n        segment = Segment(*item, orig_shape=orig_shape)\n        if segment.bbox.area &gt; 0:\n            segments.append(segment)\n    return cls(metadata, segments=segments)\n</code></pre>"},{"location":"api/results/results.html#htrflow.results.Result.text_recognition_result","title":"<code>text_recognition_result</code>  <code>classmethod</code>","text":"<p>Create a text recognition result</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[str, Any]</code> <p>Result metadata</p> required <code>text</code> <p>The recognized text</p> required <p>Returns:</p> Type Description <code>Result</code> <p>A Result instance with the specified data and no segments.</p> Source code in <code>src/htrflow/results.py</code> <pre><code>@classmethod\ndef text_recognition_result(cls, metadata: dict[str, Any], texts: list[str], scores: list[float]) -&gt; \"Result\":\n    \"\"\"Create a text recognition result\n\n    Arguments:\n        metadata: Result metadata\n        text: The recognized text\n\n    Returns:\n        A Result instance with the specified data and no segments.\n    \"\"\"\n    return cls(metadata, text=RecognizedText(texts, scores))\n</code></pre>"},{"location":"api/results/results.html#htrflow.results.Segment","title":"<code>Segment</code>","text":"<p>Segment class</p> <p>Class representing a segment of an image, typically a result from a segmentation model or a detection model.</p> <p>Attributes:</p> Name Type Description <code>bbox</code> <code>Bbox</code> <p>The bounding box of the segment</p> <code>mask</code> <code>Mask | None</code> <p>The segment's mask, if available. The mask is stored relative to the bounding box. Use the <code>global_mask()</code> method to retrieve the mask relative to the original image.</p> <code>score</code> <code>float | None</code> <p>Segment confidence score, if available.</p> <code>class_label</code> <code>str | None</code> <p>Segment class label, if available.</p> <code>polygon</code> <code>Polygon | None</code> <p>An approximation of the segment mask, relative to the original image. If no mask is available, <code>polygon</code> defaults to a polygon representation of the segment's bounding box.</p> <code>orig_shape</code> <code>tuple[int, int] | None</code> <p>The shape of the orginal input image.</p> <p>Create a <code>Segment</code> instance</p> <p>A segment can be created from a bounding box, a polygon, a mask or any combination of the three.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>tuple[int, int, int, int] | Bbox | None</code> <p>The segment's bounding box, as either a <code>geometry.Bbox</code> instance or as a (xmin, ymin, xmax, ymax) tuple. Required if <code>mask</code> and <code>polygon</code> are None. Defaults to None.</p> <code>None</code> <code>mask</code> <code>Mask | None</code> <p>The segment's mask relative to the original input image. Required if both <code>polygon</code> and <code>bbox</code> are None. Defaults to None.</p> <code>None</code> <code>score</code> <code>float | None</code> <p>Segment confidence score. Defaults to None.</p> <code>None</code> <code>class_label</code> <code>str | None</code> <p>Segment class label. Defaults to None.</p> <code>None</code> <code>polygon</code> <code>Polygon | Sequence[tuple[int, int]] | None</code> <p>A polygon defining the segment, relative to the input image. Defaults to None. Required if both <code>mask</code> and <code>bbox</code> are None.</p> <code>None</code> <code>orig_shape</code> <code>tuple[int, int] | None</code> <p>The shape of the orginal input image. Defaults to None.</p> <code>None</code> Source code in <code>src/htrflow/results.py</code> <pre><code>def __init__(\n    self,\n    bbox: tuple[int, int, int, int] | Bbox | None = None,\n    mask: Mask | None = None,\n    score: float | None = None,\n    class_label: str | None = None,\n    polygon: Polygon | Sequence[tuple[int, int]] | None = None,\n    orig_shape: tuple[int, int] | None = None,\n    data: dict[str, Any] | None = None,\n):\n    \"\"\"Create a `Segment` instance\n\n    A segment can be created from a bounding box, a polygon, a mask\n    or any combination of the three.\n\n    Arguments:\n        bbox: The segment's bounding box, as either a `geometry.Bbox`\n            instance or as a (xmin, ymin, xmax, ymax) tuple. Required\n            if `mask` and `polygon` are None. Defaults to None.\n        mask: The segment's mask relative to the original input image.\n            Required if both `polygon` and `bbox` are None. Defaults\n            to None.\n        score: Segment confidence score. Defaults to None.\n        class_label: Segment class label. Defaults to None.\n        polygon: A polygon defining the segment, relative to the input\n            image. Defaults to None. Required if both `mask` and `bbox`\n            are None.\n        orig_shape: The shape of the orginal input image. Defaults to\n            None.\n    \"\"\"\n    if all(item is None for item in (bbox, mask, polygon)):\n        raise ValueError(\"Cannot create a Segment without bbox, mask or polygon\")\n\n    # Mask is given: Compute a polygon and a bounding box from the mask\n    if mask is not None:\n        bbox = geometry.mask2bbox(mask)\n        polygon = geometry.mask2polygon(mask)\n        mask = imgproc.crop(mask, bbox)\n\n    # Polygon is given: Compute a bounding box and possibly mask\n    elif polygon is not None:\n        polygon = geometry.Polygon(polygon)\n        bbox = polygon.bbox()\n        if orig_shape:\n            mask = geometry.polygon2mask(polygon, orig_shape)\n            mask = imgproc.crop(mask, Bbox(*bbox))\n\n    self.bbox = geometry.Bbox(*bbox)\n    self.polygon = polygon\n    self.mask = mask\n    self.score = score\n    self.class_label = class_label\n    self.orig_shape = orig_shape\n    self.data = data or {}\n</code></pre>"},{"location":"api/results/results.html#htrflow.results.Segment.global_mask","title":"<code>global_mask</code>  <code>property</code>","text":"<p>The segment mask relative to the original input image.</p> <p>Parameters:</p> Name Type Description Default <code>orig_shape</code> <p>Pass this argument to use another original shape than the segment's <code>orig_shape</code> attribute. Defaults to None.</p> required"},{"location":"api/results/results.html#htrflow.results.Segment.local_mask","title":"<code>local_mask</code>  <code>property</code>","text":"<p>The segment mask relative to the bounding box (alias for self.mask)</p>"},{"location":"api/results/results.html#htrflow.results.Segment.approximate_mask","title":"<code>approximate_mask</code>","text":"<p>A lower resolution version of the global mask</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>Size of approximate mask relative to the original.</p> required Source code in <code>src/htrflow/results.py</code> <pre><code>def approximate_mask(self, ratio: float) -&gt; Mask | None:\n    \"\"\"A lower resolution version of the global mask\n\n    Arguments:\n        ratio: Size of approximate mask relative to the original.\n    \"\"\"\n    global_mask = self.global_mask\n    if global_mask is None:\n        return None\n    return imgproc.rescale(global_mask, ratio)\n</code></pre>"},{"location":"api/results/results.html#htrflow.results.Segment.rescale","title":"<code>rescale</code>","text":"<p>Rescale the segment's mask, bounding box and polygon by <code>factor</code></p> Source code in <code>src/htrflow/results.py</code> <pre><code>def rescale(self, factor: float) -&gt; None:\n    \"\"\"Rescale the segment's mask, bounding box and polygon by `factor`\"\"\"\n    if self.mask is not None:\n        self.mask = imgproc.rescale_linear(self.mask, factor)\n    self.bbox = self.bbox.rescale(factor)\n    if self.polygon is not None:\n        self.polygon = self.polygon.rescale(factor)\n</code></pre>"},{"location":"api/utils/draw.html","title":"Draw","text":""},{"location":"api/utils/draw.html#htrflow.utils.draw.draw_bboxes","title":"<code>draw_bboxes</code>","text":"<p>Draw bounding boxes on image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image</p> required <code>bboxes</code> <code>Iterable[Bbox]</code> <p>List of bounding boxes to draw</p> required <code>color</code> <code>Color</code> <p>Box border color</p> <code>BLUE</code> <code>thickness</code> <code>int</code> <p>Box border thickness</p> <code>3</code> <p>Returns:     A copy of the input image with the bounding boxes drawn.</p> Source code in <code>src/htrflow/utils/draw.py</code> <pre><code>def draw_bboxes(\n    image: np.ndarray,\n    bboxes: Iterable[Bbox],\n    color: Color = Colors.BLUE,\n    thickness: int = 3,\n    alpha: float = 0.2,\n    labels: Optional[Sequence[str]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Draw bounding boxes on image\n\n    Args:\n        image: The input image\n        bboxes: List of bounding boxes to draw\n        color: Box border color\n        thickness: Box border thickness\n    Returns:\n        A copy of the input image with the bounding boxes drawn.\n    \"\"\"\n    polygons = [bbox.polygon() for bbox in bboxes]\n    return draw_polygons(image, polygons, color, thickness, alpha, labels)\n</code></pre>"},{"location":"api/utils/draw.html#htrflow.utils.draw.draw_masks","title":"<code>draw_masks</code>","text":"<p>Draw masks on image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image</p> required <code>masks</code> <code>Iterable[Mask]</code> <p>The masks</p> required <code>color</code> <code>Color</code> <p>Mask color</p> <code>BLUE</code> <code>alpha</code> <code>float</code> <p>Mask opacity</p> <code>0.2</code> <p>Returns:     A copy of the input image with the masked areas colored.</p> Source code in <code>src/htrflow/utils/draw.py</code> <pre><code>def draw_masks(\n    image: np.ndarray,\n    masks: Iterable[Mask],\n    color: Color = Colors.BLUE,\n    alpha: float = 0.2,\n) -&gt; np.ndarray:\n    \"\"\"Draw masks on image\n\n    Args:\n        image: The input image\n        masks: The masks\n        color: Mask color\n        alpha: Mask opacity\n    Returns:\n        A copy of the input image with the masked areas colored.\n    \"\"\"\n    for mask_ in masks:\n        masked = imgproc.mask(image, mask_, inverse=True, fill=color)\n        image = image * (1 - alpha) + masked * alpha\n    return image\n</code></pre>"},{"location":"api/utils/draw.html#htrflow.utils.draw.draw_polygons","title":"<code>draw_polygons</code>","text":"<p>Draw polygons on image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image</p> required <code>polygons</code> <code>Iterable[Polygon]</code> <p>The polygons</p> required <code>color</code> <code>Color</code> <p>Fill and border color</p> <code>BLUE</code> <code>alpha</code> <code>float</code> <p>Opacity of the fill</p> <code>0.2</code> <code>labels</code> <code>Optional[Sequence[str]]</code> <p>Polygon labels</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A copy of the input image with polygons drawn on it.</p> Source code in <code>src/htrflow/utils/draw.py</code> <pre><code>def draw_polygons(\n    image: np.ndarray,\n    polygons: Iterable[Polygon],\n    color: Color = Colors.BLUE,\n    thickness: int = 3,\n    alpha: float = 0.2,\n    labels: Optional[Sequence[str]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Draw polygons on image\n\n    Args:\n        image: The input image\n        polygons: The polygons\n        color: Fill and border color\n        alpha: Opacity of the fill\n        labels: Polygon labels\n\n    Returns:\n        A copy of the input image with polygons drawn on it.\n    \"\"\"\n    image = image.copy()\n    polygons = [polygon.as_nparray() for polygon in polygons]\n    cv2.polylines(image, polygons, isClosed=True, color=color, thickness=thickness)\n    if alpha &gt; 0:\n        for polygon in polygons:\n            filled = cv2.fillPoly(image.copy(), [polygon], color=color)\n            image = image * (1 - alpha) + filled * alpha\n\n    labels = labels if labels else []\n    for label, polygon in zip(labels, polygons):\n        x = min(x for x, _ in polygon)\n        y = min(y for _, y in polygon)\n        image = draw_label(image, label, (x, y), bg_color=color, font_thickness=thickness)\n\n    return image\n</code></pre>"},{"location":"api/utils/draw.html#htrflow.utils.draw.draw_label","title":"<code>draw_label</code>","text":"<p>Draw label on image</p> <p>This function draws text on the image. It tries to put the text at the given position, but will move it downwards if the requested position would put the text above the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image</p> required <code>label</code> <code>str</code> <p>Text to write on image</p> required <code>pos</code> <code>tuple[int, int]</code> <p>(x, y) coordinate of the text's bottom left corner</p> required <code>font</code> <p>A cv2 font</p> <code>FONT_HERSHEY_SIMPLEX</code> <code>font_scale</code> <code>int</code> <p>Font scale</p> <code>2</code> <code>font_thickness</code> <code>int</code> <p>Font thickness</p> <code>2</code> <code>font_color</code> <code>Color</code> <p>Font color</p> <code>WHITE</code> <code>bg_color</code> <code>Optional[Color]</code> <p>Optional background color</p> <code>BLACK</code> Source code in <code>src/htrflow/utils/draw.py</code> <pre><code>def draw_label(\n    image: np.ndarray,\n    label: str,\n    pos: tuple[int, int],\n    font=cv2.FONT_HERSHEY_SIMPLEX,\n    font_scale: int = 2,\n    font_thickness: int = 2,\n    font_color: Color = Colors.WHITE,\n    bg_color: Optional[Color] = Colors.BLACK,\n):\n    \"\"\"Draw label on image\n\n    This function draws text on the image. It tries to put the text at\n    the given position, but will move it downwards if the requested\n    position would put the text above the image.\n\n    Arguments:\n        image: Input image\n        label: Text to write on image\n        pos: (x, y) coordinate of the text's bottom left corner\n        font: A cv2 font\n        font_scale: Font scale\n        font_thickness: Font thickness\n        font_color: Font color\n        bg_color: Optional background color\n    \"\"\"\n\n    (text_w, text_h), _ = cv2.getTextSize(label, font, font_scale, font_thickness)\n\n    # Adjust y coordinate if needed. It must be at least equal to the\n    # height of the text to ensure that the text is visible, or else\n    # the text ends up above the image.\n    pos = pos[0], max(pos[1], text_h)\n\n    if bg_color is not None:\n        x, y = pos\n        pad = font_thickness\n        p1 = x - pad, y + pad\n        p2 = x + text_w + pad, y - text_h + pad\n        image = cv2.rectangle(image, p1, p2, bg_color, -1)\n\n    return cv2.putText(image, label, pos, font, font_scale, font_color, font_thickness)\n</code></pre>"},{"location":"api/utils/draw.html#htrflow.utils.draw.draw_reading_order","title":"<code>draw_reading_order</code>","text":"<p>draw_reading_order()</p> <p>Not yet implemented</p> Source code in <code>src/htrflow/utils/draw.py</code> <pre><code>def draw_reading_order():\n    \"\"\"draw_reading_order()\n\n    Not yet implemented\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/utils/geometry.html","title":"Geometry","text":""},{"location":"api/utils/imgproc.html","title":"Image Processing","text":""},{"location":"api/utils/layout.html","title":"Layout","text":""},{"location":"api/volume/node.html","title":"Noe","text":""},{"location":"api/volume/volume.html","title":"Volume","text":""},{"location":"getting_started/index.html","title":"Getting Started (Work in progress...)","text":"<ul> <li> <p> How to run Htrflow pipeline</p> <p>(WIP)</p> <p> Getting started</p> </li> <li> <p> How to add model</p> <p>(WIP)</p> <p> Reference</p> </li> <li> <p> The data structure</p> <p>(WIP)</p> <p> Customization</p> </li> <li> <p> Different serialization</p> <p>(WIP)</p> <p> License</p> </li> </ul>"},{"location":"getting_started/collection.html","title":"Collection","text":"<p>Warning</p> <p>Experimental - Syntax or Docs might change</p> <p>The Collection class is the core data structure in HTRFlow, designed to manage and process document pages in a hierarchical structure. It provides a flexible and intuitive way to handle document analysis tasks, from page segmentation to text recognition.</p>"},{"location":"getting_started/collection.html#overview","title":"Overview","text":"<p>The Collection class maintains a tree structure where:</p> <ul> <li>Root level contains PageNode objects (individual document pages)</li> <li>Pages can contain regions (text blocks, tables, etc.)</li> <li>Regions can contain paragraphs or lines of text</li> <li>Lines contain individual words</li> </ul> <p>Each node in the tree has associated attributes like position (coordinates), dimensions, and potentially recognized text.</p> <p>Note that the the Collection underneath consists of three main components that work together:</p> <ol> <li>Collection: The root container managing document pages and their hierarchy</li> <li>Result: Processing outputs that update the Collection's structure</li> <li>Geometry: Spatial utilities used throughout the Collection tree</li> </ol> <p>Here's how they interact: <pre><code>Collection\n\u251c\u2500\u2500 Manages PageNodes\n\u2502   \u2514\u2500\u2500 Updated by Results\n\u2502       \u2514\u2500\u2500 Uses Geometry for spatial operations\n</code></pre></p>"},{"location":"getting_started/collection.html#basic-usage","title":"Basic Usage","text":"<p>Here's a typical workflow using Collection with a pipeline:</p> <pre><code>from htrflow.pipeline.pipeline import Pipeline\nfrom htrflow.volume.volume import Collection\nimport yaml\n\n# Create collection from images\ncollection = Collection(['image1.jpg', 'image2.jpg'])\n\n# Define pipeline configuration\nconfig = yaml.safe_load(\"\"\"\nsteps:\n- step: Segmentation\n  settings:\n    model: yolo\n    model_settings:\n      model: Riksarkivet/yolov9-lines-within-regions-1\n- step: TextRecognition\n  settings:\n    model: TrOCR\n    model_settings:\n      model: Riksarkivet/trocr-base-handwritten-hist-swe-2\n\"\"\")\n\n\n# Process with pipeline\npipe = Pipeline.from_config(config)\ncollection = pipe.run(collection)\n\nprint(collection)\n</code></pre> <p>output: <pre><code>collection tree: # Root\nimg_h x img_w node (image0) at (origo) # Image 0 (parent)\n    \u2514\u2500\u2500node0_h x node0_w  (image0_node0) at (px_0, py_0)  # (child)\n        \u251c\u2500\u2500node00_h x node00_w (image0_node0_node0) at (px_00, py_00): text0 # (child's child) \n        \u251c\u2500\u2500node01_h x node01_w (image0_node0_node1) at (px_01, py_01): text1\n        \u251c\u2500\u2500node02_h x node02_w (image0_node0_node2) at (px_02, py_02): text2\n...\nimg_h x img_w node (image1) at (origo) # Image 1\n    \u2514\u2500\u2500...\n</code></pre></p>"},{"location":"getting_started/collection.html#working-with-collection","title":"Working with Collection","text":""},{"location":"getting_started/collection.html#navigation","title":"Navigation","text":"<p>The Collection class uses intuitive indexing for accessing nodes:</p> <pre><code>page = collection[0]               # First page\nregion = collection[0][0]          # First region in first page\nline = collection[0][0][0]         # First line in first region\n</code></pre> <p>or</p> <pre><code>page = collection[0]            \nregion = collection[0,0]       \nline = collection[0,0,0]    \n</code></pre> <p>For instance if we have a populated collection class that looked like this: <pre><code>collection label: Col_output\ncollection tree:\n2413x1511 node (img) at (0, 0)\n    \u2514\u2500\u25002123x1444 node (img_node0) at (54, 218)\n        \u251c\u2500\u2500166x965 node (img_node0_node0) at (358, 224): text0\n        \u251c\u2500\u2500222x1138 node (img_node0_node1) at (331, 450): text1\n        \u251c\u2500\u2500156x1045 node (img_node0_node2) at (437, 702): text2\n        \u251c\u2500\u2500119x1191 node (img_node0_node3) at (238, 888): text3\n</code></pre></p> <p>Running: <pre><code>print(col[0])\nprint(col[0,0])\nprint(col[0,0,0])\n</code></pre></p> <p>outputs: <pre><code>2413x1511 node (img) at (0, 0)\n2123x1444 node (img_node0) at (54, 218)\n166x965 node (img_node0_node0) at (358, 224): text0\n</code></pre></p>"},{"location":"getting_started/collection.html#node-types","title":"Node Types","text":"<p>The tree structure consists of different types of nodes that can be identified using these methods:</p> <ul> <li><code>is_region()</code>: True for nodes containing lines/text blocks</li> <li><code>is_line()</code>: True for nodes containing words/text lines</li> <li><code>is_word()</code>: True for nodes containing single words</li> </ul> <p>Example: <pre><code># Check node types\ncol[0].is_region()       # True - Page is a region\ncol[0,0].is_region()     # True - First child is a region\ncol[0,0,0].is_line()     # True - First grandchild is a line\n\ncol[0].is_line()         # False\ncol[0,0].is_line()       # False\ncol[0,0,0].is_region()   # False\n</code></pre></p>"},{"location":"getting_started/collection.html#traversing-the-tree","title":"Traversing the Tree","text":"<p>You can traverse nodes using filters:</p> <pre><code># Get specific node types\nlines = collection.traverse(filter=lambda node: node.is_line())\nregions = collection.traverse(filter=lambda node: node.is_region())\ntext_nodes = collection.traverse(filter=lambda node: node.contains_text())\n</code></pre>"},{"location":"getting_started/collection.html#saving-and-serialization","title":"Saving and Serialization","text":"<p>The Collection class supports various serialization formats:</p> <pre><code># ALTO XML\ncollection.save(directory=\"output\", serializer=\"alto\")\n\n# Other supported formats:\n# - Page XML\n# - txt\n# - Json\n</code></pre>"},{"location":"getting_started/collection.html#creating-a-collection","title":"Creating a Collection","text":"<pre><code>from htrflow.volume.volume import Collection\n\n# From individual image files\ncollection = Collection(['page1.jpg', 'page2.jpg'])\n\n# From a directory\ncollection = Collection.from_directory('path/to/images')\n\n# From a previously saved collection\ncollection = Collection.from_pickle('saved_collection.pkl')\n</code></pre>"},{"location":"getting_started/collection.html#updating-collection-without-pipeline","title":"Updating Collection (without pipeline)","text":"<p>Collection nodes are updated through model results, with each update potentially modifying the tree structure. Here's a complete example with actual output:</p> <p>Python: </p> 1. Create Collection2. Region Detection3. Line Detection4. Text Recognition <pre><code># 1. Create collection from images\ncollection = Collection([\"img.jpg\"])\n</code></pre> <pre><code>region_results = dummy_segmentation_model(collection.segments()) # or images()\ncollection.update(region_results)\n</code></pre> <pre><code>line_results = dummy_segmentation_model(collection.segments())\ncollection.update(line_results)\n</code></pre> <pre><code>text_results = dummy_text_recognition_model(collection.segments())\ncollection.update(text_results)  \n</code></pre> <p>Output:</p> 1. Create Collection2. Region Detection3. Line Detection4. Text Recognition <pre><code>Initial tree structure:\ncollection label: img\ncollection tree:\n5168x6312 node (img) at (0, 0)\n</code></pre> <pre><code>Tree after region detection:\ncollection label: img\ncollection tree:\n5168x6312 node (img) at (0, 0)\n    \u251c\u2500\u25001293x1579 node (img_node0) at (3020, 831)\n    \u2514\u2500\u25001293x1579 node (img_node1) at (2880, 1376)\n</code></pre> <pre><code>Tree after line detection:\ncollection label: img\ncollection tree:\n5168x6312 node (img) at (0, 0)\n    \u251c\u2500\u25001293x1579 node (img_node0) at (3020, 831)\n    \u2502   \u251c\u2500\u2500167x395 node (img_node0_node0) at (4204, 1957)\n    \u2502   \u2514\u2500\u2500323x378 node (img_node0_node1) at (3020, 1086)\n    \u2514\u2500\u25001293x1579 node (img_node1) at (2880, 1376)\n        \u251c\u2500\u2500323x395 node (img_node1_node0) at (2971, 2208)\n        \u2514\u2500\u2500323x243 node (img_node1_node1) at (4216, 2272)\n</code></pre> <pre><code>Final tree with text:\ncollection label: img.jpg\ncollection tree:\n5168x6312 node (img) at (0, 0)\n    \u251c\u2500\u25001293x1579 node (img_node0) at (3020, 831)\n    \u2502   \u251c\u2500\u2500167x395 node (img_node0_node0) at (4204, 1957): \"Magnam sit est ut dolorem consectetur.\"\n    \u2502   \u2514\u2500\u2500323x378 node (img_node0_node1) at (3020, 1086): \"Dolorem dolore consectetur porro voluptatem eius quaerat dolore.\"\n    \u2514\u2500\u25001293x1579 node (img_node1) at (2880, 1376)\n        \u251c\u2500\u2500323x395 node (img_node1_node0) at (2971, 2208): \"Sit est velit numquam modi adipisci dolorem ut.\"\n        \u2514\u2500\u2500323x243 node (img_node1_node1) at (4216, 2272): \"Quiquia quiquia modi modi consectetur sit numquam.\"\n</code></pre>"},{"location":"getting_started/installation.html","title":"Installation","text":"pip install htrflowInstalled"},{"location":"getting_started/installation.html#with-pip","title":"With pip","text":"<p>Install HTRflow with pip: <pre><code>pip install htrflow\n</code></pre></p> <p>Requirements:</p> <ul> <li>Python &gt;=3.10 and &lt;3.13 (Python 3.10 is required for OpenMMLab)</li> <li>With GPU: CUDA &gt;=11.8 (required due to PyTorch 2.0, can still run on CPU)</li> </ul> <p>Verify the installation of HTRflow with <code>htrflow --help</code>. If the installation was successful, the following message is shown:</p> htrflow --help Usage: htrflow [OPTIONS] COMMAND [ARGS]... CLI inferface for htrflow\u256d- Options ----------------------------------------------------\u256e\u2502 --help          Show this message and exit.                  \u2502\u2570--------------------------------------------------------------\u256f\u256d- Commands ---------------------------------------------------\u256e\u2502 evaluate   Evaluate HTR transcriptions against ground truth  \u2502\u2502 pipeline   Run a HTRflow pipeline                            \u2502\u2570--------------------------------------------------------------\u256f <p>Great! Read Quickstart to learn to use the <code>htrflow pipeline</code> command.</p> <p>Tip</p> <p>To speed up the installation of HTRflow use <code>uv</code>:</p> <pre><code>pip install uv\nuv pip install htrflow\n</code></pre>"},{"location":"getting_started/installation.html#from-source","title":"From source","text":"<p>Requirements:</p> <ul> <li>uv or pip</li> <li>Python 3.10</li> <li>With GPU: CUDA &gt;=11.8 (required due to PyTorch 2.0, can still run on CPU)</li> </ul> <pre><code>uv venv #or uv venv --python 3.10\n</code></pre> <pre><code>source .venv/bin/activate # activate virtual environment\n</code></pre> <p>Clone this repository and run: <pre><code>uv sync\n</code></pre> This will install the HTRflow package in a virtual environment.</p>"},{"location":"getting_started/installation.html#docker","title":"Docker","text":"<p>This guide explains how to run HTRflow using Docker Compose, ensuring a consistent environment and simplifying dependency management. Follow the instructions below to set up and run the application using Docker.</p> <p>HTRflow on Docker hub:</p> <p>Docker hub</p>"},{"location":"getting_started/installation.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker: Install Docker from the official website.</li> <li>Docker Compose: Usually included with Docker installations. Verify by running <code>docker-compose --version</code>.</li> <li>NVIDIA GPU (Optional): If you plan to use GPU acceleration, ensure you have an NVIDIA GPU and the NVIDIA Container Toolkit installed.</li> </ul>"},{"location":"getting_started/installation.html#docker-compose-configuration","title":"Docker compose configuration","text":"<p>The <code>docker-compose.yml</code> file defines the services, configurations, and volume mappings needed to run HTRflow.</p> docker-compose.yml<pre><code>version: \"3.8\"\n\nservices:\n  htrflow:\n    image: docker/htrflow.dockerfile\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n\n    command:\n      [\n        \"/bin/sh\",\n        \"-c\",\n        \"htrflow pipeline pipeline/demo.yaml input --logfile logs/htrflow/htrflow.log\",\n      ]\n\n    volumes:\n      - ./examples/images/pages:/app/input      \n      - ./output-volume:/app/outputs            \n      - ./logs-volume:/app/logs                  \n      - ./examples/pipelines:/app/pipeline       \n      - ./.cache:/app/models                   \n</code></pre>"},{"location":"getting_started/installation.html#volume-mappings","title":"Volume mappings","text":"<pre><code>volumes:\n  - ./examples/images/pages:/app/input       # Input folder\n  - ./output-volume:/app/outputs             # Output folder\n  - ./logs-volume:/app/logs                  # Logs folder\n  - ./examples/pipelines:/app/pipeline       # Pipeline configuration files\n  - ./.cache:/app/models                     # Models cache\n</code></pre> <ul> <li><code>./examples/images/pages:/app/input</code>: Maps your local <code>examples/images/pages</code> directory to <code>/app/input</code> inside the container. This is where HTRflow reads input images.</li> <li><code>./output-volume:/app/outputs</code>: Maps to <code>/app/outputs</code> inside the container for output files.</li> <li><code>./logs-volume:/app/logs</code>: Maps to <code>/app/logs</code> inside the container for application logs.</li> <li><code>./examples/pipelines:/app/pipeline</code>: Provides pipeline configuration files to the container.</li> <li><code>./.cache:/app/models</code>: Shares the models cache to avoid re-downloading.</li> </ul>"},{"location":"getting_started/installation.html#setup-instructions","title":"Setup instructions","text":""},{"location":"getting_started/installation.html#1-create-necessary-directories","title":"1. Create necessary directories","text":"<p>Before running the Docker container, create the directories that will be used as volumes:</p> <pre><code>mkdir -p output-volume logs-volume .cache\n</code></pre> <p>This command creates:</p> <ul> <li><code>output-volume</code>: Stores output files.</li> <li><code>logs-volume</code>: Stores log files.</li> <li><code>.cache</code>: Caches models and data.</li> </ul>"},{"location":"getting_started/installation.html#2-build-and-run-the-docker-container","title":"2. Build and run the Docker container","text":"<p>Use Docker Compose to build the image and start the container:</p> <pre><code>docker-compose up --build\n</code></pre> <ul> <li><code>--build</code>: Forces a rebuild of the Docker image.</li> <li>Docker Compose uses <code>docker-compose.yml</code> to set up the service and volumes.</li> </ul>"},{"location":"getting_started/installation.html#3-stop-the-container","title":"3. Stop the container","text":"<p>To stop the Docker container and remove resources:</p> <pre><code>docker-compose down --rmi all\n</code></pre> <ul> <li><code>--rmi all</code>: Removes all images used by services.</li> </ul>"},{"location":"getting_started/models.html","title":"Models","text":""},{"location":"getting_started/models.html#supported-models","title":"Supported Models","text":"<p>HTRflow natively supports specific models from the following frameworks: Ultralytics, Hugging Face, and OpenMMLab.</p> <p>Tip</p> <p>For a complete list of predefined models compatible with our Pipeline steps, see the Model reference.</p>"},{"location":"getting_started/models.html#riksarkivet-models","title":"Riksarkivet Models","text":"<p>Riksarkivet provides several ready-to-use models available on Hugging Face.</p>"},{"location":"getting_started/models.html#openmmlab-models","title":"OpenMMLab Models","text":"<p>To use OpenMMLab models (e.g SATRN), specific dependencies need to be installed, including <code>torch</code>, <code>mmcv</code>, <code>mmdet</code>, <code>mmengine</code>, <code>mmocr</code>, and <code>yapf</code>. Follow the instructions below to ensure the correct versions are installed.</p> <p>Note</p> <p>OpenMMLab requires a specific PyTorch version. Make sure you have <code>pytorch==2.0.0</code> installed:</p> <pre><code>pip install -U torch==2.0.0\n</code></pre> <p>You can install the OpenMMLab dependencies using either <code>mim</code> or <code>pip</code>.</p> Using mimUsing pip <p>The recommended method, according to OpenMMLab, is to use <code>mim</code>, which is a package and model manager.</p> <p>First, install <code>mim</code>:</p> <pre><code>pip install -U openmim\n</code></pre> <p>Then, use <code>mim</code> to install the required packages:</p> <pre><code>mim install -U mmdet\nmim install -U mmengine\nmim install -U mmocr\nmim install -U mmcv\n</code></pre> <p>Alternatively, you can install the dependencies using <code>pip</code>:</p> <pre><code>pip install -U mmcv==2.0.0\npip install -U mmdet==3.1.0\npip install -U mmengine==0.7.2\npip install -U mmocr==1.0.1\npip install -U yapf==0.40.1\n</code></pre> <p>Here are links to the documentation for each OpenMMLab package used in HTRflow:</p> <ul> <li>mim</li> <li>mmdet</li> <li>mmocr</li> <li>mmengine</li> <li>mmcv</li> </ul>"},{"location":"getting_started/models.html#teklia-models","title":"Teklia Models","text":"<p>To use models from Teklia (currently only PyLaia), specific dependencies need to be installed, including <code>pylaia</code>.  Follow the instructions below to ensure the correct versions are installed.</p> <pre><code>pip install -U pylaia\n</code></pre> <p>Note</p> <p>Pylaia requires a specific PyTorch version. Make sure you have <code>pytorch==1.13.0</code> installed:</p> <pre><code>pip install -U torch==1.13.0\n</code></pre> <p>Note</p> <p>Pylaia requires a specific Python version. Make sure you have <code>python=&lt;3.10</code> </p> <p>Link to the documentation for PyLaia from Teklia:</p> <ul> <li>PyLaia</li> </ul>"},{"location":"getting_started/models.html#custom-models","title":"Custom Models","text":"<p>If your model (or framework) is not supported, you can implement a custom model in HTRflow. Below is a basic example:</p> <pre><code>class Model(BaseModel):\n    def __init__(self, *args, **kwargs):\n        # Initialize your model here\n        pass\n\n    def _predict(self, images, **kwargs) -&gt; list[Result]:\n        # Run inference on `images`\n        # Should return, for example, Result.text_recognition_result() \n        # or Result.segmentation_result()\n</code></pre> <p>See Result reference on different types of return formats from the models. For instance, <code>Result.text_recognition_result()</code> for HTR or <code>Result.segmentation_result()</code> for segmenetation or object detection.</p>"},{"location":"getting_started/models.html#examples-of-custom-implementations","title":"Examples of Custom Implementations","text":"<p>Text Recognition Model:</p> <pre><code>class RecognitionModel(BaseModel):\n    def _predict(self, images: list[np.ndarray]) -&gt; list[Result]:\n        metadata = {\"model\": \"Lorem dummy model\"}\n        n = 2\n        return [\n            Result.text_recognition_result(\n                metadata,\n                texts=[lorem.sentence() for _ in range(n)],\n                scores=[random.random() for _ in range(n)],\n            )\n            for _ in images\n        ]\n</code></pre> <p>Document Classification Model:</p> <pre><code>class ClassificationModel(BaseModel):\n    \"\"\"Model that classifies input images into different types of potato dishes.\"\"\"\n\n    def _predict(self, images: list[np.ndarray]) -&gt; list[Result]:\n        classes = [\"baked potato\", \"french fry\", \"raggmunk\"]\n        return [\n            Result(metadata={\"model\": \"Potato classifier 2000\"}, data=[{\"classification\": random.choice(classes)}])\n            for _ in images\n        ]\n</code></pre>"},{"location":"getting_started/pipeline.html","title":"Pipeline","text":"<p>Welcome to the HTRflow Pipeline documentation. This section introduces the modular pipeline framework designed to process handwritten text images using customizable \"pipeline\" steps. You can define your pipelines using <code>YAML</code> configuration files or <code>Python</code> code.</p>"},{"location":"getting_started/pipeline.html#pipeline-guide","title":"Pipeline Guide","text":"<p>This guide provides an overview of the pipeline architecture, explains some of the available steps, and shows you how to configure and run your pipelines for handwritten text recognition tasks.</p> <p>The following images illustrate how YAML pipeline templates function. The example below serves as a template for common document types such as letters, notes, and individual pages.</p> pipeline.yaml<pre><code>steps:\n- step: Segmentation\n  settings:\n    model: yolo\n    model_settings:\n      model: Riksarkivet/yolov9-lines-within-regions-1\n- step: TextRecognition\n  settings:\n    model: TrOCR\n    model_settings:\n      model: Riksarkivet/trocr-base-handwritten-hist-swe-2\n- step: OrderLines\n</code></pre> <p>Tip</p> <p>See Example Pipeline for inspiration of templates.</p>"},{"location":"getting_started/pipeline.html#understanding-yaml-pipeline-templates","title":"Understanding YAML Pipeline Templates","text":"<p>Each template is designed for specific document types \u2013 the example below shows a template optimized for single-column running text, such as letters, notes, and individual pages.</p> Template chosen based on the material (a letter)"},{"location":"getting_started/pipeline.html#pipeline-steps","title":"Pipeline Steps","text":"<p>Each pipeline consists of sequential steps executed from top to bottom. In this example, we focus on two primary steps:</p> <ol> <li>Segmentation: Identifies and extracts text lines from the image.</li> <li>Text Recognition: Performs Handwritten Text Recognition (HTR) on the segmented lines.</li> </ol> Instance segmentation (polygons) for the HTR step"},{"location":"getting_started/pipeline.html#model-integration","title":"Model Integration","text":"<p>Models specified in the pipeline can be downloaded directly from the Huggingface model hub. For a comprehensive list of supported models, refer to the HTRflow Models Documentation.</p> <p>Note: For English text recognition, you'll need to specify an appropriate model ID, such as the Microsoft TrOCR base handwritten model.</p> Both models IDs are from Riksarkivet"},{"location":"getting_started/pipeline.html#processing-workflow","title":"Processing Workflow","text":"<p>These steps illustrate how an input image is transformed into recognized text through a series of targeted operations. Each step is designed to progressively refine the data, ensuring that the final output is both accurate and well-organized.</p>"},{"location":"getting_started/pipeline.html#text-line-detection","title":"Text Line Detection","text":"<p>The first step is to identify and extract individual text lines from the input image. In this phase, a dedicated model detects the boundaries of each text line, segmenting the image into smaller, manageable parts.</p> Illustrates the text line segmentation process"},{"location":"getting_started/pipeline.html#text-recognition","title":"Text Recognition","text":"<p>Once the text lines have been isolated, they are forwarded to the Handwritten Text Recognition (HTR) component. This stage involves decoding the visual patterns into machine-readable text. The HTR model is specifically optimized for interpreting handwritten text, ensuring that even challenging scripts are transcribed accurately.</p>  This TrOCR is a fine-tuned HTR-model specifacally for historical Swedish text"},{"location":"getting_started/pipeline.html#reading-order-determination","title":"Reading Order Determination","text":"<p>After the text has been recognized, the final step is to arrange the text lines in the correct reading order. This is crucial when dealing with documents that may have complex or non-linear layouts. In this example, a simple top-down ordering transformation is applied, ensuring that the extracted text flows naturally from one line to the next.</p> A simple top-down ordering transformation of the HTR output <p>There are additional steps available, and you can also configure your own custom steps. See Pipeline steps for more details.</p>"},{"location":"getting_started/pipeline.html#pipeline-steps_1","title":"Pipeline steps","text":"<p>HTRflow pipelines operate on <code>Collection</code> instances. A <code>Collection</code> represents one or several input images. Each pipeline step takes a <code>Collection</code> and returns an updated <code>Collection</code>. Here is a toy example of a pipeline step in python:</p> <pre><code>class ExampleStep(PipelineStep):\n    def run(self, collection: Collection) -&gt; Collection:\n        for page in collection:\n            # Do something\n        return collection\n</code></pre> <p>The step is based on <code>PipelineStep</code>, which is the common base class of all pipeline steps. It is defined by overriding the <code>run</code> method.</p> <p>A pipeline step may take arguments at initialization time. Here is an example pipeline step with a <code>value</code> argument: <pre><code>class ExampleStepWithArgument(PipelineStep):\n\n    def __init__(self, value):\n        self.value = value\n\n    def run(self, collection: Collection) -&gt; Collection:\n        for page in collection:\n            if self.value:\n                # Do something\n        return collection\n</code></pre></p> <p>Tip</p> <p>See Reference for a list of all predefined pipeline steps.</p>"},{"location":"getting_started/pipeline.html#defining-a-pipeline","title":"Defining a pipeline","text":"<p>A pipeline is a sequence of pipeline steps. HTRflow offers two ways of defining pipelines: As YAML files or in Python code.</p>"},{"location":"getting_started/pipeline.html#yaml","title":"YAML","text":"<p>YAML pipelines are used with HTRflow's CLI and offers a no-code interface to HTRflow. Here is a one-step YAML pipeline that would run <code>ExampleStep()</code>: pipeline.yaml<pre><code>steps:\n- step: ExampleStep\n</code></pre></p> <p>Arguments can be passed to steps by adding <code>settings</code>. Any key-value pairs under <code>settings</code> are forwarded to the step's constructor. This pipeline would run <code>ExampleStep()</code> followed by <code>ExampleStepWithArgument(value=0.5)</code>: pipeline.yaml<pre><code>steps:\n- step: ExampleStep\n- step: ExampleStepWithArgument\n  settings:\n    value: 0.5\n</code></pre></p> <p>Use the <code>htrflow pipeline</code> command to run a YAML pipeline:</p> <pre><code>htrflow pipeline path/to/pipeline.yaml path/to/inputs\n</code></pre> <p>The <code>path/to/inputs</code> should point to a single image (for example, <code>images/image0.jpg</code>) or a directory of images (for example, <code>images</code>).</p>"},{"location":"getting_started/pipeline.html#python","title":"Python","text":"<p>Pipelines can also be defined directly in python code. The above pipeline is equivalent to this code snippet: <pre><code>from htrflow.pipeline import Pipeline\nfrom htrflow.volume import Collection\n\ncollection = Collection.from_directory(\"path/to/inputs\")\n\npipeline = Pipeline(\n    [\n        ExampleStep(),\n        ExampleStepWithArgument(value=0.5)\n    ]\n)\n\npipeline.run(collection)\n</code></pre></p>"},{"location":"getting_started/pipeline.html#loading-yaml-files-with-python","title":"Loading YAML Files with python","text":"<p>You can easily define and execute your pipelines by loading a YAML configuration file with the HTRflow package. This allows you to quickly modify your pipeline settings without changing your code.</p> <p>Below is an example demonstrating how to load a YAML configuration from a string (or file), create a pipeline, and run it on your input images:</p> <pre><code>from htrflow.pipeline.pipeline import Pipeline\nfrom htrflow.volume.volume import Collection\nfrom htrflow.pipeline.steps import auto_import\nimport yaml\n\n# Define the YAML configuration as a string.\nmy_yaml = \"\"\"\nsteps:\n- step: Segmentation\n  settings:\n    model: yolo\n    model_settings:\n      model: Riksarkivet/yolov9-lines-within-regions-1\n- step: TextRecognition\n  settings:\n    model: TrOCR\n    model_settings:\n      model: Riksarkivet/trocr-base-handwritten-hist-swe-2\n- step: OrderLines\n- step: Export\n  settings:\n    format: txt\n    dest: outputs\n\"\"\"\n\n# Load the YAML configuration.\nconfig = yaml.safe_load(my_yaml)\n\n# Specify your images path or folder.\nimages = ['./my_image.jpg']\n\n# Create a pipeline instance from the loaded configuration.\npipe = Pipeline.from_config(config)\n\ncollection = Collection(images)\n# Run pipeline\ncollection = pipe.run(collection)\nprint(collection)\n</code></pre> <p>In this example:</p> <ul> <li>YAML Configuration: The <code>my_yaml</code> string defines a pipeline with four steps: Segmentation, TextRecognition, OrderLines, and Export.</li> <li>Loading YAML: The configuration is loaded using <code>yaml.safe_load(my_yaml)</code>.</li> <li>Pipeline Creation: A pipeline is created using <code>Pipeline.from_config(config)</code>.</li> <li>Image Importing: <code>Collection</code> can take list of image paths.</li> <li>Pipeline Execution: The pipeline is run on each collection, and the final output is printed.</li> </ul> <p>This method enables you to experiment with different pipeline configurations effortlessly by simply modifying the YAML file.</p>"},{"location":"getting_started/pipeline.html#example-pipelines","title":"Example pipelines","text":""},{"location":"getting_started/pipeline.html#simple-layout","title":"Simple layout","text":"<p>Letters, notes, single pages, and other single-column running text only need one <code>Segmentation</code> step. This pipeline uses a <code>Segmentation</code> step with the <code>Riksarkivet/yolov9-lines-within-regions-1</code> model to find lines to transcribe. Those lines are then transcribed by a <code>TextRecognition</code> step, ordered by <code>OrderLines</code> and finally exported as plain text.</p> pipeline.yaml<pre><code>steps:\n- step: Segmentation\n  settings:\n    model: yolo\n    model_settings:\n      model: Riksarkivet/yolov9-lines-within-regions-1\n- step: TextRecognition\n  settings:\n    model: TrOCR\n    model_settings:\n      model: Riksarkivet/trocr-base-handwritten-hist-swe-2\n- step: OrderLines\n- step: Export\n  settings:\n    format: txt\n    dest: outputs\n</code></pre> <p>Example output from segmentation step:</p> A letter dated 1882. Source."},{"location":"getting_started/pipeline.html#nested-segmentation","title":"Nested segmentation","text":"<p>Segmentation steps can be chained to create a nested segmentation. In this pipeline, the first <code>Segmentation</code> step uses a model that segments the page into regions. Those regions are then segmented into lines by another model in the next <code>Segmentation</code> step. Inference steps such as <code>Segmentation</code> and <code>TextRecognition</code> always works on the leaf nodes of the document tree, so the <code>TextRecognition</code> step will only transcribe the line images, and not the regions. This pipeline uses two <code>Export</code> steps to export the results as both plain text and Alto XML.</p> pipeline.yaml<pre><code>steps:\n- step: Segmentation\n  settings:\n    model: yolo\n    model_settings:\n       model: Riksarkivet/yolov9-regions-1\n- step: Segmentation\n  settings:\n    model: yolo\n    model_settings:\n      model: Riksarkivet/yolov9-lines-within-regions-1\n- step: TextRecognition\n  settings:\n    model: TrOCR\n    model_settings:\n      model: Riksarkivet/trocr-base-handwritten-hist-swe-2\n- step: ReadingOrderMarginalia\n- step: Export\n  settings:\n    format: txt\n    dest: text-outputs\n- step: Export\n  settings:\n    format: alto\n    dest: alto-outputs\n</code></pre> <p>Example output from multiple (nested) segmentation steps:</p> A page from Bergskollegium dated 1698. Source."},{"location":"getting_started/pipeline.html#example-snippets","title":"Example snippets","text":"<p>This section provides some example snippets that can be pasted into an existing pipeline. For more details and pipeline steps, read the Pipeline steps reference.</p>"},{"location":"getting_started/pipeline.html#generation-configuration","title":"Generation configuration","text":"<p>A model's generation settings can be controlled by including <code>generation_settings</code>. Different models support different arguments here. For example, <code>transformers</code>-based models such as <code>TrOCR</code> accept the same arguments as <code>model.generate()</code>. Check the Model reference for details about each model. Here, we set <code>num_beams=4</code> to use beam search instead of greedy decoding.</p> <pre><code>- step: TextRecognition\n  settings:\n    model: TrOCR\n    model_settings:\n       model: Riksarkivet/trocr-base-handwritten-hist-swe-2\n    generation_settings:\n       num_beams: 4\n</code></pre>"},{"location":"getting_started/pipeline.html#multiple-output-formats","title":"Multiple output formats","text":"<p>Chain <code>Export</code> steps to export results to different formats.</p> <pre><code>- step: Export\n  settings:\n    format: txt\n    dest: outputs\n- step: Export\n  settings:\n    format: json\n    dest: outputs\n</code></pre>"},{"location":"getting_started/pipeline.html#export-partial-results","title":"Export partial results","text":"<p>An <code>Export</code> step can be placed within a pipeline to export partial results. Here, an <code>Export</code> step is used to export the raw results before post-processing, and another <code>Export</code> step exports the results after post-processing. <pre><code>- step: Export\n  settings:\n    format: txt\n    dest: raw-outputs\n- step: RemoveLowTextConfidenceLines\n  settings:\n    threshold: 0.95\n- step: Export\n  settings:\n    format: txt\n    dest: cleaned-outputs\n</code></pre></p>"},{"location":"getting_started/pipeline.html#setting-node-labels","title":"Setting node labels","text":"<p>The nodes in the document tree are by default labelled <code>nodeX</code> where <code>X</code> is a serial number unique among the node's siblings. The IDs in Alto XML and Page XML outputs are derived from these node labels. The following snippet sets the labels to <code>regionX_lineY_wordZ</code> instead:</p> <pre><code>labels:\n  level_labels:\n    - region  # first level nodes are labelled \"region\"\n    - line    # second-level nodes are labelled \"line\"\n    - word    # third-level nodes are labelled \"word\" \n  sep: _\n  template: \"{label}{number}\"\n</code></pre>"},{"location":"getting_started/quick_start.html","title":"Quickstart","text":""},{"location":"getting_started/quick_start.html#running-htrflow","title":"Running HTRflow","text":"<p>Once HTRflow is installed, run it with: <pre><code>htrflow pipeline &lt;path/to/pipeline.yaml&gt; &lt;path/to/image&gt;\n</code></pre> The <code>pipeline</code> sub-command tells HTRflow to apply the pipeline defined in <code>pipeline.yaml</code> on <code>image.jpg</code>. To get started, try the example pipeline in the next section.</p>"},{"location":"getting_started/quick_start.html#an-example-pipeline","title":"An example pipeline","text":"<p>Here is an example of an HTRflow pipeline: pipeline.yaml<pre><code>steps:\n- step: Segmentation\n  settings:\n    model: yolo\n    model_settings:\n      model: Riksarkivet/yolov9-lines-within-regions-1\n- step: TextRecognition\n  settings:\n    model: TrOCR\n    model_settings:\n      model: Riksarkivet/trocr-base-handwritten-hist-swe-2\n- step: OrderLines\n- step: Export\n  settings:\n    format: txt\n    dest: outputs\n</code></pre> This pipeline consists of four steps. The <code>Segmentation</code> step segments the image into lines. Those lines are transcribed by the <code>TextRecognition</code> step and then ordered by reading order by <code>OrderLines</code>. The <code>Export</code> step exports the result as a text file to a directory called <code>outputs</code>.</p> <p>This pipeline works well for single-column running text such as letters, notes and single pages. Here are some examples to try:</p> Example 1Example 2Example 3 <p> A moving certificate (sv: \"flyttattest\") from the 1700s. Source.  </p> <p> A moving certificate (sv: \"flyttattest\") from the 1700s. Source.  </p> <p> A letter dated 1882. Source.  </p> <p>To run the demo pipeline on your selected image, paste the pipeline content into an empty text file and save it as <code>pipeline.yaml</code>. Assuming the input image is called <code>image.jpg</code>, run HTRflow with: <pre><code>htrflow pipeline pipeline.yaml image.jpg\n</code></pre> The first run may take some time because the two models need to be downloaded. The outputs are written as text files to a new directory called <code>outputs</code>. Here are the expected results:</p> Example 1Example 2Example 3 image.txt<pre><code>M\u00e4staren med det lofliga hammarsmeds h\u00e4mbetet b\u00f6r\u00ac\nr\u00f6mliga Sven Svensson Hjerpe, som med sin hustru r\u00f6r\u00ac\nb\u00e4ra och dygdesanna Lisa Jansdotter bortflyttrande\ntill Lungsundt Sochn; bekoma h\u00e4rifr\u00e5n f\u00f6ljande\nbewis, at mannen \u00e4r f\u00f6dd 1746, hustrun 1754.\nbegge i sina Christandoms stycken grundrade och i\nsin lofnad f\u00f6relgifwande. warit till hittwarden\nsin 25/4 och wid F\u00f6rh\u00f6ren p\u00e5 beh\u00f6rig tid.\nCarlskoja fyld 21 Sept: 1773. Bengt Forsman\nadj: Past:\nSundberg\n</code></pre> image.txt<pre><code>Beskedeliga mannen J\u00f6ns H\u00e5kansson\nF\u00f6dd 1730, som med dess hustru \u00c5hreborn till\nMargreta Andersdotter f\u00f6dd 1736 flygge w\u00e4l\nLiungsunds f\u00f6rsamling, kunna i \u00e4gge w\u00e4l\nL\u00e4fft i och utan bok, och f\u00f6rst\u00e5 sin Christendom\nF\u00f6rswarligen, hafwa under sitt wistelig af\nh\u00e4r i F\u00f6rsamlingen f\u00f6rdt en Christa g\u00e5ng\nlarbar wandel, Commarerade sista g\u00e5ng\nd. 21. n\u00e4stl. dotren Maria \u00e4r f\u00f6dd. 1760.\nDet. Sigrid 1767. Son Anders 1768. Attr\nPhilipstad d. 25. Martii 1773.\nAnd: Levin\nMalborj\nComminist loci.\n</code></pre> image.txt<pre><code>Monmouth den 29 1882.\n.\nPlatskade Syster emot Sv\u00e5ger\nH\u00e5 godt \u00e4r min \u00f6nskan\nJag f\u00e5r \u00e5terigen g\u00f6ra f\u00f6rs\u00f6ket\natt s\u00e4nda eder bref, jag har\nf\u00f6rut skrifvitt men ej erhallett\nn\u00e5gott w\u00e5r fr\u00e5n eder var. varf\u00f6r\njag tager det f\u00f6r troligt att\nbrefven icke har g\u00e5tt fram.\njag har erinu den stora g\u00e5fvan\natt hafva en god helsa intill\nskrifvande dag, och \u00f6nskligt\nvoro att dessa rader trefar\neder vid samma goda gofva.\noch jag f\u00e5r \u00f6nska eder lycka\np\u00e5 det nya \u00e5ratt samt god\nforts\u00e4ttning p\u00e5 detsamma.\n</code></pre> <p>\ud83c\udf89 You have successfully created your first HTRflow pipeline!</p>"},{"location":"getting_started/quick_start.html#next-steps","title":"Next steps","text":"<ul> <li>Learn more about pipelines</li> <li>Check out all pipeline steps</li> </ul>"},{"location":"getting_started/serialization.html","title":"Serialization","text":""},{"location":"getting_started/serialization.html#support-output-formats","title":"Support output formats..","text":"<p>(WIP)</p>"},{"location":"help/contributing.html","title":"Contributing to HTRflow \ud83d\udee0\ufe0f","text":"<p>Thank you for your interest in contributing to HTRflow! We appreciate contributions in the following areas:</p> <ol> <li>New Features: Enhance the library by adding new functionality. Refer to the section below for guidelines.</li> <li>Documentation: Help us improve our documentation with clear examples demonstrating how to use HTRflow.</li> <li>Bug Reports: Identify and report any issues in the project.</li> <li>Feature Requests: Suggest new features or improvements.</li> </ol>"},{"location":"help/contributing.html#contributing-features","title":"Contributing Features","text":"<p>HTRflow is developed as an open source project, were we value contributions that offer generic solutions to common problems. Before proposing a new feature, please open an issue to discuss your idea with the community. This encourages feedback and support.</p>"},{"location":"help/contributing.html#how-to-contribute","title":"How to Contribute","text":"<p>Develop your feature, fix, or documentation update on your branch.</p>"},{"location":"help/contributing.html#code-quality","title":"Code Quality \ud83c\udfa8","text":"<p>Ensure your code adheres to our quality standards using tools like:</p> <pre><code>ruff\n</code></pre>"},{"location":"help/contributing.html#documentation","title":"Documentation \ud83d\udcdd","text":"<p>Our documentation utilizes docstrings combined with type hinting from mypy. Update or add necessary documentation in the <code>docs/</code> directory and test it locally with:</p> <pre><code>mkdocs serve -v\n</code></pre>"},{"location":"help/contributing.html#tests","title":"Tests \ud83e\uddea","text":"<p>We employ pytest for testing. Ensure you add tests for your changes and run:</p> <pre><code>pytest\n</code></pre>"},{"location":"help/contributing.html#making-a-pull-request","title":"Making a Pull Request","text":"<p>After pushing your changes to GitHub, initiate a pull request from your fork to the main <code>HTRflow</code> repository:</p> <p>Push your branch:</p> <pre><code>git push -u origin &lt;your_branch_name&gt;\n</code></pre> <p>Visit the repository on GitHub and click \"New Pull Request.\" Set the base branch to <code>develop</code> and describe your changes.</p> <p>Ensure all tests pass before requesting a review.</p>"},{"location":"help/faq.html","title":"Frequently Asked Questions","text":""},{"location":"help/faq.html#what-is-htrflow","title":"What is HTRflow?","text":"<ul> <li>HTRflow is an open source tool for HTR and OCR developed by the AI-lab at the Swedish National Archives (Riksarkivet). We use it for batch processing on our own infrastructure for archival data.</li> </ul>"},{"location":"help/faq.html#is-it-free-to-use","title":"Is it free to use?","text":"<ul> <li>Yes. HTRflow is an open-source project and free to use, see license</li> </ul>"},{"location":"help/faq.html#how-to-contact-us","title":"How to contact us?","text":"<ul> <li>You can reach us at this email address: ai@riksarkivet.se</li> </ul>"},{"location":"help/faq.html#does-htrflow-train-models","title":"Does HTRflow train models?","text":"<ul> <li>No! HTRflow is used to faciliate inference for HTR in a simplifed pipeline design and structure your results for seamless serialization of you choice, go to Getting started / Models for more information.</li> </ul>"},{"location":"help/faq.html#how-does-htrflow-compare-to-transkribus-kraken-or-escriptorium","title":"How does HTRflow compare to Transkribus, Kraken or Escriptorium?","text":"<ul> <li>HTRflow distinguishes itself by focusing on abstractions and the use of the pipeline pattern, see Getting started / Pipeline for more information.</li> </ul>"},{"location":"integrations/index.html","title":"WIP","text":""},{"location":"integrations/haas.html","title":"Haas","text":"Image caption"},{"location":"integrations/search.html","title":"Enabling Full Text Search in The National Archives","text":"<p>This project focuses on integrating advanced search functionality within The National Archives' online interface to maximize the potential of transcribed texts derived from Htrflow. Currently, users can only access archival documents by searching metadata. If a search term isn't in the metadata, no results are returned, even if the term appears in the document.</p> <p>By utilizing Htrflow to transcribe a large volume of digitized archival documents, The National Archives aims to enable direct searching within document texts via the online interface. This enhancement will significantly improve accessibility and usability of archival information for researchers and the general public.</p> <p>The project involves indexing Alto XML files generated by Htrflow into Solr, enabling queries via a REST API to locate words within archival documents. This indexed data will power the new document search feature within the online interface, facilitating more comprehensive access to archival content</p> <p></p>"},{"location":"integrations/spaces.html","title":"Spaces","text":""},{"location":"integrations/spaces.html#fasttrack-stepwsie","title":"Fasttrack &amp; Stepwsie","text":"<p>htrflow_app</p> <p>htrflow_app is designed to provide users with a step-by-step visualization of the HTR-process, and offer non-expert users an inside look into the workings of an AI-transcription pipeline. At the moment htrflow_app is mainly a demo-application. It\u2019s not intended for production, but instead to showcase the immense possibilities that HTR-technology is opening up for cultural heritage institutions around the world.</p> <p>All code is open-source, all our models are on Hugging Face and are free to use, and all data will be made available for download and use on Hugging Face as well.</p> <p>Note</p> <p>Note</p> <p>The backend (src) for the app will be rewritten and packaged to be more optimized under the project name htrflow.</p> <p> </p>"},{"location":"notebooks/attention-based-word-segmentation.html","title":"Attention-based word segmentation with TrOCR","text":"In\u00a0[\u00a0]: Copied! <pre>from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"Riksarkivet/trocr-base-handwritten-swe\").to(\"cuda\")\nprocessor = TrOCRProcessor.from_pretrained(\"Riksarkivet/trocr-base-handwritten-swe\")\n</pre> from transformers import TrOCRProcessor, VisionEncoderDecoderModel   model = VisionEncoderDecoderModel.from_pretrained(\"Riksarkivet/trocr-base-handwritten-swe\").to(\"cuda\") processor = TrOCRProcessor.from_pretrained(\"Riksarkivet/trocr-base-handwritten-swe\") <p>Load an input image.</p> In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\n\nfrom htrflow.utils.imgproc import read\n\n\nplt.rcParams[\"figure.figsize\"] = [15, 10]\n\n\nimg = read(\"examples/images/lines/A0068699_00021_region0_line2.jpg\")\nplt.imshow(img)\n</pre> import matplotlib.pyplot as plt  from htrflow.utils.imgproc import read   plt.rcParams[\"figure.figsize\"] = [15, 10]   img = read(\"examples/images/lines/A0068699_00021_region0_line2.jpg\") plt.imshow(img) Out[4]: <pre>&lt;matplotlib.image.AxesImage at 0x704411f0e2f0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>inputs = processor([img], return_tensors=\"pt\").pixel_values.to(\"cuda\")\noutput = model.generate(inputs, return_dict_in_generate=True, output_attentions=True, num_beams=1)\nprint(processor.decode(output.sequences[0], skip_special_tokens=True))\n</pre> inputs = processor([img], return_tensors=\"pt\").pixel_values.to(\"cuda\") output = model.generate(inputs, return_dict_in_generate=True, output_attentions=True, num_beams=1) print(processor.decode(output.sequences[0], skip_special_tokens=True)) In\u00a0[6]: Copied! <pre>token_ids = output.sequences[0]\ntokens = processor.batch_decode(token_ids)\nprint(tokens)\n</pre> token_ids = output.sequences[0] tokens = processor.batch_decode(token_ids) print(tokens) <pre>['&lt;s&gt;', '&lt;s&gt;', '&lt;s&gt;', 'h', '\u00f6', 'f', 'ding', 'en', ' e', 'ller', ' General', ' Aud', 'ite', 'ns', '&lt;/s&gt;']\n</pre> <p>The output <code>output.cross_attentions</code> is a list of attention at each step of the decoding process. We use the last one, <code>output.cross_attentions[-1]</code>, which includes information about the entire output sequence. It has shape $(12, 1, 16, n, 577)$, where 12 is the number of layers in the model, 1 is the number of beams (which we excplicitly set to 1), 16 is the number of attention heads in each layer, $n$ is the number of output tokens, and 577 is the length of the input sequence ($24 \\cdot 24$ image patches $+1$ extra token).</p> <p>These are aggregated to the shape $(n, 577)$, an attention map mapping each of the $n$ input tokens to each patch of the input image. We take the mean over all attention heads and sum over all layers. There are lots of ways to do this, this seems to work fine for word segmentation.</p> In\u00a0[7]: Copied! <pre>import torch\n\n\n# Attention in the final decoding step, with shape\n# (n layers, n beams (=1), n attention heads, output sequence length, input sequence length)\n# With the default settings of TrOCR, this should be (12, 1, 16, x, 577)\ncross_attention = torch.stack(output.cross_attentions[-1])\nprint(cross_attention.shape)\n\n# Take the mean attention over each attention head and sum over all layers.\n# There are other ways to do this! This seems to work fine for word segmentation.\nagg_attention = cross_attention.mean(axis=2).sum(axis=0).squeeze()\nprint(agg_attention.shape)\n</pre> import torch   # Attention in the final decoding step, with shape # (n layers, n beams (=1), n attention heads, output sequence length, input sequence length) # With the default settings of TrOCR, this should be (12, 1, 16, x, 577) cross_attention = torch.stack(output.cross_attentions[-1]) print(cross_attention.shape)  # Take the mean attention over each attention head and sum over all layers. # There are other ways to do this! This seems to work fine for word segmentation. agg_attention = cross_attention.mean(axis=2).sum(axis=0).squeeze() print(agg_attention.shape) <pre>torch.Size([12, 1, 16, 14, 577])\ntorch.Size([14, 577])\n</pre> <p>Plot the resulting atttentions.</p> In\u00a0[8]: Copied! <pre>def patches2image(patches, size=24):\n    \"\"\"Convert a 1d sequence of patches to a 2d image\"\"\"\n    return torch.reshape(patches, (size, size))\n\n\ndef plot_attentions(token_attentions):\n    for i, (token, attentions) in enumerate(token_attentions):\n        plt.subplot(4, 6, i + 1)\n        plt.imshow(patches2image(attentions).cpu())\n        plt.title(repr(token))\n\n    plt.tight_layout()\n\n\nplt.rcParams[\"figure.figsize\"] = [20, 10]\n\nplt.matshow(agg_attention.cpu()[:, :])\nagg2 = agg_attention - agg_attention.mean(axis=0)\nagg2[agg2 &lt; 0] = 0\nagg2 = agg2[:, 1:]\ntoken_attentions = list(zip(tokens[1:], agg2))\ninputs.shape\n</pre> def patches2image(patches, size=24):     \"\"\"Convert a 1d sequence of patches to a 2d image\"\"\"     return torch.reshape(patches, (size, size))   def plot_attentions(token_attentions):     for i, (token, attentions) in enumerate(token_attentions):         plt.subplot(4, 6, i + 1)         plt.imshow(patches2image(attentions).cpu())         plt.title(repr(token))      plt.tight_layout()   plt.rcParams[\"figure.figsize\"] = [20, 10]  plt.matshow(agg_attention.cpu()[:, :]) agg2 = agg_attention - agg_attention.mean(axis=0) agg2[agg2 &lt; 0] = 0 agg2 = agg2[:, 1:] token_attentions = list(zip(tokens[1:], agg2)) inputs.shape Out[8]: <pre>torch.Size([1, 3, 384, 384])</pre> In\u00a0[9]: Copied! <pre>plot_attentions(token_attentions)\n\n# Plot original reshaped image\nimport cv2\n\n\nim2 = cv2.resize(img.copy(), (384, 384), interpolation=cv2.INTER_LINEAR)\nplt.subplot(4, 6, len(tokens))\nplt.imshow(im2)\n</pre> plot_attentions(token_attentions)  # Plot original reshaped image import cv2   im2 = cv2.resize(img.copy(), (384, 384), interpolation=cv2.INTER_LINEAR) plt.subplot(4, 6, len(tokens)) plt.imshow(im2) Out[9]: <pre>&lt;matplotlib.image.AxesImage at 0x7044081312d0&gt;</pre> In\u00a0[17]: Copied! <pre>current_word = []\nword_attentions = []\nplt.rcParams[\"figure.figsize\"] = [20, 10]\n\nfor i, ta in enumerate(token_attentions):\n    if (ta[0].startswith(\" \") and len(ta[0]) &gt; 1) or i + 1 == len(token_attentions):\n        word = \"\".join(token for token, _ in current_word)\n        # Compute the word's combined attention heatmap\n        # Weigh each contribution by the number of characters in the token\n        attention = sum(attn * len(token.strip()) for token, attn in current_word if token not in (\"&lt;s&gt;\", \"&lt;/s&gt;\"))\n        attention[attention &lt; 0] = 0\n        word_attentions.append((word.strip(), attention))\n        current_word = []\n\n    current_word.append(ta)\n\nplot_attentions(word_attentions)\n\nnorms = []\nplt.rcParams[\"figure.figsize\"] = [5, 3]\n\nfor i, (_, attention) in enumerate(word_attentions):\n    attention = patches2image(attention)\n    cols = (attention * attention).sum(axis=0)  # attention.sum(axis=0)\n    norm = cols / cols.max()\n    norms.append(norm)\n    plt.subplot(4, 6, i + 1)\n    plt.plot(20 * (1 - norms[-1].cpu()), color=\"yellow\")\n</pre> current_word = [] word_attentions = [] plt.rcParams[\"figure.figsize\"] = [20, 10]  for i, ta in enumerate(token_attentions):     if (ta[0].startswith(\" \") and len(ta[0]) &gt; 1) or i + 1 == len(token_attentions):         word = \"\".join(token for token, _ in current_word)         # Compute the word's combined attention heatmap         # Weigh each contribution by the number of characters in the token         attention = sum(attn * len(token.strip()) for token, attn in current_word if token not in (\"\", \"\"))         attention[attention &lt; 0] = 0         word_attentions.append((word.strip(), attention))         current_word = []      current_word.append(ta)  plot_attentions(word_attentions)  norms = [] plt.rcParams[\"figure.figsize\"] = [5, 3]  for i, (_, attention) in enumerate(word_attentions):     attention = patches2image(attention)     cols = (attention * attention).sum(axis=0)  # attention.sum(axis=0)     norm = cols / cols.max()     norms.append(norm)     plt.subplot(4, 6, i + 1)     plt.plot(20 * (1 - norms[-1].cpu()), color=\"yellow\") In\u00a0[18]: Copied! <pre>norms = []\nplt.rcParams[\"figure.figsize\"] = [10, 6]\nfor _, attention in word_attentions:\n    attention = patches2image(attention)\n    cols = (1 * attention).sum(axis=0)\n    norm = cols / cols.max()\n    norms.append(norm)\n    plt.plot(norms[-1].cpu(), \"-o\")\n</pre> norms = [] plt.rcParams[\"figure.figsize\"] = [10, 6] for _, attention in word_attentions:     attention = patches2image(attention)     cols = (1 * attention).sum(axis=0)     norm = cols / cols.max()     norms.append(norm)     plt.plot(norms[-1].cpu(), \"-o\") <p>From this, we estimate the spaces between the words by taking the intersection of each pair of consecutive words.</p> In\u00a0[19]: Copied! <pre>spaces = []\nfor w1, w2 in zip(norms, norms[1:]):\n    m1 = w1.argmax()\n    m2 = w2.argmax()\n    intersection_idx = torch.argwhere(w1[m1 : m2 + 1] &lt;= w2[m1 : m2 + 1])[0][0]\n    spaces.append(int(intersection_idx + m1))\n</pre> spaces = [] for w1, w2 in zip(norms, norms[1:]):     m1 = w1.argmax()     m2 = w2.argmax()     intersection_idx = torch.argwhere(w1[m1 : m2 + 1] &lt;= w2[m1 : m2 + 1])[0][0]     spaces.append(int(intersection_idx + m1)) <p>Draw the estimated spaces on the original image.</p> In\u00a0[20]: Copied! <pre>plt.rcParams[\"figure.figsize\"] = [20, 10]\nh, w = img.shape[:2]\nfor x in spaces:\n    x = int(x / 24 * w)\n    plt.plot([x, x], [0, h], color=\"blue\", linewidth=3)\nplt.imshow(img)\n</pre> plt.rcParams[\"figure.figsize\"] = [20, 10] h, w = img.shape[:2] for x in spaces:     x = int(x / 24 * w)     plt.plot([x, x], [0, h], color=\"blue\", linewidth=3) plt.imshow(img) Out[20]: <pre>&lt;matplotlib.image.AxesImage at 0x7043f85411e0&gt;</pre>"},{"location":"notebooks/attention-based-word-segmentation.html#attention-based-word-segmentation-with-trocr","title":"Attention-based word segmentation with TrOCR\u00b6","text":"<p>This notebook shows a way to estimate the word boundaries from the model's encoder-decoder attention.</p>"},{"location":"notebooks/attention-based-word-segmentation.html#load-the-model-and-input-image","title":"Load the model and input image\u00b6","text":"<p>We use the Swedish TrOCR model from Riksarkivet.</p>"},{"location":"notebooks/attention-based-word-segmentation.html#run-the-model","title":"Run the model\u00b6","text":"<p>Note the arguments passed to <code>model.generate</code>. Setting <code>return_dict_in_generate</code> and <code>output_attentions</code> to <code>True</code> makes the model return the attention matrices. It is also important to set <code>num_beams</code> to 1, otherwise the output attention matrices may not be aligned with the tokens. (Not sure why, but it could be related to this: https://discuss.huggingface.co/t/t5-why-do-we-have-more-tokens-expressed-via-cross-attentions-than-the-decoded-sequence/31893)</p>"},{"location":"notebooks/attention-based-word-segmentation.html#get-the-attention-map-for-each-token","title":"Get the attention map for each token\u00b6","text":"<p>These are the output tokens:</p>"},{"location":"notebooks/attention-based-word-segmentation.html#combine-token-attentions-to-word-attentions","title":"Combine token attentions to word attentions\u00b6","text":""},{"location":"notebooks/attention-based-word-segmentation.html#estimate-word-boundaries-from-the-attention-maps","title":"Estimate word boundaries from the attention maps\u00b6","text":"<p>We sum each attention map along each column to simplify the attention maps down to one dimension. The intuition here is that each (normalized) column sum indicates how much the pixels along the column matter for the each output word. As we see, the first word (in blue) peaks around column 2-3, which indicates that the model used the pixels in column 2-3 to decode the tokens of the first word.</p>"},{"location":"reference/export-formats.html","title":"Export formats","text":""},{"location":"reference/export-formats.html#serialization.serialization.AltoXML","title":"<code>AltoXML</code>","text":"<p>               Bases: <code>Serializer</code></p> <p>Alto XML serializer.</p> <p>This serializer uses a jinja template to produce Alto XML files according to version 4.4 of the Alto schema.</p>"},{"location":"reference/export-formats.html#serialization.serialization.AltoXML--features","title":"Features","text":"<ul> <li>Uses Alto version 4.4.</li> <li>Includes detailed processing metadata in the <code>&lt;Description&gt;</code> block.</li> <li>Supports rendering of region locations (printspace and margins). To enable this, first make sure that the regions are tagged by calling <code>layout.label_regions(...)</code> before serialization.</li> <li>Will always produce a file, but the file may be empty.</li> </ul>"},{"location":"reference/export-formats.html#serialization.serialization.AltoXML--limitations","title":"Limitations","text":"<ul> <li>Two-level segmentation: The Alto schema only supports two-level segmentation, i.e. pages with regions and lines. Pages with deeper segmentation will be flattened so that only the innermost regions are rendered.</li> <li>Only includes text confidence at the page level.</li> </ul>"},{"location":"reference/export-formats.html#serialization.serialization.AltoXML--examples","title":"Examples","text":"<p>Example usage with the <code>Export</code> pipeline step: <pre><code>- step: Export\n  settings:\n    dest: alto-ouptut\n    format: alto\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>template_dir</code> <p>Name of template directory.</p> <code>_TEMPLATES_DIR</code> <code>template_name</code> <p>Name of template file in <code>template_dir</code>.</p> <code>'alto'</code> Source code in <code>src/htrflow/serialization/serialization.py</code> <pre><code>def __init__(self, template_dir=_TEMPLATES_DIR, template_name=\"alto\"):\n    \"\"\"\n    Arguments:\n        template_dir: Name of template directory.\n        template_name: Name of template file in `template_dir`.\n    \"\"\"\n    env = Environment(loader=FileSystemLoader([template_dir, \".\"]))\n    self.template = env.get_template(template_name)\n    self.schema = os.path.join(_SCHEMA_DIR, \"alto-4-4.xsd\")\n</code></pre>"},{"location":"reference/export-formats.html#serialization.serialization.AltoXML.validate","title":"<code>validate</code>","text":"<p>Validate <code>doc</code> against the current schema</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>str</code> <p>Input document</p> required Source code in <code>src/htrflow/serialization/serialization.py</code> <pre><code>def validate(self, doc: str) -&gt; None:\n    \"\"\"Validate `doc` against the current schema\n\n    Arguments:\n        doc: Input document\n\n    Raises:\n        xmlschema.XMLSchemaValidationError if the document violates\n        the current schema.\n    \"\"\"\n    xmlschema.validate(doc, self.schema)\n</code></pre>"},{"location":"reference/export-formats.html#serialization.serialization.PageXML","title":"<code>PageXML</code>","text":"<p>               Bases: <code>Serializer</code></p> <p>Page XML serializer</p> <p>This serializer uses a jinja template to produce Page XML files according to the 2019-07-15 version of the schema.</p>"},{"location":"reference/export-formats.html#serialization.serialization.PageXML--features","title":"Features","text":"<ul> <li>Includes line confidence scores.</li> <li>Supports nested segmentation.</li> </ul>"},{"location":"reference/export-formats.html#serialization.serialization.PageXML--limitations","title":"Limitations","text":"<ul> <li>Will not create an output file if the page is not serializable, for example if it does not contain any regions. (This behaviour differs from the Alto serializer, which instead would produce an empty file.)</li> </ul>"},{"location":"reference/export-formats.html#serialization.serialization.PageXML--examples","title":"Examples","text":"<p>Example usage with the <code>Export</code> pipeline step: <pre><code>- step: Export\n  settings:\n    dest: page-ouptut\n    format: page\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>template_dir</code> <p>Name of template directory.</p> <code>_TEMPLATES_DIR</code> <code>template_name</code> <p>Name of template file in <code>template_dir</code>.</p> <code>'page'</code> Source code in <code>src/htrflow/serialization/serialization.py</code> <pre><code>def __init__(self, template_dir=_TEMPLATES_DIR, template_name=\"page\"):\n    \"\"\"\n    Arguments:\n        template_dir: Name of template directory.\n        template_name: Name of template file in `template_dir`.\n    \"\"\"\n    env = Environment(loader=FileSystemLoader([template_dir, \".\"]))\n    self.template = env.get_template(template_name)\n    self.schema = os.path.join(_SCHEMA_DIR, \"pagecontent.xsd\")\n</code></pre>"},{"location":"reference/export-formats.html#serialization.serialization.PageXML.validate","title":"<code>validate</code>","text":"<p>Validate <code>doc</code> against the current schema</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>str</code> <p>Input document</p> required Source code in <code>src/htrflow/serialization/serialization.py</code> <pre><code>def validate(self, doc: str) -&gt; None:\n    \"\"\"Validate `doc` against the current schema\n\n    Arguments:\n        doc: Input document\n\n    Raises:\n        xmlschema.XMLSchemaValidationError if the document violates\n        the current schema.\n    \"\"\"\n    xmlschema.validate(doc, self.schema)\n</code></pre>"},{"location":"reference/export-formats.html#serialization.serialization.Json","title":"<code>Json</code>","text":"<p>               Bases: <code>Serializer</code></p> <p>JSON serializer</p> <p>This serializer extracts all content from the collection and saves it as json. The resulting json file(s) include properties that are not supported by Alto or Page XML, such as region confidence scores.</p>"},{"location":"reference/export-formats.html#serialization.serialization.Json--examples","title":"Examples","text":"<p>Example usage with the <code>Export</code> pipeline step: <pre><code>- step: Export\n  settings:\n    dest: json-ouptut\n    format: json\n    one_file: False\n    indent: 2\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>one_file</code> <p>Export all pages of the collection to the same file. Defaults to False.</p> <code>False</code> <code>indent</code> <p>The indentation level of the output json file(s).</p> <code>4</code> Source code in <code>src/htrflow/serialization/serialization.py</code> <pre><code>def __init__(self, one_file=False, indent=4):\n    \"\"\"\n    Arguments:\n        one_file: Export all pages of the collection to the same file.\n            Defaults to False.\n        indent: The indentation level of the output json file(s).\n    \"\"\"\n    self.one_file = one_file\n    self.indent = indent\n</code></pre>"},{"location":"reference/export-formats.html#serialization.serialization.PlainText","title":"<code>PlainText</code>","text":"<p>               Bases: <code>Serializer</code></p> <p>Plain text serializer</p> <p>This serializer extracts all text content from the collection and saves it as plain text. All other data (metadata, coordinates, geometries, confidence scores, and so on) is discarded.</p>"},{"location":"reference/export-formats.html#serialization.serialization.PlainText--examples","title":"Examples","text":"<p>Example usage with the <code>Export</code> pipeline step: <pre><code>- step: Export\n  settings:\n    dest: text-ouptut\n    format: txt\n</code></pre></p>"},{"location":"reference/models.html","title":"Models","text":""},{"location":"reference/models.html#base-model","title":"Base model","text":""},{"location":"reference/models.html#models.base_model.BaseModel","title":"<code>BaseModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Model base class</p> <p>This is the abstract base class of HTRflow models. It handles batching of inputs, some shared initialization arguments and generic logging.</p> <p>Concrete model implementations bases this class and defines their prediction method in <code>_predict()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str | None</code> <p>Model device as a string, recognizable by torch. Defaults to <code>None</code>, which sets the device to <code>cuda</code> or <code>cpu</code> depending on availability.</p> <code>None</code> <code>allow_tf32</code> <code>bool</code> <p>Allow running matrix multiplications with TensorFloat-32. This speeds up inference at the expense of inference quality. On Ampere and newer CUDA devices, enabling TF32 can improve performance for matrix multiplications and convolutions. Read more here: https://huggingface.co/docs/diffusers/optimization/fp16#tensorfloat-32</p> <code>True</code> <code>allow_cudnn_benchmark</code> <code>bool</code> <p>When True, enables cuDNN benchmarking to select the fastest convolution algorithms for fixed input sizes, potentially increasing performance. Note that this may introduce nondeterminism. Defaults to False. Read more here: https://huggingface.co/docs/transformers/en/perf_train_gpu_one#tf32</p> <code>False</code> Source code in <code>src/htrflow/models/base_model.py</code> <pre><code>def __init__(self, device: str | None = None, allow_tf32: bool = True, allow_cudnn_benchmark: bool = False):\n    \"\"\"\n    Arguments:\n        device: Model device as a string, recognizable by torch. Defaults\n            to `None`, which sets the device to `cuda` or `cpu` depending\n            on availability.\n        allow_tf32: Allow running matrix multiplications with TensorFloat-32.\n            This speeds up inference at the expense of inference quality.\n            On Ampere and newer CUDA devices, enabling TF32 can improve\n            performance for matrix multiplications and convolutions.\n            Read more here:\n            https://huggingface.co/docs/diffusers/optimization/fp16#tensorfloat-32\n        allow_cudnn_benchmark: When True, enables cuDNN benchmarking to\n            select the fastest convolution algorithms for fixed input sizes,\n            potentially increasing performance. Note that this may introduce\n            nondeterminism. Defaults to False.\n            Read more here:\n            https://huggingface.co/docs/transformers/en/perf_train_gpu_one#tf32\n    \"\"\"\n    self.metadata = {\"model_class\": self.__class__.__name__}\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    self.device = torch.device(device)\n\n    if torch.cuda.is_available():\n        # Allow matrix multiplication with TensorFloat-32\n        torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n        torch.backends.cudnn.allow_tf32 = allow_tf32\n        torch.backends.cudnn.benchmark = allow_cudnn_benchmark\n</code></pre>"},{"location":"reference/models.html#models.base_model.BaseModel._predict","title":"<code>_predict</code>  <code>abstractmethod</code>","text":"<p>Model specific prediction method</p> Source code in <code>src/htrflow/models/base_model.py</code> <pre><code>@abstractmethod\ndef _predict(self, images: list[NumpyImage], **kwargs) -&gt; list[Result]:\n    \"\"\"Model specific prediction method\"\"\"\n</code></pre>"},{"location":"reference/models.html#models.base_model.BaseModel.predict","title":"<code>predict</code>","text":"<p>Perform inference on images</p> <p>Takes an arbitrary number of inputs and runs batched inference. The inputs can be streamed from an iterator and don't need to be simultaneously read into memory. Prints a progress bar using <code>tqdm</code>. This is a template method which uses the model-specific <code>_predict(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Collection[NumpyImage]</code> <p>Input images</p> required <code>batch_size</code> <code>int</code> <p>Inference batch size, defaults to 1</p> <code>1</code> <code>image_scaling_factor</code> <code>float</code> <p>If &lt; 1, all input images will be down- scaled by this factor, which can be useful for speeding up inference on higher resolution images. All geometric data in the result (e.g., bounding boxes) are reported with respect to the original resolution.</p> <code>1.0</code> <code>tqdm_kwargs</code> <code>dict[str, Any] | None</code> <p>Optional keyword arguments to control the progress bar.</p> <code>None</code> <code>**kwargs</code> <p>Optional keyword arguments that are forwarded to the model specific prediction method <code>_predict(...)</code>.</p> <code>{}</code> Source code in <code>src/htrflow/models/base_model.py</code> <pre><code>def predict(\n    self,\n    images: Collection[NumpyImage],\n    batch_size: int = 1,\n    image_scaling_factor: float = 1.0,\n    tqdm_kwargs: dict[str, Any] | None = None,\n    **kwargs,\n) -&gt; list[Result]:\n    \"\"\"Perform inference on images\n\n    Takes an arbitrary number of inputs and runs batched inference.\n    The inputs can be streamed from an iterator and don't need to\n    be simultaneously read into memory. Prints a progress bar using\n    `tqdm`. This is a template method which uses the model-specific\n    `_predict(...)`.\n\n    Arguments:\n        images: Input images\n        batch_size: Inference batch size, defaults to 1\n        image_scaling_factor: If &lt; 1, all input images will be down-\n            scaled by this factor, which can be useful for speeding\n            up inference on higher resolution images. All geometric\n            data in the result (e.g., bounding boxes) are reported\n            with respect to the original resolution.\n        tqdm_kwargs: Optional keyword arguments to control the\n            progress bar.\n        **kwargs: Optional keyword arguments that are forwarded to\n            the model specific prediction method `_predict(...)`.\n    \"\"\"\n\n    batch_size = max(batch_size, 1)  # make sure batch size is at least 1\n    image_scaling_factor = max(10e-10, min(image_scaling_factor, 1))  # clip scaling factor to (0, 1]\n\n    n_batches = (len(images) + batch_size - 1) // batch_size\n    model_name = self.__class__.__name__\n    logger.info(\n        \"Model '%s' on device '%s' received %d images in batches of %d images per batch (%d batches)\",\n        model_name,\n        self.device,\n        len(images),\n        batch_size,\n        n_batches,\n    )\n\n    results = []\n    batches = _batch(images, batch_size)\n    desc = f\"{model_name}: Running inference (batch size {batch_size})\"\n    for i, batch in enumerate(tqdm(batches, desc, n_batches, **(tqdm_kwargs or {}))):\n        msg = \"%s: Running inference on %d images (batch %d of %d)\"\n        logger.info(msg, model_name, len(batch), i + 1, n_batches)\n        scaled_batch = [rescale_linear(image, image_scaling_factor) for image in batch]\n        batch_results = self._predict(scaled_batch, **kwargs)\n        for result in batch_results:\n            result.rescale(1 / image_scaling_factor)\n            results.append(result)\n    return results\n</code></pre>"},{"location":"reference/models.html#text-recognition-models","title":"Text recognition models","text":""},{"location":"reference/models.html#models.huggingface.trocr.TrOCR","title":"<code>TrOCR</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>HTRflow adapter of the tranformer-based OCR model TrOCR.</p> <p>Uses huggingface's implementation of TrOCR. For further information, see https://huggingface.co/docs/transformers/model_doc/trocr.</p> <p>Example usage with the <code>TextRecognition</code> step: <pre><code>- step: TextRecognition\n  settings:\n    model: TrOCR\n    model_settings:\n      model: Riksarkivet/trocr-base-handwritten-hist-swe-2\n      device: cpu\n      model_kwargs:\n        revision: 6ecbb5d643430385e1557001ae78682936f8747f\n    generation_settings:\n      batch_size: 8\n      num_beams: 1\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Path or name of pretrained VisisonEncoderDeocderModel.</p> required <code>processor</code> <code>str | None</code> <p>Optional path or name of pretrained TrOCRProcessor. If not given, the model path or name is used.</p> <code>None</code> <code>model_kwargs</code> <code>dict[str, Any] | None</code> <p>Model initialization kwargs which are forwarded to VisionEncoderDecoderModel.from_pretrained.</p> <code>None</code> <code>processor_kwargs</code> <code>dict[str, Any] | None</code> <p>Processor initialization kwargs which are forwarded to TrOCRProcessor.from_pretrained.</p> <code>None</code> <code>kwargs</code> <p>Additional kwargs which are forwarded to BaseModel's init.</p> <code>{}</code> Source code in <code>src/htrflow/models/huggingface/trocr.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    processor: str | None = None,\n    model_kwargs: dict[str, Any] | None = None,\n    processor_kwargs: dict[str, Any] | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Arguments:\n        model: Path or name of pretrained VisisonEncoderDeocderModel.\n        processor: Optional path or name of pretrained TrOCRProcessor.\n            If not given, the model path or name is used.\n        model_kwargs: Model initialization kwargs which are forwarded to\n            VisionEncoderDecoderModel.from_pretrained.\n        processor_kwargs: Processor initialization kwargs which are\n            forwarded to TrOCRProcessor.from_pretrained.\n        kwargs: Additional kwargs which are forwarded to BaseModel's\n            __init__.\n    \"\"\"\n    super().__init__(**kwargs)\n\n    # Initialize model\n    model_kwargs = HF_CONFIG | (model_kwargs or {})\n    self.model = VisionEncoderDecoderModel.from_pretrained(model, **model_kwargs)\n    self.model.to(self.device)\n    logger.info(\"Initialized TrOCR model from %s on device %s.\", model, self.model.device)\n\n    # Initialize processor\n    processor = processor or model\n    processor_kwargs = HF_CONFIG | (processor_kwargs or {})\n    self.processor = TrOCRProcessor.from_pretrained(processor, **processor_kwargs)\n    logger.info(\"Initialized TrOCR processor from %s.\", processor)\n\n    self.metadata.update(\n        {\n            \"model\": model,\n            \"model_version\": get_model_info(model),\n            \"processor\": processor,\n            \"processor_version\": get_model_info(processor),\n        }\n    )\n</code></pre>"},{"location":"reference/models.html#models.huggingface.trocr.TrOCR._predict","title":"<code>_predict</code>","text":"<p>TrOCR-specific prediction method.</p> <p>This method is used by <code>predict()</code> and should typically not be called directly. However, <code>predict()</code> forwards additional kwargs to this method.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[ndarray]</code> <p>Input images.</p> required <code>**generation_kwargs</code> <p>Optional keyword arguments that are forwarded to the model's .generate() method.</p> <code>{}</code> Source code in <code>src/htrflow/models/huggingface/trocr.py</code> <pre><code>def _predict(self, images: list[np.ndarray], **generation_kwargs) -&gt; list[Result]:\n    \"\"\"TrOCR-specific prediction method.\n\n    This method is used by `predict()` and should typically not be\n    called directly. However, `predict()` forwards additional kwargs\n    to this method.\n\n    Arguments:\n        images: Input images.\n        **generation_kwargs: Optional keyword arguments that are\n            forwarded to the model's .generate() method.\n    \"\"\"\n\n    # Prepare generation keyword arguments: Generally, all kwargs are\n    # forwarded to the model's .generate method, but some need to be\n    # explicitly set (and possibly overridden) to ensure that we get the\n    # output format we want.\n    generation_kwargs[\"num_return_sequences\"] = generation_kwargs.get(\"num_beams\", 1)\n    generation_kwargs[\"output_scores\"] = True\n    generation_kwargs[\"return_dict_in_generate\"] = True\n\n    # Do inference\n    with torch.no_grad():\n        model_inputs = self.processor(images, return_tensors=\"pt\").pixel_values\n        model_outputs = self.model.generate(model_inputs.to(self.model.device), **generation_kwargs)\n\n        texts = self.processor.batch_decode(model_outputs.sequences, skip_special_tokens=True)\n        scores = self._compute_sequence_scores(model_outputs)\n\n    # Assemble and return a list of Result objects from the prediction outputs.\n    # `texts` and `scores` are flattened lists so we need to iterate over them in steps.\n    # This is done to ensure that the list of results correspond 1-to-1 with the list of images.\n    results = []\n    metadata = self.metadata | {\"generation_kwargs\": generation_kwargs}\n    step = generation_kwargs[\"num_return_sequences\"]\n    for i in range(0, len(texts), step):\n        texts_chunk = texts[i : i + step]\n        scores_chunk = scores[i : i + step]\n        result = Result.text_recognition_result(metadata, texts_chunk, scores_chunk)\n        results.append(result)\n    return results\n</code></pre>"},{"location":"reference/models.html#models.huggingface.trocr.WordLevelTrOCR","title":"<code>WordLevelTrOCR</code>","text":"<p>               Bases: <code>TrOCR</code></p> <p>A version of TrOCR which outputs words instead of lines.</p> <p>This TrOCR wrapper uses the model's attention weights to estimate word boundaries. See notebook [TODO: link] for more details. It does not support beam search, but can otherwise be used as a drop- in replacement of TrOCR.</p> <p>Example usage with the <code>TextRecognition</code> step: <pre><code>- step: TextRecognition\n  settings:\n    model: WordLevelTrOCR\n    model_settings:\n      model: Riksarkivet/trocr-base-handwritten-hist-swe-2\n      device: cpu\n      model_kwargs:\n        revision: 6ecbb5d643430385e1557001ae78682936f8747f\n    generation_settings:\n      batch_size: 8\n</code></pre></p> Source code in <code>src/htrflow/models/huggingface/trocr.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    processor: str | None = None,\n    model_kwargs: dict[str, Any] | None = None,\n    processor_kwargs: dict[str, Any] | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Arguments:\n        model: Path or name of pretrained VisisonEncoderDeocderModel.\n        processor: Optional path or name of pretrained TrOCRProcessor.\n            If not given, the model path or name is used.\n        model_kwargs: Model initialization kwargs which are forwarded to\n            VisionEncoderDecoderModel.from_pretrained.\n        processor_kwargs: Processor initialization kwargs which are\n            forwarded to TrOCRProcessor.from_pretrained.\n        kwargs: Additional kwargs which are forwarded to BaseModel's\n            __init__.\n    \"\"\"\n    super().__init__(**kwargs)\n\n    # Initialize model\n    model_kwargs = HF_CONFIG | (model_kwargs or {})\n    self.model = VisionEncoderDecoderModel.from_pretrained(model, **model_kwargs)\n    self.model.to(self.device)\n    logger.info(\"Initialized TrOCR model from %s on device %s.\", model, self.model.device)\n\n    # Initialize processor\n    processor = processor or model\n    processor_kwargs = HF_CONFIG | (processor_kwargs or {})\n    self.processor = TrOCRProcessor.from_pretrained(processor, **processor_kwargs)\n    logger.info(\"Initialized TrOCR processor from %s.\", processor)\n\n    self.metadata.update(\n        {\n            \"model\": model,\n            \"model_version\": get_model_info(model),\n            \"processor\": processor,\n            \"processor_version\": get_model_info(processor),\n        }\n    )\n</code></pre>"},{"location":"reference/models.html#models.openmmlab.satrn.Satrn","title":"<code>Satrn</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>HTRflow adapter of Openmmlabs' Satrn model</p> <p>Example usage with the <code>TextRecognition</code> pipeline step: <pre><code>- step: TextRecognition\n  settings:\n    model: Satrn\n    model_settings:\n      model: Riksarkivet/satrn_htr\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Path to a local .pth model weights file or to a huggingface repo which contains a .pth file, for example 'Riksarkivet/satrn_htr'.</p> required <code>config</code> <code>str | None</code> <p>Path to a local config.py file or to a huggingface repo which contains a config.py file, for example 'Riksarkivet/satrn_htr'.</p> <code>None</code> <code>kwargs</code> <p>Additional kwargs which are forwarded to BaseModel's <code>__init__</code>.</p> <code>{}</code> Source code in <code>src/htrflow/models/openmmlab/satrn.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    config: str | None = None,\n    revision: str | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Arguments:\n        model: Path to a local .pth model weights file or to a\n            huggingface repo which contains a .pth file, for example\n            'Riksarkivet/satrn_htr'.\n        config: Path to a local config.py file or to a huggingface\n            repo which contains a config.py file, for example\n            'Riksarkivet/satrn_htr'.\n        kwargs: Additional kwargs which are forwarded to BaseModel's\n            `__init__`.\n    \"\"\"\n    super().__init__(**kwargs)\n\n    config = config or model\n    model_weights, model_config = load_mmlabs(model, config, revision)\n\n    with SuppressOutput():\n        self.model = TextRecInferencer(model=model_config, weights=model_weights, device=self.device)\n\n    logger.info(\n        \"Loaded Satrn model '%s' from %s with config %s on device %s\",\n        model,\n        model_weights,\n        model_config,\n        self.device,\n    )\n\n    self.metadata.update(\n        {\n            \"model\": model,\n            \"model_version\": commit_hash_from_path(model_weights),\n            \"config\": config,\n            \"config_version\": commit_hash_from_path(model_config),\n        }\n    )\n</code></pre>"},{"location":"reference/models.html#models.openmmlab.satrn.Satrn._predict","title":"<code>_predict</code>","text":"<p>Satrn-specific prediction method</p> <p>This method is used by <code>predict()</code> and should typically not be called directly.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[NumpyImage]</code> <p>Input images</p> required <code>kwargs</code> <p>Additional keyword arguments that are forwarded to <code>mmocr.apis.TextRecInferencer.__call__()</code>.</p> <code>{}</code> Source code in <code>src/htrflow/models/openmmlab/satrn.py</code> <pre><code>def _predict(self, images: list[NumpyImage], **kwargs) -&gt; list[Result]:\n    \"\"\"\n    Satrn-specific prediction method\n\n    This method is used by `predict()` and should typically not be\n    called directly.\n\n    Arguments:\n        images: Input images\n        kwargs: Additional keyword arguments that are forwarded to\n            `mmocr.apis.TextRecInferencer.__call__()`.\n    \"\"\"\n    outputs = self.model(\n        images,\n        batch_size=len(images),\n        return_datasamples=False,\n        progress_bar=False,\n        **kwargs,\n    )\n    results = []\n    for prediction in outputs[\"predictions\"]:\n        texts = prediction[\"text\"]\n        scores = prediction[\"scores\"]\n        results.append(Result.text_recognition_result(self.metadata, texts, scores))\n    return results\n</code></pre>"},{"location":"reference/models.html#models.teklia.pylaia.PyLaia","title":"<code>PyLaia</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A minimal HTRflow-style model wrapper around PyLaia.</p> <p>Uses Teklia's implementation of PyLaia. For further information, see: https://atr.pages.teklia.com/pylaia/usage/prediction/#decode-arguments</p> <p>Example usage with the <code>TextRecognition</code> step: <pre><code>- step: TextRecognition\n  settings:\n    model: PyLaia\n    model_settings:\n      model: Teklia/pylaia-belfort\n      device: cuda\n      revision: d35f921605314afc7324310081bee55a805a0b9f\n    generation_settings:\n      batch_size: 8\n      temperature: 1\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The Hugging Face Hub repository ID or a local path with PyLaia artifacts: - weights.ckpt - syms.txt - (optionally) language_model.arpa.gz, lexicon.txt, tokens.txt</p> required <code>revision</code> <code>str | None</code> <p>Optional revision of the Huggingface repository.</p> <code>None</code> <code>use_binary_lm</code> <code>bool</code> <p>Whether to use binary language model format (default: False),                   see <code>get_pylaia_model</code> for more info.</p> <code>False</code> <code>kwargs</code> <p>Additional kwargs passed to BaseModel.init (e.g., 'device').</p> <code>{}</code> Source code in <code>src/htrflow/models/teklia/pylaia.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    revision: str | None = None,\n    use_binary_lm: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Arguments:\n        model (str):\n            The Hugging Face Hub repository ID or a local path with PyLaia artifacts:\n            - weights.ckpt\n            - syms.txt\n            - (optionally) language_model.arpa.gz, lexicon.txt, tokens.txt\n        revision: Optional revision of the Huggingface repository.\n        use_binary_lm (bool): Whether to use binary language model format (default: False),\n                              see `get_pylaia_model` for more info.\n        kwargs:\n            Additional kwargs passed to BaseModel.__init__ (e.g., 'device').\n    \"\"\"\n    super().__init__(**kwargs)\n\n    model_info_dict: PyLaiaModelInfo = get_pylaia_model(model, revision=revision, use_binary_lm=use_binary_lm)\n    self.model = model_info_dict\n    self.model_dir = model_info_dict.model_dir\n    model_version = model_info_dict.model_version\n    self.use_language_model = model_info_dict.use_language_model\n    self.language_model_params = model_info_dict.language_model_params\n\n    self.metadata.update(\n        {\n            \"model\": model,\n            \"model_version\": model_version,\n            \"use_binary_lm\": use_binary_lm,\n        }\n    )\n\n    logger.info(f\"Initialized PyLaiaModel from '{model}' on device '{self.device}'.\")\n</code></pre>"},{"location":"reference/models.html#models.teklia.pylaia.PyLaia._predict","title":"<code>_predict</code>","text":"<p>PyLaia-specific prediction method: runs text recognition.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[ndarray]</code> <p>List of images as NumPy arrays (e.g., shape [H, W, C]).</p> required <code>batch_size</code> <code>int</code> <p>Batch size for decoding. Defaults to 1.</p> required <code>reading_order</code> <code>str</code> <p>Reading order for text recognition. Defaults to \"LTR\".</p> required <code>resize_input_height</code> <code>int</code> <p>If set, resizes input images to the specified height, while maintaining aspect ratio. If <code>-1</code>, resizing is skipped. Defaults to 128.</p> required <code>num_workers</code> <code>int</code> <p>Number of workers for parallel processing. Defaults to <code>multiprocessing.cpu_count()</code>.</p> required <p>Returns:</p> Type Description <code>list[Result]</code> <p>list[Result]: A list of Result objects containing recognized text and optionally confidence scores.</p> Source code in <code>src/htrflow/models/teklia/pylaia.py</code> <pre><code>def _predict(self, images: list[np.ndarray], **decode_kwargs) -&gt; list[Result]:\n    \"\"\"\n    PyLaia-specific prediction method: runs text recognition.\n\n    Args:\n        images (list[np.ndarray]):\n            List of images as NumPy arrays (e.g., shape [H, W, C]).\n        batch_size (int, optional):\n            Batch size for decoding. Defaults to 1.\n        reading_order (str, optional):\n            Reading order for text recognition. Defaults to \"LTR\".\n        resize_input_height (int, optional):\n            If set, resizes input images to the specified height,\n            while maintaining aspect ratio. If `-1`, resizing is skipped. Defaults to 128.\n        num_workers (int, optional):\n            Number of workers for parallel processing. Defaults to `multiprocessing.cpu_count()`.\n\n    Returns:\n        list[Result]:\n            A list of Result objects containing recognized text and\n            optionally confidence scores.\n    \"\"\"\n\n    temperature = decode_kwargs.get(\"temperature\", 1.0)\n    batch_size = decode_kwargs.get(\"batch_size\", 1)\n    reading_order = decode_kwargs.get(\"reading_order\", \"LTR\")\n    resize_input_height = decode_kwargs.get(\"resize_input_height\", 128)\n    num_workers = decode_kwargs.get(\"num_workers\", multiprocessing.cpu_count())\n\n    common_args = CommonArgs(\n        checkpoint=\"weights.ckpt\",\n        train_path=str(self.model_dir),\n        experiment_dirname=\"\",\n    )\n\n    data_args = DataArgs(\n        batch_size=batch_size, color_mode=\"L\", reading_order=reading_order, num_workers=num_workers\n    )\n\n    gpus_flag = 1 if self.device.type == \"cuda\" else 0\n    trainer_args = TrainerArgs(gpus=gpus_flag)\n\n    decode_args = DecodeArgs(\n        include_img_ids=True,\n        join_string=\"\",\n        convert_spaces=True,\n        print_line_confidence_scores=True,\n        print_word_confidence_scores=False,\n        temperature=temperature,\n        use_language_model=self.use_language_model,\n        **self.language_model_params.model_dump(),\n    )\n\n    # Note: PyLaia's 'decode' function expects disk-based file paths rather than in-memory data.\n    # Because it is tightly integrated as a CLI tool, we must create temporary image files\n    # and pass their paths to the PyLaia decoder. Otherwise, PyLaia cannot process these images.\n    tmp_images_dir = Path(mkdtemp())\n    logger.debug(f\"Created temp folder for images: {tmp_images_dir}\")\n\n    image_ids = [str(uuid4()) for _ in images]\n\n    for img_id, np_img in zip(image_ids, images):\n        rezied_img = _ensure_fixed_height(np_img, resize_input_height)\n        cv2.imwrite(str(tmp_images_dir / f\"{img_id}.jpg\"), rezied_img)\n\n    with NamedTemporaryFile() as pred_stdout, NamedTemporaryFile() as img_list:\n        Path(img_list.name).write_text(\"\\n\".join(image_ids))\n\n        with redirect_stdout(open(pred_stdout.name, mode=\"w\")):\n            decode(\n                syms=str(self.model_dir / \"syms.txt\"),\n                img_list=img_list.name,\n                img_dirs=[str(tmp_images_dir)],\n                common=common_args,\n                data=data_args,\n                trainer=trainer_args,\n                decode=decode_args,\n                num_workers=num_workers,\n            )\n            sys.stdout.flush()\n\n        decode_output_lines = Path(pred_stdout.name).read_text().strip().splitlines()\n\n    results = []\n    metadata = self.metadata | {\"decode_kwargs\": decode_kwargs}\n\n    for line in decode_output_lines:\n        match = self.LINE_PREDICTION.match(line)\n        if not match:\n            logger.warning(\"Could not parse line: %s\", line)\n            continue\n        _, score_str, text = match.groups()  # _ = image_id\n\n        try:\n            score_val = float(score_str)\n        except ValueError:\n            score_val = 0.0\n\n        result = Result.text_recognition_result(metadata, [text], [score_val])\n        results.append(result)\n\n    logger.debug(f\"PyLaia recognized {len(results)} lines of text.\")\n\n    return results\n</code></pre>"},{"location":"reference/models.html#segmentation-models","title":"Segmentation models","text":""},{"location":"reference/models.html#models.openmmlab.rtmdet.RTMDet","title":"<code>RTMDet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>HTRFLOW adapter of Openmmlabs' RTMDet model</p> <p>This model can be used for region and line segmentation. Riksarkivet provides two pre-trained RTMDet models:</p> <pre><code>-   https://huggingface.co/Riksarkivet/rtmdet_lines\n-   https://huggingface.co/Riksarkivet/rtmdet_regions\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Path to a local .pth model weights file or to a huggingface repo which contains a .pth file, for example 'Riksarkivet/rtmdet_lines'.</p> required <code>config</code> <code>str | None</code> <p>Path to a local config.py file or to a huggingface repo which contains a config.py file, for example 'Riksarkivet/rtmdet_lines'.</p> <code>None</code> <code>revision</code> <code>str | None</code> <p>A specific model revision, as a commit hash of the model's huggingface repo. If None, the latest available revision is used.</p> <code>None</code> <code>kwargs</code> <p>Additional kwargs which are forwarded to BaseModel's init.</p> <code>{}</code> Source code in <code>src/htrflow/models/openmmlab/rtmdet.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    config: str | None = None,\n    revision: str | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Arguments:\n        model: Path to a local .pth model weights file or to a\n            huggingface repo which contains a .pth file, for example\n            'Riksarkivet/rtmdet_lines'.\n        config: Path to a local config.py file or to a huggingface\n            repo which contains a config.py file, for example\n            'Riksarkivet/rtmdet_lines'.\n        revision: A specific model revision, as a commit hash of the\n            model's huggingface repo. If None, the latest available\n            revision is used.\n        kwargs: Additional kwargs which are forwarded to BaseModel's\n            __init__.\n    \"\"\"\n    super().__init__(**kwargs)\n\n    config = config or model\n    model_weights, model_config = load_mmlabs(model, config, revision)\n\n    with SuppressOutput():\n        self.model = DetInferencer(\n            model=model_config,\n            weights=model_weights,\n            device=self.device,\n            show_progress=False,\n        )\n\n    logger.info(\n        \"Loaded RTMDet model '%s' from %s with config %s on device %s\",\n        model,\n        model_weights,\n        model_config,\n        self.device,\n    )\n\n    self.metadata.update(\n        {\n            \"model\": model,\n            \"model_version\": commit_hash_from_path(model_weights),\n            \"config\": config,\n            \"config_version\": commit_hash_from_path(model_config),\n        }\n    )\n</code></pre>"},{"location":"reference/models.html#models.openmmlab.rtmdet.RTMDet._predict","title":"<code>_predict</code>","text":"<p>RTMDet-specific prediction method</p> <p>This method is used by <code>predict()</code> and should typically not be called directly.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[NumpyImage]</code> <p>List of input images</p> required <code>nms_downscale</code> <code>float</code> <p>If &lt; 1, all masks will be downscaled by this factor before applying NMS. This leads to faster NMS at the expense of accuracy.</p> <code>1.0</code> <code>nms_threshold</code> <code>float</code> <p>Score threshold for segments to keep after NMS.</p> <code>0.4</code> <code>nms_sigma</code> <code>float</code> <p>NMS parameter that affects the score calculation.</p> <code>2.0</code> <code>**kwargs</code> <p>Additional arguments that are passed to DetInferencer.call.</p> <code>{}</code> Source code in <code>src/htrflow/models/openmmlab/rtmdet.py</code> <pre><code>def _predict(\n    self,\n    images: list[NumpyImage],\n    nms_downscale: float = 1.0,\n    nms_threshold: float = 0.4,\n    nms_sigma: float = 2.0,\n    **kwargs,\n) -&gt; list[Result]:\n    \"\"\"\n    RTMDet-specific prediction method\n\n    This method is used by `predict()` and should typically not be\n    called directly.\n\n    Arguments:\n        images: List of input images\n        nms_downscale: If &lt; 1, all masks will be downscaled by this factor\n            before applying NMS. This leads to faster NMS at the expense of\n            accuracy.\n        nms_threshold: Score threshold for segments to keep after NMS.\n        nms_sigma: NMS parameter that affects the score calculation.\n        **kwargs: Additional arguments that are passed to DetInferencer.__call__.\n    \"\"\"\n    batch_size = max(1, len(images))\n    outputs = self.model(\n        images,\n        batch_size=batch_size,\n        draw_pred=False,\n        return_datasample=True,\n        **kwargs,\n    )\n    results = []\n    for image, output in zip(images, outputs[\"predictions\"]):\n        results.append(self._create_segmentation_result(image, output, nms_downscale, nms_threshold, nms_sigma))\n    return results\n</code></pre>"},{"location":"reference/models.html#models.ultralytics.yolo.YOLO","title":"<code>YOLO</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>HTRflow adapter of Ultralytics' YOLO model</p> <p>Example usage with the <code>Segmentation</code> step: <pre><code>- step: Segmentation\n  settings:\n    model: YOLO\n    model_settings:\n      model: Riksarkivet/yolov9-regions-1\n      revision: 7c44178d85926b4a096c55c89bf224855a201fbf\n      device: cpu\n    generation_settings:\n      batch_size: 8\n</code></pre></p> <p><code>generation_settings</code> accepts the same arguments as <code>YOLO.predict()</code>. See the Ultralytics documentation for a list of supported arguments.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Path to a YOLO model. The path can be a path to a local .pt model file (for example, <code>my-model.py</code>) or an indentifier of a Huggingface repo contatining a .pt model file (for example, <code>Riksarkivet/yolov9-regions-1</code>).</p> required <code>revision</code> <code>str | None</code> <p>Optional revision of the Huggingface repository.</p> <code>None</code> Source code in <code>src/htrflow/models/ultralytics/yolo.py</code> <pre><code>def __init__(self, model: str, revision: str | None = None, **kwargs) -&gt; None:\n    \"\"\"\n    Arguments:\n        model: Path to a YOLO model. The path can be a path to a\n            local .pt model file (for example, `my-model.py`) or an\n            indentifier of a Huggingface repo contatining a .pt\n            model file (for example, `Riksarkivet/yolov9-regions-1`).\n        revision: Optional revision of the Huggingface repository.\n    \"\"\"\n    super().__init__(**kwargs)\n\n    model_file = load_ultralytics(model, revision)\n    self.model = UltralyticsYOLO(model_file).to(self.device)\n\n    logger.info(\n        \"Initialized YOLO model '%s' from %s on device %s\",\n        model,\n        model_file,\n        self.model.device,\n    )\n\n    self.metadata.update({\"model\": model, \"model_version\": commit_hash_from_path(model_file)})\n</code></pre>"},{"location":"reference/models.html#models.ultralytics.yolo.YOLO._predict","title":"<code>_predict</code>","text":"<p>Run inference.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[ndarray]</code> <p>Input images</p> required <code>use_polygons</code> <code>bool</code> <p>Wheter to include output polygons (if available), default True.</p> <code>True</code> <code>polygon_approx_level</code> <code>float</code> <p>A parameter which controls the maximum distance between the original polygon and the approximated low-resolution polygon, as a fraction of the original polygon arc length. Example: With <code>polygon_approx_level=0.005</code> and a generated polygon with arc length 100, the approximated polygon will not differ more than 0.5 units from the original.</p> <code>0.005</code> <code>**kwargs</code> <p>Keyword arguments forwarded to the inner YOLO model instance.</p> <code>{}</code> Source code in <code>src/htrflow/models/ultralytics/yolo.py</code> <pre><code>def _predict(\n    self, images: list[np.ndarray], use_polygons: bool = True, polygon_approx_level: float = 0.005, **kwargs\n) -&gt; list[Result]:\n    \"\"\"\n    Run inference.\n\n    Arguments:\n        images: Input images\n        use_polygons: Wheter to include output polygons (if available), default True.\n        polygon_approx_level: A parameter which controls the maximum distance between the original polygon\n            and the approximated low-resolution polygon, as a fraction of the original polygon arc length.\n            Example: With `polygon_approx_level=0.005` and a generated polygon with arc length 100, the\n            approximated polygon will not differ more than 0.5 units from the original.\n        **kwargs: Keyword arguments forwarded to the inner YOLO model instance.\n    \"\"\"\n    outputs = self.model(images, stream=True, verbose=False, **kwargs)\n\n    results = []\n    for image, output in zip(images, outputs):\n        polygons = bboxes = scores = class_labels = None\n        if output.boxes is not None:\n            bboxes = output.boxes.xyxy.int().tolist()\n            scores = output.boxes.conf.tolist()\n            class_labels = [output.names[label] for label in output.boxes.cls.tolist()]\n\n        if use_polygons:\n            if output.masks is None:\n                logger.warning(\"`use_polygons` was set to True but the model did not return any polygons.\")\n            else:\n                polygons = _simplify_polygons(output.masks.xy, polygon_approx_level)\n\n        result = Result.segmentation_result(\n            image.shape[:2],\n            bboxes=bboxes,\n            polygons=polygons,\n            scores=scores,\n            labels=class_labels,\n            metadata=self.metadata,\n        )\n        results.append(result)\n    return results\n</code></pre>"},{"location":"reference/models.html#other-models","title":"Other models","text":""},{"location":"reference/models.html#models.huggingface.dit.DiT","title":"<code>DiT</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>HTRFLOW adapter of DiT for image classification.</p> <p>Uses huggingface's implementation of DiT. For further information about the model, see https://huggingface.co/docs/transformers/model_doc/dit.</p> <p>Initialize a DiT model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Path or name of pretrained AutoModelForImageClassification.</p> required <code>processor</code> <code>str | None</code> <p>Optional path or name of a pretrained AutoImageProcessor. If not given, the given <code>model</code> is used.</p> <code>None</code> <code>model_kwargs</code> <code>dict | None</code> <p>Model initialization kwargs that are forwarded to AutoModelForImageClassification.from_pretrained().</p> <code>None</code> <code>processor_kwargs</code> <code>dict | None</code> <p>Processor initialization kwargs that are forwarded to AutoImageProcessor.from_pretrained().</p> <code>None</code> <code>kwargs</code> <p>Additional kwargs that are forwarded to BaseModel's init.</p> required Source code in <code>src/htrflow/models/huggingface/dit.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    processor: str | None = None,\n    model_kwargs: dict | None = None,\n    processor_kwargs: dict | None = None,\n    device: str | None = None,\n):\n    \"\"\"Initialize a DiT model\n\n    Arguments:\n        model: Path or name of pretrained AutoModelForImageClassification.\n        processor: Optional path or name of a pretrained AutoImageProcessor.\n            If not given, the given `model` is used.\n        model_kwargs: Model initialization kwargs that are forwarded to\n            AutoModelForImageClassification.from_pretrained().\n        processor_kwargs: Processor initialization kwargs that are forwarded\n            to AutoImageProcessor.from_pretrained().\n        kwargs: Additional kwargs that are forwarded to BaseModel's __init__.\n    \"\"\"\n    super().__init__(device)\n\n    # Initialize model\n    model_kwargs = HF_CONFIG | (model_kwargs or {})\n    self.model = AutoModelForImageClassification.from_pretrained(model, **model_kwargs)\n    self.model.to(self.device)\n    logger.info(\"Initialized DiT model from %s on device %s.\", model, self.device)\n\n    # Initialize processor\n    processor = processor or model\n    processor_kwargs = HF_CONFIG | (processor_kwargs or {})\n    self.processor = AutoImageProcessor.from_pretrained(processor, **processor_kwargs)\n    logger.info(\n        \"Initialized DiT processor from %s. Initialization parameters: %s\",\n        processor,\n        processor_kwargs,\n    )\n\n    self.metadata.update(\n        {\n            \"model\": model,\n            \"model_version\": get_model_info(model),\n            \"processor\": processor,\n            \"processor_version\": get_model_info(processor),\n        }\n    )\n</code></pre>"},{"location":"reference/models.html#models.huggingface.dit.DiT._predict","title":"<code>_predict</code>","text":"<p>Perform inference on <code>images</code></p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[ndarray]</code> <p>List of input images.</p> required <code>return_format</code> <code>Literal['argmax', 'softmax']</code> <p>Decides the format of the output. Options are: - \"softmax\": returns the confidence scores for each class     label and image. Default. - \"argmax\": returns the most probable class label for each     image.</p> <code>'softmax'</code> Source code in <code>src/htrflow/models/huggingface/dit.py</code> <pre><code>def _predict(\n    self,\n    images: list[np.ndarray],\n    return_format: Literal[\"argmax\", \"softmax\"] = \"softmax\",\n) -&gt; list[Result]:\n    \"\"\"Perform inference on `images`\n\n    Arguments:\n        images: List of input images.\n        return_format: Decides the format of the output. Options are:\n            - \"softmax\": returns the confidence scores for each class\n                label and image. Default.\n            - \"argmax\": returns the most probable class label for each\n                image.\n    \"\"\"\n    inputs = self.processor(images, return_tensors=\"pt\").pixel_values\n\n    with torch.no_grad():\n        batch_logits = self.model(inputs.to(self.model.device)).logits\n\n    return [\n        Result(\n            metadata=self.metadata,\n            data=[{\"classification\": self._get_labels(logits, return_format)}],\n        )\n        for logits in batch_logits\n    ]\n</code></pre>"},{"location":"reference/pipeline-steps.html","title":"Pipeline steps","text":"<p>This page lists HTRflow's built-in pipeline steps.</p>"},{"location":"reference/pipeline-steps.html#base-step","title":"Base step","text":""},{"location":"reference/pipeline-steps.html#pipeline.steps.PipelineStep","title":"<code>PipelineStep</code>","text":"<p>Pipeline step base class.</p> <p>Pipeline steps are implemented by subclassing this class and overriding the <code>run()</code> method.</p>"},{"location":"reference/pipeline-steps.html#pipeline.steps.PipelineStep.run","title":"<code>run</code>","text":"<p>Run the pipeline step.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>Collection</code> <p>Input collection</p> required <p>Returns:</p> Type Description <code>Collection</code> <p>A new collection, updated with the results of the pipeline step.</p> Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def run(self, collection: Collection) -&gt; Collection:\n    \"\"\"\n    Run the pipeline step.\n\n    Arguments:\n        collection: Input collection\n\n    Returns:\n        A new collection, updated with the results of the pipeline step.\n    \"\"\"\n</code></pre>"},{"location":"reference/pipeline-steps.html#pre-processing-steps","title":"Pre-processing steps","text":""},{"location":"reference/pipeline-steps.html#pipeline.steps.ProcessImages","title":"<code>ProcessImages</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Base for image preprocessing steps.</p> <p>This is a base class for all image preprocessing steps. Subclasses define their image processing operation by overriding the <code>op()</code> method. This step does not alter the original image. Instead, a new copy of the image is saved in the directory specified by <code>ProcessImages.output_directory</code>. The <code>PageNode</code>'s image path is then updated to point to the new processed image.</p> <p>Attributes:</p> Name Type Description <code>output_directory</code> <code>str</code> <p>Where to write the processed images.</p>"},{"location":"reference/pipeline-steps.html#pipeline.steps.ProcessImages.op","title":"<code>op</code>","text":"<p>Perform the image processing operation on <code>image</code>.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NumpyImage</code> <p>Input image.</p> required <p>Returns:</p> Type Description <code>NumpyImage</code> <p>A processed version of <code>image</code>.</p> Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def op(self, image: NumpyImage) -&gt; NumpyImage:\n    \"\"\"\n    Perform the image processing operation on `image`.\n\n    Arguments:\n        image: Input image.\n\n    Returns:\n        A processed version of `image`.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/pipeline-steps.html#pipeline.steps.Binarization","title":"<code>Binarization</code>","text":"<p>               Bases: <code>ProcessImages</code></p> <p>Binarize images.</p> <p>Runs image binarization on the collection's images. Saves the resulting images in a directory named <code>binarized</code>. All subsequent pipeline steps will use the binarized images.</p> <p>Example YAML: <pre><code>- step: Binarization\n</code></pre></p>"},{"location":"reference/pipeline-steps.html#inference-steps","title":"Inference steps","text":""},{"location":"reference/pipeline-steps.html#pipeline.steps.Inference","title":"<code>Inference</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Run model inference.</p> <p>This is a generic pipeline step for any type of model inference. This step always runs the model on the images of the collection's leaf nodes.</p> <p>Example YAML: <pre><code>- step: Inference\n  settings:\n    model: DiT\n    model_settings:\n      model: ...\n</code></pre></p> Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def __init__(self, model_class, model_kwargs, generation_kwargs):\n    self.model_class = model_class\n    self.model_kwargs = model_kwargs\n    self.generation_kwargs = generation_kwargs\n    self.model = None\n</code></pre>"},{"location":"reference/pipeline-steps.html#pipeline.steps.Segmentation","title":"<code>Segmentation</code>","text":"<p>               Bases: <code>Inference</code></p> <p>Run a segmentation model.</p> <p>See Segmentation models for available models.</p> <p>Example YAML: <pre><code>- step: Segmentation\n  settings:\n    model: yolo\n    model_settings:\n      model: Riksarkivet/yolov9-regions-1\n</code></pre></p> Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def __init__(self, model_class, model_kwargs, generation_kwargs):\n    self.model_class = model_class\n    self.model_kwargs = model_kwargs\n    self.generation_kwargs = generation_kwargs\n    self.model = None\n</code></pre>"},{"location":"reference/pipeline-steps.html#pipeline.steps.TextRecognition","title":"<code>TextRecognition</code>","text":"<p>               Bases: <code>Inference</code></p> <p>Run a text recognition model.</p> <p>See Text recognition models for available models.</p> <p>Example YAML: <pre><code>- step: TextRecognition\n  settings:\n    model: TrOCR\n    model_settings:\n      model: Riksarkivet/trocr-base-handwritten-hist-swe-2\n</code></pre></p> Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def __init__(self, model_class, model_kwargs, generation_kwargs):\n    self.model_class = model_class\n    self.model_kwargs = model_kwargs\n    self.generation_kwargs = generation_kwargs\n    self.model = None\n</code></pre>"},{"location":"reference/pipeline-steps.html#post-processing-steps","title":"Post-processing steps","text":""},{"location":"reference/pipeline-steps.html#pipeline.steps.Prune","title":"<code>Prune</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Remove nodes based on a given condition.</p> <p>This is a generic pruning (filtering) step which removes nodes (segments, lines, words) based on the given condition. The condition is a function <code>f</code> such that <code>f(node) == True</code> if <code>node</code> should be removed from the tree. This step runs <code>f</code> on all nodes, at all segmentation levels. See the <code>RemoveLowTextConfidence[Lines|Regions|Pages]</code> steps for examples of how to formulate <code>condition</code>.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>Callable[[Node], bool]</code> <p>A function <code>f</code> such that <code>f(node) == True</code> if <code>node</code> should be removed from the document tree.</p> required Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def __init__(self, condition: Callable[[Node], bool]):\n    \"\"\"\n    Arguments:\n        condition: A function `f` such that `f(node) == True` if\n            `node` should be removed from the document tree.\n    \"\"\"\n    self.condition = condition\n</code></pre>"},{"location":"reference/pipeline-steps.html#pipeline.steps.RemoveLowTextConfidencePages","title":"<code>RemoveLowTextConfidencePages</code>","text":"<p>               Bases: <code>Prune</code></p> <p>Remove all pages where the average text confidence score is below <code>threshold</code>.</p> <p>Example YAML: <pre><code>- step: RemoveLowTextConfidencePages\n  settings:\n    threshold: 0.8\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Confidence score threshold.</p> required Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def __init__(self, threshold: float):\n    \"\"\"\n    Arguments:\n        threshold: Confidence score threshold.\n    \"\"\"\n    super().__init__(\n        lambda node: node.parent and node.parent.is_root() and metrics.average_text_confidence(node) &lt; threshold\n    )\n</code></pre>"},{"location":"reference/pipeline-steps.html#pipeline.steps.RemoveLowTextConfidenceRegions","title":"<code>RemoveLowTextConfidenceRegions</code>","text":"<p>               Bases: <code>Prune</code></p> <p>Remove all regions where the average text confidence score is below <code>threshold</code>.</p> <p>Example YAML: <pre><code>- step: RemoveLowTextConfidenceRegions\n  settings:\n    threshold: 0.8\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Confidence score threshold.</p> required Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def __init__(self, threshold: float):\n    \"\"\"\n    Arguments:\n        threshold: Confidence score threshold.\n    \"\"\"\n    super().__init__(lambda node: node.is_region() and metrics.average_text_confidence(node) &lt; threshold)\n</code></pre>"},{"location":"reference/pipeline-steps.html#pipeline.steps.RemoveLowTextConfidenceLines","title":"<code>RemoveLowTextConfidenceLines</code>","text":"<p>               Bases: <code>Prune</code></p> <p>Remove all lines with text confidence score below <code>threshold</code>.</p> <p>Example YAML: <pre><code>- step: RemoveLowTextConfidenceLines\n  settings:\n    threshold: 0.8\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Confidence score threshold.</p> required Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def __init__(self, threshold: float):\n    \"\"\"\n    Arguments:\n        threshold: Confidence score threshold.\n    \"\"\"\n    super().__init__(lambda node: node.is_line() and metrics.line_text_confidence(node) &lt; threshold)\n</code></pre>"},{"location":"reference/pipeline-steps.html#pipeline.steps.ReadingOrderMarginalia","title":"<code>ReadingOrderMarginalia</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Order regions and lines by reading order.</p> <p>This step orders the pages' first- and second-level segments (corresponding to regions and lines). Both the regions and their lines are ordered using <code>reading_order.order_regions</code>.</p> <p>Parameters:</p> Name Type Description Default <code>two_page</code> <code>Literal['auto'] | bool</code> <p>Whether the page is a two-page spread. Three modes: - 'auto': determine heuristically for each page using     <code>layout.is_twopage</code> - True: assume all pages are spreads - False: assume all pages are single pages</p> <code>False</code> Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def __init__(self, two_page: Literal[\"auto\"] | bool = False):\n    \"\"\"\n    Arguments:\n        two_page: Whether the page is a two-page spread. Three modes:\n            - 'auto': determine heuristically for each page using\n                `layout.is_twopage`\n            - True: assume all pages are spreads\n            - False: assume all pages are single pages\n    \"\"\"\n    self.two_page = two_page\n</code></pre>"},{"location":"reference/pipeline-steps.html#pipeline.steps.OrderLines","title":"<code>OrderLines</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Order lines top-down.</p> <p>This step orders the lines within each region top-down.</p> <p>Example YAML: <pre><code>- step: OrderLines\n</code></pre></p>"},{"location":"reference/pipeline-steps.html#pipeline.steps.WordSegmentation","title":"<code>WordSegmentation</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Segment lines into words.</p> <p>This step segments lines of text into words. It estimates the word boundaries from the recognized text, which means that this step must be run after a line-based text recognition model.</p> <p>See also <code>&lt;models.huggingface.trocr.WordLevelTrOCR&gt;</code>, which is a version of TrOCR that outputs word-level text directly using a more sophisticated method.</p> <p>Example YAML: <pre><code>- step: WordSegmentation\n</code></pre></p>"},{"location":"reference/pipeline-steps.html#export-steps","title":"Export steps","text":""},{"location":"reference/pipeline-steps.html#pipeline.steps.Export","title":"<code>Export</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Export results.</p> <p>Exports the current state of the collection in the given format. This step is typically the last step of a pipeline, however, it can be inserted at any pipeline stage. For example, you could put an <code>Export</code> step before a post processing step in order to save a copy without post processing. A pipeline can include as many <code>Export</code> steps as you like.</p> <p>See Export formats or the <code>&lt;serialization.serialization&gt;</code> module for more details about each export format.</p> <p>Example: <pre><code>- step: Export\n  settings:\n    format: Alto\n    dest: alto-outputs\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>dest</code> <code>str</code> <p>Output directory.</p> required <code>format</code> <code>Literal['alto', 'page', 'txt', 'json']</code> <p>Output format as a string.</p> required Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def __init__(\n    self,\n    dest: str,\n    format: Literal[\"alto\", \"page\", \"txt\", \"json\"],\n    **serializer_kwargs,\n):\n    \"\"\"\n    Arguments:\n        dest: Output directory.\n        format: Output format as a string.\n    \"\"\"\n    self.serializer = get_serializer(format, **serializer_kwargs)\n    self.dest = dest\n</code></pre>"},{"location":"reference/pipeline-steps.html#pipeline.steps.ExportImages","title":"<code>ExportImages</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Export the collection's images.</p> <p>This step writes all existing images (regions, lines, etc.) in the collection to disk. The exported images are the images that have been passed to previous <code>Inference</code> steps and the images that would be passed to a following <code>Inference</code> step.</p> <p>Example YAML: <pre><code>- step: ExportImages\n  settings:\n    dest: exported_images\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>dest</code> <code>str</code> <p>Destination directory.</p> required Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def __init__(self, dest: str):\n    \"\"\"\n    Arguments:\n        dest: Destination directory.\n    \"\"\"\n    self.dest = dest\n    os.makedirs(self.dest, exist_ok=True)\n</code></pre>"},{"location":"reference/pipeline-steps.html#misc","title":"Misc","text":""},{"location":"reference/pipeline-steps.html#pipeline.steps.Break","title":"<code>Break</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Break the pipeline! Used for testing.</p> <p>Example YAML: <pre><code>- step: Break\n</code></pre></p>"},{"location":"reference/pipeline-steps.html#pipeline.steps.ImportSegmentation","title":"<code>ImportSegmentation</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Import segmentation from PageXML files.</p> <p>This step replicates the line segmentation from PageXML files. It can be used to import ground truth segmentation for evaluation purposes.</p> <p>Example YAML: <pre><code>- step: ImportSegmentation\n  settings:\n    source: /path/to/pageXMLs\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to a directory with PageXML files. The XML files must have the same names as the input image files (ignoring the file extension).</p> required Source code in <code>src/htrflow/pipeline/steps.py</code> <pre><code>def __init__(self, source: str):\n    \"\"\"\n    Arguments:\n        source: Path to a directory with PageXML files. The XML files\n            must have the same names as the input image files (ignoring\n            the file extension).\n    \"\"\"\n    self.source = source\n</code></pre>"},{"location":"resources/index.html","title":"Open Source","text":""},{"location":"resources/index.html#hugging-face-wip","title":"Hugging Face (WIP)","text":""},{"location":"resources/cases.html","title":"Projects","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p>"},{"location":"resources/cases.html#indexing-projects","title":"Indexing projects","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.</p>"},{"location":"resources/cases.html#seamless-access-to-transcribed-text-swedish-national-archives-universalviewer-customization","title":"Seamless Access to Transcribed Text: Swedish National Archives' UniversalViewer Customization","text":"<p>Work in progress</p> <p>Riksarkivet's UniversalViewer is still under development, with an estimated finish the end of this year.</p> <p>This project aims to make the archival documents held by the Swedish National Archives, transcribed using Htrflow, accessible to everyone in a convenient and effortless way. To achieve this, the National Archives is utilizing the community-developed, open-source media viewer UniversalViewer, capable of displaying images, audio, video, and even 3D versions of digitized and born-digital cultural heritage collections. UniversalViewer is built on the IIIF (International Image Interoperability Framework) standard for interoperable images. By utilizing a IIIF Content Search API endpoint, the National Archives make the transcribed text from Htrflow accessible for users through UniversalViewer. In October 2024, the National Archives are launching a new customized version of UniversalViewer 4 (UV4), which includes a function to search within transcriptions of archival documents displayed in the viewer. This function enables researchers, students, and the general public to interact with the documents held by the National Archives in a new and efficient way \u2013 without needing to decipher old handwriting. Users can search the archival documents and view the image of the original document next to the transcribed text.</p> <ul> <li>The National Archives are contributing these new functions to the main UV4 GitHub repository. Key features of the National Archives UV4 include:</li> <li>Search Pane: A new search pane on the left to search and display hits, with text snippets, within the archival document.</li> <li>Transcribed Text Pane: A new pane on the right displaying the transcribed text next to the image of the original archival document.</li> <li>Highlighting Function: A function that highlights the search hits both in the image and in the text snippets in the search pane.</li> <li>Image Adjustments: Tools for users to adjust image settings (saturation, contrast, light) within the viewer.</li> <li>User-Friendly Features: Options to print, download, and copy files, transcribed text, and metadata in user-friendly ways within the viewer.</li> </ul>"},{"location":"resources/datasets.html","title":"Datasets","text":""},{"location":"resources/datasets.html#datasets","title":"Datasets","text":"Datasets Description Link Svea hovr\u00e4tt Svea hovr\u00e4tt (Renskrivna protokoll) - 1713\u20131735 https://sok.riksarkivet.se/arkiv/H2hpDbNn14scxjzdWqAaJ1 Bergm\u00e4staren Bergm\u00e4staren i Nora m fl bergslag (Hammartingsprotokoll) - 1698\u20131765 https://sok.riksarkivet.se/arkiv/M5Fe2TT9rH6cxG02H087k3 Trolldomskommissionen Trolldomskommissionen - mainly 1670s https://sok.riksarkivet.se/trolldomskommissionen Bergskollegium Bergskollegium - 1718\u20131758 https://sok.riksarkivet.se/arkiv/SMFky31ekQ80Qsk0UCZZE2 J\u00e4mtlands J\u00e4mtlands domsaga - 1647\u20131688 https://sok.riksarkivet.se/arkiv/2l4NYFT8rH6cxG02H087k3 Stockholms domkapitel Stockholms domkapitel - 1728\u20131759 https://sok.riksarkivet.se/arkiv/etg1tyeEaIPMBzKbUKTjw1 Politikollegiet Politikollegiet - 1729\u20131759 https://sok.riksarkivet.se/arkiv/1lQnXIDiKaYxRLBlK1dGF3 Poliskammaren G\u00f6teborgs poliskammare (Detektiva polisens rapportb\u00f6cker) -  1868\u20131901 https://sok.riksarkivet.se/arkiv/oLTOi9yxweZJUG018W43t3 Court Records Renovated Court Records (the National Archives of Finland) - 1800s https://tuomiokirjat.kansallisarkisto.fi/"},{"location":"resources/datasets.html#dataset-contributions","title":"dataset \u2013 Contributions","text":"<p>The AI models used in HTRFLOW is the result of a collaborative effort, involving the National Archives in both Sweden and Finland, in partnership with the Stockholm City Archives, J\u00e4mtlands l\u00e4ns fornskrifts\u00e4llskap, citizen science volunteers and researchers from Stockholm and Uppsala Universities.</p> <p>Several datasets have been created by participants through Citizen Science using the Handwritten Text Recognition (HTR) software, Transkribus, provided by READ-COOP SCE .</p>"},{"location":"resources/datasets.html#ongoing-research-collaborations","title":"Ongoing research collaborations","text":"<p>Transcription node Sweden \u2013 machine interpretation and citizen research combined, Swedish National Archives and University of Gothenburg, funded by the Swedish National Heritage Board.</p> <p>Mapping the geographies of early modern mining knowledge. A digital history of the study tours of the Swedish Bureau of Mines, 1691\u20131826, Uppsala University and Stockholm University, funded by the Swedish Research Council.</p> <p>The Swedish National Archives' research and development on HTR is part of the Swedish national infrastructure Huminfra. Click here for more information.</p>"},{"location":"resources/models.html","title":"Models","text":""},{"location":"resources/models.html#models","title":"Models","text":"<p>The models used in this demo are very much a work in progress, and as more data, and new architectures, becomes available, they will be retrained and reevaluated. For more information about the models, please refer to their model-cards on Huggingface.</p> Model Name Description Task Riksarkivet/rtmdet_regions nan instance segmentation and object detection Riksarkivet/rtmdet_lines nan instance segmentation and object detection Riksarkivet/satrn_htr nan image-to-text"}]}