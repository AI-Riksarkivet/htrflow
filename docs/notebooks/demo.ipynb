{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models / inferencers\n",
    "Models & inferencers accept lists of images, and return lists of results (either segmentation or recognition results)\n",
    "\n",
    "I have made a dummy `SegmentationModel` and `RecognitionModel` in `models.py`. These do the same thing as the current inferencers.\n",
    "\n",
    "```python\n",
    "class SegmentationModel:\n",
    "    def __call__(self, images: list[np.ndarray]) -> list[SegmentationResult]:\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SegmentationResult:\n",
    "    boxes: np.ndarray\n",
    "    masks: np.ndarray\n",
    "    scores: np.ndarray\n",
    "    labels: np.ndarray\n",
    "```\n",
    "\n",
    "\n",
    "(It would be nice to wrap all models in a \"batching\" function, which divides an input list into chunks if it is too long) -> This is a card in DevOps\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Volume class\n",
    "\n",
    "To load images, create a `Volume`. The name of this class is not set in stone... It represents what Catrin called a \"batch\", a divison of an archive volume, but I don't want to use \"batch\" because of potential confusion with a model's batch (the number of inputs it operates on simultaneously). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrflow_core.volume import Volume\n",
    "\n",
    "images = ['../assets/demo_image.jpg'] * 5 \n",
    "\n",
    "volume = Volume(images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Volume` instance holds a tree. We see the root `node` and its five children, each representing one input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└──<htrflow_core.volume.Node object at 0x7f70f6f38250>\n",
      "    ├──626x1629 image demo_image\n",
      "    ├──626x1629 image demo_image\n",
      "    ├──626x1629 image demo_image\n",
      "    ├──626x1629 image demo_image\n",
      "    └──626x1629 image demo_image\n"
     ]
    }
   ],
   "source": [
    "print(volume)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are available through `volume.images()`. We pass them through a segmentation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegmentationResult(metadata={'model_name': 'SegmentationModel'}, image=array([[[118, 120, 128],\n",
      "        [115, 117, 125],\n",
      "        [114, 116, 124],\n",
      "        ...,\n",
      "        [215, 219, 220],\n",
      "        [209, 213, 214],\n",
      "        [206, 210, 211]],\n",
      "\n",
      "       [[110, 112, 120],\n",
      "        [110, 112, 120],\n",
      "        [110, 112, 120],\n",
      "        ...,\n",
      "        [211, 215, 216],\n",
      "        [207, 211, 212],\n",
      "        [209, 213, 214]],\n",
      "\n",
      "       [[109, 112, 120],\n",
      "        [109, 112, 120],\n",
      "        [104, 107, 115],\n",
      "        ...,\n",
      "        [207, 211, 212],\n",
      "        [205, 209, 210],\n",
      "        [209, 213, 214]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[146, 152, 151],\n",
      "        [147, 153, 152],\n",
      "        [147, 153, 152],\n",
      "        ...,\n",
      "        [212, 218, 213],\n",
      "        [214, 222, 211],\n",
      "        [211, 221, 204]],\n",
      "\n",
      "       [[144, 150, 149],\n",
      "        [146, 152, 151],\n",
      "        [148, 154, 153],\n",
      "        ...,\n",
      "        [217, 223, 212],\n",
      "        [220, 231, 205],\n",
      "        [216, 234, 187]],\n",
      "\n",
      "       [[147, 153, 152],\n",
      "        [149, 155, 154],\n",
      "        [151, 157, 156],\n",
      "        ...,\n",
      "        [214, 221, 208],\n",
      "        [214, 228, 194],\n",
      "        [208, 231, 169]]], dtype=uint8), segments=[Segment(bbox=(345, 751, 11, 167), mask=array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), score=0.7689563885870707, class_label='region', polygon=array([[345,  85],\n",
      "       [356, 115],\n",
      "       [393, 140],\n",
      "       [527, 167],\n",
      "       [672, 151],\n",
      "       [726, 127],\n",
      "       [751,  93],\n",
      "       [740,  63],\n",
      "       [703,  38],\n",
      "       [570,  11],\n",
      "       [417,  29],\n",
      "       [365,  55]], dtype=int32))])\n"
     ]
    }
   ],
   "source": [
    "from htrflow_core.models.dummy_models import SegmentationModel\n",
    "\n",
    "model = SegmentationModel()\n",
    "results = model(volume.images())\n",
    "print(results[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are a list of `SegmentationResult`. To apply the results to the input images, we pass them back to the volume with its `update` method. It returns the new regions as a list of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = volume.update(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The volume tree has now grown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└──<htrflow_core.volume.Node object at 0x7f70f6f38250>\n",
      "    ├──626x1629 image demo_image\n",
      "    │   └──156x406 region at (345, 11)\n",
      "    ├──626x1629 image demo_image\n",
      "    │   ├──117x406 region at (17, 0)\n",
      "    │   ├──156x406 region at (948, 262)\n",
      "    │   └──156x309 region at (0, 85)\n",
      "    ├──626x1629 image demo_image\n",
      "    │   ├──156x406 region at (480, 173)\n",
      "    │   ├──156x406 region at (690, 11)\n",
      "    │   ├──149x406 region at (570, 0)\n",
      "    │   ├──156x332 region at (1296, 381)\n",
      "    │   └──156x292 region at (0, 16)\n",
      "    ├──626x1629 image demo_image\n",
      "    │   ├──99x213 region at (1415, 0)\n",
      "    │   └──116x406 region at (678, 509)\n",
      "    └──626x1629 image demo_image\n",
      "        ├──156x278 region at (0, 234)\n",
      "        ├──156x406 region at (786, 133)\n",
      "        ├──156x406 region at (1105, 461)\n",
      "        └──90x406 region at (442, 0)\n"
     ]
    }
   ],
   "source": [
    "print(volume)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new regions can be passed through a segmentation model (such as a line model) again. The `update` method always updates the leaves of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└──<htrflow_core.volume.Node object at 0x7f70f6f38250>\n",
      "    ├──626x1629 image demo_image\n",
      "    │   └──156x406 region at (345, 11)\n",
      "    │       ├──37x100 region at (517, 129)\n",
      "    │       ├──22x100 region at (636, 144)\n",
      "    │       ├──38x100 region at (543, 125)\n",
      "    │       ├──38x100 region at (486, 122)\n",
      "    │       └──38x69 region at (681, 38)\n",
      "    ├──626x1629 image demo_image\n",
      "    │   ├──117x406 region at (17, 0)\n",
      "    │   │   └──28x100 region at (216, 70)\n",
      "    │   ├──156x406 region at (948, 262)\n",
      "    │   │   ├──33x100 region at (1070, 384)\n",
      "    │   │   ├──38x87 region at (948, 359)\n",
      "    │   │   └──38x57 region at (1296, 329)\n",
      "    │   └──156x309 region at (0, 85)\n",
      "    │       ├──38x76 region at (7, 159)\n",
      "    │       ├──38x76 region at (142, 124)\n",
      "    │       ├──34x76 region at (218, 85)\n",
      "    │       ├──38x76 region at (215, 125)\n",
      "    │       └──38x76 region at (52, 105)\n",
      "    ├──626x1629 image demo_image\n",
      "    │   ├──156x406 region at (480, 173)\n",
      "    │   │   ├──38x100 region at (623, 272)\n",
      "    │   │   ├──38x100 region at (498, 270)\n",
      "    │   │   ├──38x100 region at (561, 244)\n",
      "    │   │   └──38x100 region at (652, 261)\n",
      "    │   ├──156x406 region at (690, 11)\n",
      "    │   │   ├──38x82 region at (690, 122)\n",
      "    │   │   ├──38x95 region at (690, 13)\n",
      "    │   │   ├──37x54 region at (690, 129)\n",
      "    │   │   ├──38x100 region at (919, 95)\n",
      "    │   │   └──38x100 region at (805, 59)\n",
      "    │   ├──149x406 region at (570, 0)\n",
      "    │   │   └──23x71 region at (904, 125)\n",
      "    │   ├──156x332 region at (1296, 381)\n",
      "    │   │   ├──38x53 region at (1296, 403)\n",
      "    │   │   ├──35x82 region at (1469, 381)\n",
      "    │   │   └──38x82 region at (1328, 457)\n",
      "    │   └──156x292 region at (0, 16)\n",
      "    │       └──38x65 region at (0, 129)\n",
      "    ├──626x1629 image demo_image\n",
      "    │   ├──99x213 region at (1415, 0)\n",
      "    │   │   ├──24x52 region at (1426, 71)\n",
      "    │   │   ├──24x52 region at (1463, 37)\n",
      "    │   │   └──24x52 region at (1525, 31)\n",
      "    │   └──116x406 region at (678, 509)\n",
      "    │       ├──28x100 region at (929, 544)\n",
      "    │       └──28x76 region at (1007, 512)\n",
      "    └──626x1629 image demo_image\n",
      "        ├──156x278 region at (0, 234)\n",
      "        │   └──38x68 region at (144, 330)\n",
      "        ├──156x406 region at (786, 133)\n",
      "        │   ├──38x100 region at (891, 223)\n",
      "        │   ├──38x64 region at (786, 154)\n",
      "        │   ├──38x100 region at (1000, 245)\n",
      "        │   └──38x100 region at (911, 242)\n",
      "        ├──156x406 region at (1105, 461)\n",
      "        │   ├──29x100 region at (1170, 587)\n",
      "        │   ├──38x100 region at (1194, 571)\n",
      "        │   └──38x100 region at (1219, 509)\n",
      "        └──90x406 region at (442, 0)\n",
      "            ├──22x91 region at (442, 14)\n",
      "            ├──13x67 region at (780, 0)\n",
      "            ├──22x100 region at (681, 18)\n",
      "            ├──21x100 region at (554, 0)\n",
      "            └──22x100 region at (667, 6)\n"
     ]
    }
   ],
   "source": [
    "results = model(volume.segments())\n",
    "volume.update(results)\n",
    "print(volume)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the segmentation is done, the segments can be passed to a text recognition model. The results are passed to the workbench in the same manner as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└──<htrflow_core.volume.Node object at 0x7f70f6f38250>\n",
      "    ├──626x1629 image demo_image\n",
      "    │   └──156x406 region at (345, 11)\n",
      "    │       ├──37x100 region at (517, 129) \"Dolor velit non non tempora magnam ut adipisci.\"\n",
      "    │       ├──22x100 region at (636, 144) \"Dolor quiquia quisquam adipisci velit velit quiquia quiquia.\"\n",
      "    │       ├──38x100 region at (543, 125) \"Ipsum labore dolorem ut neque ipsum velit.\"\n",
      "    │       ├──38x100 region at (486, 122) \"Consectetur est numquam voluptatem quiquia ipsum.\"\n",
      "    │       └──38x69 region at (681, 38) \"Magnam etincidunt consectetur neque quaerat ut sit ipsum.\"\n",
      "    ├──626x1629 image demo_image\n",
      "    │   ├──117x406 region at (17, 0)\n",
      "    │   │   └──28x100 region at (216, 70) \"Modi sed non tempora.\"\n",
      "    │   ├──156x406 region at (948, 262)\n",
      "    │   │   ├──33x100 region at (1070, 384) \"Numquam quiquia ut etincidunt sit quaerat adipisci.\"\n",
      "    │   │   ├──38x87 region at (948, 359) \"Est etincidunt dolore modi.\"\n",
      "    │   │   └──38x57 region at (1296, 329) \"Dolore ut tempora numquam voluptatem dolorem etincidunt non.\"\n",
      "    │   └──156x309 region at (0, 85)\n",
      "    │       ├──38x76 region at (7, 159) \"Numquam amet quisquam magnam modi.\"\n",
      "    │       ├──38x76 region at (142, 124) \"Dolorem dolorem eius aliquam eius.\"\n",
      "    │       ├──34x76 region at (218, 85) \"Eius tempora modi sit.\"\n",
      "    │       ├──38x76 region at (215, 125) \"Tempora labore velit dolor.\"\n",
      "    │       └──38x76 region at (52, 105) \"Consectetur neque labore porro quiquia.\"\n",
      "    ├──626x1629 image demo_image\n",
      "    │   ├──156x406 region at (480, 173)\n",
      "    │   │   ├──38x100 region at (623, 272) \"Quaerat sed ipsum tempora.\"\n",
      "    │   │   ├──38x100 region at (498, 270) \"Ipsum aliquam consectetur dolor.\"\n",
      "    │   │   ├──38x100 region at (561, 244) \"Sed magnam aliquam aliquam dolor.\"\n",
      "    │   │   └──38x100 region at (652, 261) \"Sed dolor amet sed adipisci etincidunt.\"\n",
      "    │   ├──156x406 region at (690, 11)\n",
      "    │   │   ├──38x82 region at (690, 122) \"Voluptatem aliquam aliquam porro amet.\"\n",
      "    │   │   ├──38x95 region at (690, 13) \"Modi aliquam quiquia etincidunt labore.\"\n",
      "    │   │   ├──37x54 region at (690, 129) \"Tempora dolore quiquia ipsum neque consectetur tempora.\"\n",
      "    │   │   ├──38x100 region at (919, 95) \"Tempora labore modi ut non.\"\n",
      "    │   │   └──38x100 region at (805, 59) \"Ut dolorem labore dolore consectetur.\"\n",
      "    │   ├──149x406 region at (570, 0)\n",
      "    │   │   └──23x71 region at (904, 125) \"Est labore dolor est.\"\n",
      "    │   ├──156x332 region at (1296, 381)\n",
      "    │   │   ├──38x53 region at (1296, 403) \"Neque eius adipisci amet voluptatem consectetur.\"\n",
      "    │   │   ├──35x82 region at (1469, 381) \"Voluptatem magnam voluptatem labore sed dolore voluptatem.\"\n",
      "    │   │   └──38x82 region at (1328, 457) \"Dolore ut magnam voluptatem etincidunt amet adipisci.\"\n",
      "    │   └──156x292 region at (0, 16)\n",
      "    │       └──38x65 region at (0, 129) \"Etincidunt etincidunt quiquia porro velit.\"\n",
      "    ├──626x1629 image demo_image\n",
      "    │   ├──99x213 region at (1415, 0)\n",
      "    │   │   ├──24x52 region at (1426, 71) \"Etincidunt etincidunt dolorem modi dolorem.\"\n",
      "    │   │   ├──24x52 region at (1463, 37) \"Neque quaerat dolorem magnam.\"\n",
      "    │   │   └──24x52 region at (1525, 31) \"Sed aliquam dolor quisquam numquam.\"\n",
      "    │   └──116x406 region at (678, 509)\n",
      "    │       ├──28x100 region at (929, 544) \"Velit tempora non quiquia magnam ipsum sed.\"\n",
      "    │       └──28x76 region at (1007, 512) \"Dolor sed velit quisquam dolor.\"\n",
      "    └──626x1629 image demo_image\n",
      "        ├──156x278 region at (0, 234)\n",
      "        │   └──38x68 region at (144, 330) \"Amet adipisci quaerat quiquia sit dolor numquam ut.\"\n",
      "        ├──156x406 region at (786, 133)\n",
      "        │   ├──38x100 region at (891, 223) \"Etincidunt velit ut neque labore quisquam.\"\n",
      "        │   ├──38x64 region at (786, 154) \"Aliquam labore aliquam quaerat consectetur.\"\n",
      "        │   ├──38x100 region at (1000, 245) \"Ut non numquam ut.\"\n",
      "        │   └──38x100 region at (911, 242) \"Ipsum sed non dolore eius consectetur.\"\n",
      "        ├──156x406 region at (1105, 461)\n",
      "        │   ├──29x100 region at (1170, 587) \"Sed sed magnam tempora velit.\"\n",
      "        │   ├──38x100 region at (1194, 571) \"Numquam quisquam dolore ut non.\"\n",
      "        │   └──38x100 region at (1219, 509) \"Sit amet ipsum neque neque adipisci consectetur.\"\n",
      "        └──90x406 region at (442, 0)\n",
      "            ├──22x91 region at (442, 14) \"Ipsum ut eius sit porro sit.\"\n",
      "            ├──13x67 region at (780, 0) \"Dolorem voluptatem sed voluptatem non modi quisquam.\"\n",
      "            ├──22x100 region at (681, 18) \"Sed amet labore dolorem velit aliquam.\"\n",
      "            ├──21x100 region at (554, 0) \"Sit non amet velit dolorem dolore labore.\"\n",
      "            └──22x100 region at (667, 6) \"Dolorem amet amet modi voluptatem.\"\n"
     ]
    }
   ],
   "source": [
    "from htrflow_core.models.dummy_models import RecognitionModel\n",
    "\n",
    "recognition_model = RecognitionModel()\n",
    "results = recognition_model(volume.segments())\n",
    "volume.update(results)\n",
    "print(volume)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing nodes\n",
    "Specific nodes are accessed by tuple indexing. Here we extract the first line of the first region of the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<volume.RegionNode at 0x7f296eceb130>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access image 0, region 0, subregion 0\n",
    "volume[0, 0, 0]\n",
    "\n",
    "# Access image 0, region 0\n",
    "volume[0, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image associated with each node is accessed through the `image` attribute. The image isn't stored directly in the node, instead, the node refers to the parent image, and crops it according to its box:\n",
    "\n",
    "```python\n",
    "\n",
    "class BaseImageNode:\n",
    "    \n",
    "    @property\n",
    "    def image(self):\n",
    "        x1, x2, y1, y2 = self.box\n",
    "        return self.parent.image[y1:y2, x1:x2]\n",
    "    \n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume[0, 0, 0].image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinates\n",
    "\n",
    "All nodes have a `coordinate` attribute. This is the location of the node's top-left corner *relative to the original image*. The base image node's coordinate is thus (0,0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(volume[0].coordinate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For first-level regions `coordinate` is the same as the corner of the segment bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coordinate:', volume[0, 0].coordinate)\n",
    "print('Bounding box:', volume[0, 0].data['segment'].box, '(x1, x2, y1, y2)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for nested regions the two differ, because `coordinate` is relative to the original image, while the segment bounding box is relative to the parent region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Global coordinate:', volume[0, 0, 0].coordinate)\n",
    "print('Local bounding box:', volume[0, 0, 0].data['segment'].box)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
